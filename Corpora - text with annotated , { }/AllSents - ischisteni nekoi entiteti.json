{"content": "The initial learning rate was set to 0.0005 , as shown in Table 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 12, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 21, "end": 24}]}]}
{"content": "Deep Voice 3 is a fully convolutional architecture for speech synthesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 24, "end": 36}]}]}
{"content": "Its character-to-spectrogram architecture enables fully parallel computation and the training is much faster than at the RNN architectures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 121, "end": 123}]}]}
{"content": "Those features are in a key , value form and they are fed into the attention-based decoder ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "attention-based", "start": 67, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decoder", "start": 83, "end": 89}]}]}
{"content": "The hidden layers of the decoder are fed into the third converter layer , which is capable of predicting the acoustic features for waveform synthesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decoder", "start": 25, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 11, "end": 16}]}]}
{"content": "The training of the model is followed by validation , which means that each 10K steps are evaluated before the training process proceeds ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "10K", "start": 76, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 13, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 16, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 20, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "steps", "start": 80, "end": 84}]}]}
{"content": "The evaluation is done on external , unknown sentences that provide insight into the advancement of the learnt dependencies between the dataset and the hidden layer weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 136, "end": 142}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 152, "end": 157}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 165, "end": 171}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 159, "end": 163}]}]}
{"content": "Deep Voice 3 suggested that parameters worked perfectly for our model , thus we used the same hyperparameters without increasing the demand due to our resource limitations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 28, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 64, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 94, "end": 108}]}]}
{"content": "The model started to produce an intelligible , understandable , and partially human-like speech after 50 K steps , as observed from the figure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "50", "start": 102, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "K", "start": 105, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "steps", "start": 107, "end": 111}]}]}
{"content": "Loss function is a metric that refers to the accuracy of the prediction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Loss", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 45, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 5, "end": 12}]}]}
{"content": "The main objective is to minimize the model errors or minimize the loss function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 38, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 67, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 72, "end": 79}]}]}
{"content": "In our case , the loss functions behaves in a desired manner , it gradually decreases , converging to a value of 0.1731 after 162.2 K steps in four days and 11 h of training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 18, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "converging", "start": 88, "end": 97}]}, {"label": ["B-DeepLearning"], "points": [{"text": "162.2", "start": 126, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 23, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "K", "start": 132, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "steps", "start": 134, "end": 138}]}]}
{"content": "Learning rate plays a vital role in minimizing the loss function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 51, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 9, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 56, "end": 63}]}]}
{"content": "The gradient norm that is presented in the same figure calculates the L2 norm of the gradients of the last layer of the Deep learning network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 70, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradients", "start": 85, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "last", "start": 102, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 120, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "norm", "start": 13, "end": 16}, {"start": 73, "end": 76, "text": "norm"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 125, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 134, "end": 140}]}]}
{"content": "It is an indicator showing whether the weights of the Deep learning network are properly updated ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 39, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 54, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 59, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 68, "end": 74}]}]}
{"content": "This problem affects the upper layers of the Deep learning network , making it really hard for the network to learn and tune the parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 45, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tune", "start": 120, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 50, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 59, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 125, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 129, "end": 138}]}]}
{"content": "In such case , the model is unstable and it is not able to learn from data , since the accumulation of large error gradients during the training process result in very large updates in the Deep learning model weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 19, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 109, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 189, "end": 192}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradients", "start": 115, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 194, "end": 201}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 203, "end": 207}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 209, "end": 215}]}]}
{"content": "By principles of transfer learning we tried to fine-tune the Russian TTS model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "transfer", "start": 17, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "TTS", "start": 69, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 26, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 73, "end": 77}]}]}
{"content": "So as to automatically and accurately classify the FCGR encoded data , we experimentally compared Multilayer Perceptron Artificial Neural Network MLP-ANN , Support Vector Machine SVM and Na\u00c3\u00afve Bayes NB , which are frontline pattern recognition tools in machine learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 98, "end": 107}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 120, "end": 129}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP-ANN", "start": 146, "end": 152}]}, {"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 254, "end": 260}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 109, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 131, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 138, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 262, "end": 269}]}]}
{"content": "The MLP-ANN contains 64 neurons in the input layer 64 element FCGR , 6 neurons in the output layer 5 mutation classes and 1 normal class and two hidden layers with the neurons experimentally varied from 10 to 100 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP-ANN", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "64", "start": 21, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 39, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "6", "start": 69, "end": 69}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 86, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 145, "end": 150}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 24, "end": 30}, {"start": 71, "end": 77, "text": "neurons"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 45, "end": 49}, {"start": 93, "end": 97, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 152, "end": 157}]}]}
{"content": "The result obtained by varying the neurons in the hidden layer is reported in Sect 3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 57, "end": 61}]}]}
{"content": "Deep neural networks DNN or deep learning models , were proved to be able to extract automatically useful features from input patterns ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 21, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 28, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 12, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 33, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 42, "end": 47}]}]}
{"content": "Under this framework , Long Short-Term Memory LSTM is a recurrent unit that reads a sequence one step at a time and can exploit long range relations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 23, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 46, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 56, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-Term", "start": 28, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 66, "end": 69}]}]}
{"content": "In this work , we propose a DNN model for nucleosome identification on sequences from three different species ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 28, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 32, "end": 36}]}]}
{"content": "Recently deep neural networks or deep learning models , were proved to be able to automatically extract useful features from input patterns with no a priori information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}, {"start": 33, "end": 36, "text": "deep"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 21, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 38, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 47, "end": 52}]}]}
{"content": "The two main categories of deep neural models are Convolutional Neural Networks CNN and Recurrent Neural Networks RNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 27, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 50, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 80, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Recurrent", "start": 88, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 114, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 32, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 64, "end": 69}, {"start": 98, "end": 103, "text": "Neural"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 71, "end": 78}, {"start": 105, "end": 112, "text": "Networks"}]}]}
{"content": "CNNs are characterized by an initial layer of convolutional filters , followed by a non Linearity , a sub-sampling , and a fully connected layer which realized the final classification ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 29, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 46, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 123, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 37, "end": 41}, {"start": 139, "end": 143, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "filters", "start": 60, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 129, "end": 137}]}]}
{"content": "Conversely , in this work we want to avoid the feature extraction step in order to fully exploit the capabilities of DNNs , making use of a convolutional layer for extracting features from local sequences of nucleotides , and an LSTM to take into account longer-range positional information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 117, "end": 120}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 140, "end": 152}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 229, "end": 232}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 154, "end": 158}]}]}
{"content": "Another important component of a deep neural network is the max-pooling layer , that usually follows the recurrent or convolutional layers in the computation flow ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 33, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 60, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 118, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 38, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 45, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 132, "end": 137}]}]}
{"content": "The dropout layer randomly sets to zero the output from the preceding layer during training , with a probability p given as a fixed parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 132, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 12, "end": 16}]}]}
{"content": "When p = 0.5 , it is equivalent to training 2|W| networks with shared parameters , where |W| is the number of neurons subject to dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 70, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 129, "end": 135}]}]}
{"content": "This results in a strong regularization effect , which helps in preventing overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 25, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 75, "end": 85}]}]}
{"content": "We propose three kind of architectures , obtained by the composition of six kinds of neural layers: a convolutional layer , a max pooling layer , a dropout layer , a long short-term memory LSTM layer , a fully connected layer and a softmax layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 85, "end": 90}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 102, "end": 114}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 126, "end": 128}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 148, "end": 154}]}, {"label": ["B-DeepLearning"], "points": [{"text": "long", "start": 166, "end": 169}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 189, "end": 192}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 204, "end": 208}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 232, "end": 238}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers:", "start": 92, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 116, "end": 120}, {"start": 138, "end": 142, "text": "layer"}, {"start": 156, "end": 160, "text": "layer"}, {"start": 220, "end": 224, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 130, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "short-term", "start": 171, "end": 180}]}, {"label": ["I-DeepLearning"], "points": [{"text": "memory", "start": 182, "end": 187}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 210, "end": 218}]}]}
{"content": "The Max Pooling operation with width and stride 2 helps to capture the most salient features extracted by the previous layer and reduces the output size from 145 to 72 vectors ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Max", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 41, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vectors", "start": 168, "end": 174}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Pooling", "start": 8, "end": 14}]}]}
{"content": "The Dropout operation with probability p = 0.5 is used to prevent overfitting during the training phase ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 66, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 89, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 98, "end": 102}]}]}
{"content": "We notice that the best architecture is the CONV-LSTM-FCX2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CONV-LSTM-FCX2", "start": 44, "end": 57}]}]}
{"content": "We have proposed a novel model parameter training scheme based on the concepts of quantum computing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 25, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 31, "end": 39}]}]}
{"content": "We apply deep learning methods in this paper , namely we use convolutional neural networks CNNs for description and prediction of the red blood cells\u00e2\u20ac\u2122 trajectory , which is crucial in modeling of a blood flow ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 61, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 91, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 14, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "methods", "start": 23, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 75, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 82, "end": 89}]}]}
{"content": "Training and testing sets used for neural network are extracted from simulations which differ only in initial seeding of the cells ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Training", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 35, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "seeding", "start": 110, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 21, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 42, "end": 48}]}]}
{"content": "Besides using convolution or fully connected layers , we also used a relatively new type of layers , the dense convolution layers , introduced in ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 14, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 105, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 35, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 45, "end": 50}, {"start": 123, "end": 128, "text": "layers"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 111, "end": 121}]}]}
{"content": "Used neural network architecture hyperparameters are these: Weights are initialized in the range xavier , bias is set to 0. Learning rate is 0.0002 , \u00ce\u00bb1 = \u00ce\u00bb2 = 0.000001 , dropout = 0.02 and minibatch size is 32 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 33, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Weights", "start": 60, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 106, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 124, "end": 131}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 173, "end": 179}]}, {"label": ["B-DeepLearning"], "points": [{"text": "minibatch", "start": 192, "end": 200}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 12, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 133, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "=", "start": 181, "end": 181}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.02", "start": 183, "end": 186}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 202, "end": 205}]}]}
{"content": "The training phase lasts about 6 hours for more complicated network architectures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 60, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 13, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architectures", "start": 68, "end": 80}]}]}
{"content": "The convolution region of the CNNs includes the convolution , the activation and the pooling layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 4, "end": 14}, {"start": 48, "end": 58, "text": "convolution"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 30, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 66, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pooling", "start": 85, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "region", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 93, "end": 98}]}]}
{"content": "Different techniques were used to improve the accuracy: Learning rate tuning , which controls the update of the CNNs weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy:", "start": 46, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 56, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 112, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 65, "end": 68}]}]}
{"content": "The neural networks are updated via the stochastic gradient ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 40, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 11, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 51, "end": 58}]}]}
{"content": "Data Augmentation , that consists in the realization of different random operations of rotation , translation and zoom on the images to avoid overfitting and improve generalization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Data", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 126, "end": 131}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 142, "end": 152}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 166, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Augmentation", "start": 5, "end": 16}]}]}
{"content": "Fine-Tuning , that is based on defrosting the weights of the convolutional layers , allowing the network to train in its integrity ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Fine-Tuning", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 46, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 61, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 75, "end": 80}]}]}
{"content": "It is applied using a differential learning rate , that is , introducing three different and successively higher values of the learning rate , thus taking into account the differential knowledge of the layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 35, "end": 42}, {"start": 127, "end": 134, "text": "learning"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 44, "end": 47}, {"start": 136, "end": 139, "text": "rate"}]}]}
{"content": "Therefore , although Resnet50 provided the highest accuracy is less stable than Resnet 34 1.09% vs 063% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Resnet50", "start": 21, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 51, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Resnet", "start": 80, "end": 85}]}]}
{"content": "Table 2 provides the corresponding confusion matrices in validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "confusion", "start": 35, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "matrices", "start": 45, "end": 52}]}]}
{"content": "With respect to the tune of models\u00e2\u20ac\u2122 hyper-parameters , the R package mlrMBO was used to perform a Bayesian optimization within the train set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "models\u00e2\u20ac\u2122", "start": 28, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 38, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "train", "start": 133, "end": 137}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 139, "end": 141}]}]}
{"content": "This package implements a Bayesian optimization of black-box functions which allows to find faster an optimal hyper-parameters setting in contrast to traditional hyperparameters search strategies such as grid search highly time consuming when more than 3 hyper-parameters are tuned or random search not efficient enough since similar or non-sense hyper-parameters settings might be tested ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 110, "end": 125}, {"start": 255, "end": 270, "text": "hyper-parameters"}, {"start": 347, "end": 362, "text": "hyper-parameters"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 162, "end": 176}]}, {"label": ["B-DeepLearning"], "points": [{"text": "grid", "start": 204, "end": 207}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 285, "end": 290}]}, {"label": ["I-DeepLearning"], "points": [{"text": "search", "start": 209, "end": 214}, {"start": 292, "end": 297, "text": "search"}]}]}
{"content": "Table 1 shows the average AUC performance , standard deviation and number of genes retained by the different models tested over the test sets of the cross-validation setting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 26, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 109, "end": 114}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 149, "end": 164}]}]}
{"content": "The proposed Convolutional Neural Network CNN is consisting of two parallel convolutional layers taking as inputs transversal , coronal and axial slices acquired before and after chemotherapy for each patient ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 13, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 42, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 76, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patient", "start": 201, "end": 207}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 27, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 34, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 90, "end": 95}]}]}
{"content": "As illustrated in Fig. 2 , this architecture contains two similar branches , each one contains 4 blocks of 2D convolution followed by an activation function ReLU and a Max pooling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "2D", "start": 107, "end": 108}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 137, "end": 146}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 157, "end": 160}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Max", "start": 168, "end": 170}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 110, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 148, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 172, "end": 178}]}]}
{"content": "In the first and second blocks , 32 kernels were used for each convolutional layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 36, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 63, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 77, "end": 81}]}]}
{"content": "The parallel Deep learning architecture was applied for each view using corresponding slices before and after the first chemotherapy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 18, "end": 25}]}]}
{"content": "Consequently , we used the Stochastic Gradient Descent SGD with a learning rate of 00052 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Stochastic", "start": 27, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SGD", "start": 55, "end": 57}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 66, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Gradient", "start": 38, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Descent", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 75, "end": 78}]}]}
{"content": "A learning rate decay of 3.46e\u00e2\u02c6\u20195 was used to schedule a best accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 2, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 63, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 11, "end": 14}]}]}
{"content": "To compile the model , a categorical cross entropy was used as loss function and standard accuracy was used as a metric ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 15, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 37, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 63, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 43, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 68, "end": 75}]}]}
{"content": "To avoid results\u00e2\u20ac\u2122 bias , a 5-Fold stratified cross validation with AUC as metric was used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 47, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 69, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 53, "end": 62}]}]}
{"content": "Within 150 epochs with 5-fold stratified cross validation , an accuracy of 90.03 was obtained using 20% of 3D validation data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 11, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 41, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 63, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 110, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 47, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 121, "end": 124}]}]}
{"content": "An overfitting was observed during training when using only one of the views without data augmentation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 3, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 85, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 90, "end": 101}]}]}
{"content": "Besides the work we did on building other types of agents we have also tried to explore in more depth different cognitive and affective models of agents , including symbolic BDI models as well as neural network models ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 196, "end": 201}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 203, "end": 209}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 211, "end": 216}]}]}
{"content": "While we are defining FSTs , and after having introduced the use of HMMs as stochastic FSTs , it is worth noticing that regular transduction rules can also be implemented in the form of so-called multilayer perceptrons MLP a particular type of artificial neural networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 196, "end": 205}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 219, "end": 221}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 244, "end": 253}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 207, "end": 217}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 255, "end": 260}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 262, "end": 269}]}]}
{"content": "Artificial neural networks are based on simplistic models for biological neuron behavior and the interconnections between these neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 0, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 18, "end": 25}]}]}
{"content": "Most often , the nonlinear function is a limiter or a sigmoid function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 54, "end": 60}]}]}
{"content": "The MLP is the far most widely used network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 4, "end": 6}]}]}
{"content": "It is composed of an input layer and an output layer separated by one or more hidden layers of nodes , with each layer connected to the next layer , feeding its node values forward Fig. 24 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 21, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 40, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 78, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "nodes", "start": 95, "end": 99}]}, {"label": ["B-DeepLearning"], "points": [{"text": "node", "start": 161, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 27, "end": 31}, {"start": 47, "end": 51, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 85, "end": 90}]}]}
{"content": "In many domains , neural networks are an effective alternative to statistical methods ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 18, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 25, "end": 32}]}]}
{"content": "James Henderson has identified a suitable neural network architecture for natural language parsing , called Simple Synchrony Networks SSNs , which he discusses in chapter 6 , A Neural Network Parser that Handles Sparse Data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 42, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 108, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 134, "end": 137}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 177, "end": 182}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sparse", "start": 212, "end": 217}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 115, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 125, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 184, "end": 190}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Data", "start": 219, "end": 222}]}]}
{"content": "Because neural networks learn their own internal representations\u00e2\u20ac\u0161 neural networks can decide automatically what features to count and how reliable they are for estimating the desired probabilities ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 8, "end": 13}, {"start": 68, "end": 73, "text": "neural"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 15, "end": 22}, {"start": 75, "end": 82, "text": "networks"}]}]}
{"content": "This generalization ability is a result of using a neural network method for representing sets of objects\u00e2\u20ac\u0161 called Temporal Synchrony Variable Binding TSVB Shastri and Ajjanagadde\u00e2\u20ac\u0161 1993 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 58, "end": 64}]}]}
{"content": "SRNs can learn generalizations over positions in an input sequence and thus can handle unbounded input sequences\u00e2\u20ac\u0161 which has made them of interest in natural language processing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 15, "end": 29}]}]}
{"content": "By using TSVB to represent the constituents in a syntactic structure\u00e2\u20ac\u0161 SSNs also learn generalizations over structural constituents ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 72, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 88, "end": 102}]}]}
{"content": "The linguistic relevance of this class of generalizations is what accounts for the fact that SSNs generalize from training set to testing set in an appropriate way\u00e2\u20ac\u0161 as demonstrated in Section 4 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 42, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 93, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 114, "end": 121}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 130, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 123, "end": 125}, {"start": 138, "end": 140, "text": "set"}]}]}
{"content": "In this section we briefly outline the SSN architecture and how it can be used to estimate the parameters of a probability model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 39, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 95, "end": 104}]}]}
{"content": "Simple Recurrent Networks extend MLPs to sequences by using the hidden representations as representations of the network\u00e2\u20ac\u2122s state at a given point in the sequence ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLPs", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 7, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 17, "end": 24}]}]}
{"content": "Simple Synchrony Networks extend SRNs by computing one of these sequences for each object in a set of objects ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 7, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 17, "end": 24}]}]}
{"content": "The most important feature of any learning architecture is how it generalizes from training data to testing data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalizes", "start": 66, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 83, "end": 90}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 100, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 92, "end": 95}, {"start": 108, "end": 111, "text": "data"}]}]}
{"content": "SRNs are popular for sequence processing because they inherently generalize over sequence positions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 0, "end": 3}]}]}
{"content": "Because inputs are fed to an SRN one at a time\u00e2\u20ac\u0161 and the same trained parameters called link weights apply at every time\u00e2\u20ac\u0161 information learned at one sequence position will inherently be generalized to other sequence positions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRN", "start": 29, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 71, "end": 80}]}]}
{"content": "This generalization ability manifests itself in the fact that SRNs can handle arbitrarily long sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 62, "end": 65}]}]}
{"content": "This generalization ability manifests itself in the fact that these networks can handle arbitrarily many constituents\u00e2\u20ac\u0161 and therefore unbounded phrase structure trees ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}]}
{"content": "After the parent of a terminal is chosen it is used by the SSN to estimate the parameters for latter portions of the sentence\u00e2\u20ac\u0161 including latter parent estimates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 59, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 79, "end": 88}]}]}
{"content": "We also bias the network training by providing a new-nonterminal input unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 8, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 25, "end": 32}]}]}
{"content": "To test the ability of Simple Synchrony Networks to handle sparse data we train the SSN parser described in the previous section on a relatively small set of sentences and then test how well it generalizes to a set of previously unseen sentences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sparse", "start": 59, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 84, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 30, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 40, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 66, "end": 69}]}]}
{"content": "This process can be continued until no more changes are made , but to avoid over-fitting it is better to check the performance of the network on a validation set and stop training when the performance on the validation set reaches a maximum ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 76, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 147, "end": 156}, {"start": 208, "end": 217, "text": "validation"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 158, "end": 160}, {"start": 219, "end": 221, "text": "set"}]}]}
{"content": "This is why we have split the corpus into three datasets , one for training , one for validation , and one for testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 48, "end": 55}]}]}
{"content": "This technique also allows multiple versions of the network to be trained and then evaluated using the validation set , without ever using the testing set until a single network has been chosen ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 103, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 143, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 114, "end": 116}, {"start": 151, "end": 153, "text": "set"}]}]}
{"content": "A variety of hidden layer sizes and random initial weight seeds were used in the different networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 20, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "seeds", "start": 58, "end": 62}]}]}
{"content": "Larger hidden layers result in the network being able to fit the training data more precisely , but can lead to over-fitting and therefore bad performance on the validation set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 7, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 112, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 162, "end": 171}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 173, "end": 175}]}]}
{"content": "From the multiple networks trained , the best network was chosen on the basis of its performance on the validation set , and this one network was used in testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 104, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 115, "end": 117}]}]}
{"content": "The best network had 100 hidden units and trained for a total of 145 passes through the training set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 88, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 97, "end": 99}]}]}
{"content": "Because this process does not require a validation set , we estimate these parameters using the combination of the training set and the validation set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 40, "end": 49}, {"start": 136, "end": 145, "text": "validation"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 75, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 115, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 51, "end": 53}, {"start": 124, "end": 126, "text": "set"}, {"start": 147, "end": 149, "text": "set"}]}]}
{"content": "This generalization performance is due to SSNs\u00e2\u20ac\u2122 ability to generalize across constituents as well as across sequence positions , plus the ability of neural networks in general to learn what input features are important as well as what they imply about the output ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs\u00e2\u20ac\u2122", "start": 42, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 151, "end": 156}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 158, "end": 165}]}]}
{"content": "In particular , we used a momentum of 0.9 and weight decay regularization of between 0.1 and 0.0. Both the learning rate and the weight decay were decreased as the learning proceeded , based on training error and validation error , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 26, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 46, "end": 51}, {"start": 129, "end": 134, "text": "weight"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 59, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 107, "end": 114}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 194, "end": 201}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 213, "end": 222}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 53, "end": 57}, {"start": 136, "end": 140, "text": "decay"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 116, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "error", "start": 203, "end": 207}, {"start": 224, "end": 228, "text": "error"}]}]}
{"content": "We mean feature here in the sense of features which can be computed from discourse as input to machine learning algorithms for classification tasks such as topic segmentation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 95, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 103, "end": 110}]}]}
{"content": "Moreover , generic tools are provided for iterating over discourses , processing them , and extracting sets of feature values at regular intervals which can then be piped directly into learners like decision trees , neural nets or support vector machines ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 216, "end": 221}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 223, "end": 226}]}]}
{"content": "A variety of learning parameters were explored , and the best-performing parameter set was selected: initial Q values set to 0 , exploration parameter \u00ce\u00b5 = 0.2 , and the learning rate \u00ce\u00b1 set to 1/k where k is the number of visits to the Qs , a being updated ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 13, "end": 20}, {"start": 170, "end": 177, "text": "learning"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 73, "end": 81}, {"start": 141, "end": 149, "text": "parameter"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 22, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 179, "end": 182}]}]}
{"content": "Schmid 1994a presents a neural network tagger based on multilayer perceptron networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 55, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 66, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 77, "end": 84}]}]}
{"content": "An artificial neural network consist of simple units each associated with an activation value and directed links for passing the values between the units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 3, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 21, "end": 27}]}]}
{"content": "Activation values are propagated from input to output layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Activation", "start": 0, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "values", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 54, "end": 59}]}]}
{"content": "At each unit , the input activation values are summed and a bias parameter is added ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 60, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 65, "end": 73}]}]}
{"content": "The network learns by adapting the weights of the connections between units until the correct output is produced ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 35, "end": 41}]}]}
{"content": "In the output layer , all units have a value of zero except the correct unit tag , which gets the value of one ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 7, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 14, "end": 18}]}]}
{"content": "In a system of this kind , tagging a word means i copying the tag probabilities of the word and its neighbours into the input units and ii propagating the activations to the output units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 120, "end": 124}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 174, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 126, "end": 130}, {"start": 181, "end": 185, "text": "units"}]}]}
{"content": "In back-propagation learning , this training is done by repeatedly iterating over all examples , comparing for each example the output predicted by the network random at first to the desired output and changing connection weights between network nodes in such a way that performance increases ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "back-propagation", "start": 3, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "connection", "start": 211, "end": 220}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 222, "end": 228}]}]}
{"content": "Multilayer Perceptrons Rumelhart et al. 1986 are the most popular neural network architecture ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 0, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptrons", "start": 11, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 73, "end": 79}]}]}
{"content": "The activation rule is a local rule which is used by each unit to compute its activation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 4, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rule", "start": 15, "end": 18}]}]}
{"content": "Given two words preceding context and 89 categories , the network has an input layer of 178 units and an output layer of 89 units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 73, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 105, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 79, "end": 83}, {"start": 112, "end": 116, "text": "layer"}]}]}
{"content": "Adding a hidden layer to a two-layer network did not improve performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 9, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 16, "end": 20}]}]}
{"content": "Connectionist approaches also require the computation of fewer parameters weights than statistical models N-gram probabilities , which becomes especially useful when considering a wider context than trigrams ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 63, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 74, "end": 80}]}]}
{"content": "On the theoretical side , there is a need for more insight into the differences and similarities in how generalization is achieved in this area by different statistical and machine learning techniques ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 104, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 173, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 181, "end": 188}]}]}
{"content": "Most emphasis in current deep learning artificial neural network based automatic recognition of speech is put on deep net architectures with multiple sequential levels of processing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 25, "end": 28}, {"start": 113, "end": 116, "text": "deep"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 39, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 30, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 57, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 118, "end": 120}]}]}
{"content": "Current state-of-the-art stochastic ASR systems often estimate the likelihood pX|W by a discriminatively-trained multi-layer perceptron artificial neural network MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multi-layer", "start": 113, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 136, "end": 145}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 162, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 125, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 147, "end": 152}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 154, "end": 160}]}]}
{"content": "The rule-based algorithm obtained 60.67% splitting accuracy at word level and 94.31% accuracy , within the word , at split level ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 51, "end": 58}, {"start": 85, "end": 92, "text": "accuracy"}]}]}
{"content": "The hyperparameters found to optimize it are n = 4 , \u00ce\u00b1 = 10\u00e2\u02c6\u20195 , marker=true ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 4, "end": 18}]}]}
{"content": "Recently there has been a renewed interest in applying neural networks ANNs to speech recognition , thanks to the invention of deep neural nets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 71, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 127, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 62, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 132, "end": 137}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 139, "end": 142}]}]}
{"content": "It treats the network as a deep belief network DBN built out of restricted Bolztmann machines RBMs , and optimizes an energy-based target function using the contrastive divergence CD algorithm ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 27, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 47, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 64, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 94, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "belief", "start": 32, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Bolztmann", "start": 75, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 85, "end": 92}]}]}
{"content": "As for the third method , it is different from the two above in the sense that in this case it is not the training algorithm that is slightly modified , but the neurons themselves ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 106, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "algorithm", "start": 115, "end": 123}]}]}
{"content": "Namely , the usual sigmoid activation function is replaced with the rectifier function max0 , x ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 19, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 27, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 68, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 38, "end": 45}, {"start": 78, "end": 85, "text": "function"}]}]}
{"content": "These kinds of neural units have been proposed by Glorot et al. , and were successfully applied to image recognition and NLP tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 15, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 99, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 22, "end": 26}]}]}
{"content": "Rectified linear units were also found to improve restricted Boltzmann machines ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Rectified", "start": 0, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 50, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 10, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 17, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Boltzmann", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 71, "end": 78}]}]}
{"content": "It has been shown recently that a deep rectifier network can attain the same phone recognition performance as that for the pre-trained nets of Mohamed et al. 4 , but without the need for any pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 34, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-trained", "start": 123, "end": 133}]}, {"label": ["B-DeepLearning"], "points": [{"text": "4", "start": 158, "end": 158}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 191, "end": 202}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 39, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 135, "end": 138}]}]}
{"content": "This efficient unsupervised algorithm , first described in , can be used for learning the connection weights of a deep belief network DBN consisting of several layers of restricted Boltzmann machines RBMs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "connection", "start": 90, "end": 99}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 114, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 134, "end": 136}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 170, "end": 179}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 200, "end": 203}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 101, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "belief", "start": 119, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 126, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Boltzmann", "start": 181, "end": 189}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 191, "end": 198}]}]}
{"content": "As their name implies , RBMs are a variant of Boltzmann machines , with the restriction that their neurons must form a bipartite graph ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 24, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Boltzmann", "start": 46, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 56, "end": 63}]}]}
{"content": "They have an input layer , representing the features of the given task , a hidden layer which has to learn some representation of the input , and each connection in an RBM must be between a visible unit and a hidden unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 75, "end": 80}, {"start": 209, "end": 214, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBM", "start": 168, "end": 170}]}, {"label": ["B-DeepLearning"], "points": [{"text": "visible", "start": 190, "end": 196}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 19, "end": 23}, {"start": 82, "end": 86, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 198, "end": 201}, {"start": 216, "end": 219, "text": "unit"}]}]}
{"content": "RBMs can be trained using the one-step contrastive divergence CD algorithm described in 6 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one-step", "start": 30, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CD", "start": 62, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "contrastive", "start": 39, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "divergence", "start": 51, "end": 60}]}]}
{"content": "It is a simple algorithm where first we train a network with one hidden layer to full convergence using backpropagation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 65, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 81, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 104, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 72, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convergence", "start": 86, "end": 96}]}]}
{"content": "Then we replace the softmax layer by another randomly initialized hidden layer and a new softmax layer on top , and we train the network again ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 20, "end": 26}, {"start": 89, "end": 95, "text": "softmax"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 73, "end": 77}]}]}
{"content": "This process is repeated until we reach the desired number of hidden layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 69, "end": 74}]}]}
{"content": "Seide et al. found that this method gives the best results if one performs only a few iterations of backpropagation in the pre-training phase instead of training to full convergence with an unusually large learn rate ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "iterations", "start": 86, "end": 95}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 100, "end": 114}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 123, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 170, "end": 180}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 206, "end": 210}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 212, "end": 215}]}]}
{"content": "In their paper , they concluded that this simple training strategy performs just as well as the much more complicated DBN pre-training method described above ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 118, "end": 120}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 122, "end": 133}]}]}
{"content": "In the case of the third method it is not the training algorithm , but the neurons that are slightly modified ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 46, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "algorithm", "start": 55, "end": 63}]}]}
{"content": "Instead of the usual sigmoid activation , here we apply the rectifier function max0 , x for all hidden neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 21, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 60, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 96, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 70, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "max0", "start": 79, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 103, "end": 109}]}]}
{"content": "There are two fundamental differences between the sigmoid and the rectifier functions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 50, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 66, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 76, "end": 84}]}]}
{"content": "One is that the output of rectifier neurons does not saturate as their activity gets higher ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 26, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 36, "end": 42}]}]}
{"content": "Glorot et al. conjecture that this is very important in explaining their good performance in deep nets: because of this linearity , there is no gradient vanishing effect ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 93, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 144, "end": 151}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets:", "start": 98, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vanishing", "start": 153, "end": 161}]}]}
{"content": "One might suppose that this could harm optimization by blocking gradient backpropagation , but the experimental results do not support this hypothesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 64, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "backpropagation", "start": 73, "end": 87}]}]}
{"content": "The main advantage of deep rectifier nets is that they can be trained with the standard backpropagation algorithm , without any pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 22, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 88, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 128, "end": 139}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 27, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 37, "end": 40}]}]}
{"content": "A random 10% of the training set was held out for validation purposes , and this block of data will be referred to as the \u00e2\u20ac\u2122development set\u00e2\u20ac\u2122 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 20, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "\u00e2\u20ac\u2122development", "start": 122, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 29, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set\u00e2\u20ac\u2122", "start": 137, "end": 142}]}]}
{"content": "It contains about 28 hours of recordings , from which 22 hours were selected for the training set , 2 hours for the development set and 4 hours for the test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 85, "end": 92}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 152, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 94, "end": 96}, {"start": 157, "end": 159, "text": "set"}]}]}
{"content": "In the case of the DBN-based pre-training method see Section 2.1 , we applied stochastic gradient descent i.e. backpropagation training with a mini-batch size of 128 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 29, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 78, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 111, "end": 125}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mini-batch", "start": 143, "end": 152}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 89, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 98, "end": 104}]}]}
{"content": "For Gaussian-binary RBMs , we ran 50 epochs with a fixed learning rate of 0.002 , while for binary-binary RBMs we used 30 epochs with a learning rate of 002 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 20, "end": 23}, {"start": 106, "end": 109, "text": "RBMs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 37, "end": 42}, {"start": 122, "end": 127, "text": "epochs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 57, "end": 64}, {"start": 136, "end": 143, "text": "learning"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 66, "end": 69}, {"start": 145, "end": 148, "text": "rate"}]}]}
{"content": "Then , to fine-tune the pre-trained nets , again backpropagation was applied with the same mini-batch size as that used for pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-trained", "start": 24, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 49, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mini-batch", "start": 91, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 124, "end": 135}]}]}
{"content": "The initial learn rate was set to 0.01 , and it was halved after each epoch when the error on the development set increased ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 12, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 70, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 98, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 18, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 110, "end": 112}]}]}
{"content": "During both the pretraining and fine-tuning phases , the learning was accelerated by using a momentum of 0.9 except for the first epoch of fine-tuning , which did not use the momentum method ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pretraining", "start": 16, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fine-tuning", "start": 32, "end": 42}, {"start": 139, "end": 149, "text": "fine-tuning"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 93, "end": 100}, {"start": 175, "end": 182, "text": "momentum"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 130, "end": 134}]}]}
{"content": "Turning to the discriminative pre-training method see Section 2.2 , the initial learn rate was set to 0.01 , and it was halved after each epoch when the error on the development set increased ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 30, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 80, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 138, "end": 142}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 166, "end": 176}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 86, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 178, "end": 180}]}]}
{"content": "The learn rate was restored to its initial value of 0.01 after the addition of each layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 4, "end": 8}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 10, "end": 13}]}]}
{"content": "Furthermore , we found that using 5 epochs of backpropagation after the introduction of each layer gave the best results ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 36, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 46, "end": 60}]}]}
{"content": "For both the pre-training and fine-tuning phases we used a batch size of 128 and momentum of 0.8 except for the first epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 13, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fine-tuning", "start": 30, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 59, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 81, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 118, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 65, "end": 68}]}]}
{"content": "The initial learn rate for the fine-tuning of the full network was again set to 001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 12, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fine-tuning", "start": 31, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 18, "end": 21}]}]}
{"content": "The training of deep rectifier nets see Section 2.3 did not require any pre-training at all ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 72, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 21, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 31, "end": 34}]}]}
{"content": "The training of the network was performed using backpropagation with an initial learn rate of 0.001 and a batch size of 128 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 48, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 80, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 106, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 86, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 112, "end": 115}]}]}
{"content": "As can be seen , the three training methods performed very similarly on the test set , the only exception being the case of five hidden layers , where the rectifier net performed slightly better ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 129, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 155, "end": 163}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 136, "end": 141}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 165, "end": 167}]}]}
{"content": "It also significantly outperformed the other two methods on the development set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 64, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 76, "end": 78}]}]}
{"content": "We mention that a single hidden layer net with the same amount of weights as the best deep net yielded 237% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 66, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 86, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 91, "end": 93}]}]}
{"content": "Similar to the TIMIT tests , 2048 neurons were used for each hidden layer , with a varying number of hidden layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "2048", "start": 29, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 61, "end": 66}, {"start": 101, "end": 106, "text": "hidden"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 34, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 68, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 108, "end": 113}]}]}
{"content": "The error rates seem to saturate at 4-5 hidden layers , and the curves for the three methods run parallel and have only slightly different values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 47, "end": 52}]}]}
{"content": "The lowest error rate is attained with the five-layer rectifier network , both on the development and the test sets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 54, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 106, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 64, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 111, "end": 114}]}]}
{"content": "The iteration count we applied here 50 for Gaussian RBMs and 30 for binary RBMs is an average value , and follows the work of Seide et al ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "iteration", "start": 4, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 52, "end": 55}, {"start": 75, "end": 78, "text": "RBMs"}]}]}
{"content": "Discriminative pre-training is also much faster than the DBN-based method , but is still slower than rectifier nets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 15, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 101, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 111, "end": 114}]}]}
{"content": "Tuning the parameters so that the two systems had a similar real-time factor was also out of the question , as the hybrid model was implemented on a GPU , while the HMM used a normal CPU ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Tuning", "start": 0, "end": 5}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 7, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 11, "end": 20}]}]}
{"content": "Here , we compared two training methods and a new type of activation function for deep neural nets , and evaluated them on a large vocabulary recognition task ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 58, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 82, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 69, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 87, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 94, "end": 97}]}]}
{"content": "The three algorithms yielded quite similar recognition performances , but based on the training times deep rectifier networks seem to be the preferred choice ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 102, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 107, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 117, "end": 124}]}]}
{"content": "While the resulting speech sound likelihood estimates are demonstrated to be better that the earlier used likelihoods derived by generative Gaussian Mixture Models , unexpected signal distortions that were not seen in the training data can still make the acoustic likelihoods unacceptably low ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 222, "end": 229}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 231, "end": 234}]}]}
{"content": "Here , we compare three methods namely , the unsupervised pre-training algorithm of Hinton et al. , a supervised pre-training method that constructs the network layer-by-layer , and deep rectifier networks , which differ from standard nets in their activation function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 58, "end": 69}, {"start": 113, "end": 124, "text": "pre-training"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 182, "end": 185}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 249, "end": 258}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 187, "end": 195}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 197, "end": 204}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 260, "end": 267}]}]}
{"content": "Overall , for the large vocabulary speech recognition task we study here , deep rectifier networks offer the best tradeoff between accuracy and training time ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 75, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 144, "end": 151}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 80, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 90, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "time", "start": 153, "end": 156}]}]}
{"content": "In this new proposed approach , a multi-layer neural network multi-layer perceptron is used to learn the decision function that will then be used to select the words ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multi-layer", "start": 34, "end": 44}, {"start": 61, "end": 71, "text": "multi-layer"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decision", "start": 105, "end": 112}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 46, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 53, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 73, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 114, "end": 121}]}]}
{"content": "For training the neural network parameters we have to associate a target value to each word ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 17, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 24, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 32, "end": 41}]}]}
{"content": "The scores produced by the neural network module will then be used to sort the list of candidate words , and the top of the list will be selected to define the recognition vocabulary ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 27, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 34, "end": 40}]}]}
{"content": "Feature vectors and associated target values are used for training the neural network: the input-feature vector is the input layer 36 input neurons the target value is the output of the unique output layer neuron ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Feature", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 71, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input-feature", "start": 91, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 119, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "36", "start": 131, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 193, "end": 198}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vectors", "start": 8, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network:", "start": 78, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vector", "start": 105, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 125, "end": 129}, {"start": 200, "end": 204, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 134, "end": 138}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 140, "end": 146}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neuron", "start": 206, "end": 211}]}]}
{"content": "There is one hidden layer containing 18 neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "18", "start": 37, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 20, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 40, "end": 46}]}]}
{"content": "In recent years feed forward neural networks FFNN attracted attention due their ability to overcome biggest disadvantage of n-gram models: even when the ngram is not observed in training , FFNN estimates probabilities of the word based on the full history ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feed", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "FFNN", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "forward", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 29, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 36, "end": 43}]}]}
{"content": "The RNN is going further in model generalization: instead of considering only the several previous words parameter n the recursive weights are assumed to represent short term memory ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 28, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization:", "start": 34, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 105, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recursive", "start": 121, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 131, "end": 137}]}]}
{"content": "Long Short-Term Memory LSTM neural network is different type of RNN structure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 23, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 28, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 64, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-Term", "start": 5, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 35, "end": 41}]}]}
{"content": "LSTM approved themselves in various applications and it seems to be very promising course also for the field of language modelling ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 0, "end": 3}]}]}
{"content": "Typical NN unit consists of the input activation which is transformed to output activation with activation function usually sigmoidal ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN", "start": 8, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 32, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 96, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoidal", "start": 124, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 38, "end": 47}, {"start": 80, "end": 89, "text": "activation"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 107, "end": 114}]}]}
{"content": "Firstly , the activation function is applied to all gates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 14, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 25, "end": 32}]}]}
{"content": "There is a softmax function used in output layer to produce normalized probabilities ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 11, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 36, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 19, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 43, "end": 47}]}]}
{"content": "Normalization of input vector which is generally advised for neural networks is not needed due the 1-of-N input coding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 17, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 61, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vector", "start": 23, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 68, "end": 75}]}]}
{"content": "All models were trained with 20 cells in hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 41, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 48, "end": 52}]}]}
{"content": "The recurrent neural network language model RNNLM , originally proposed by 2 and 3 , incorporates the time dimension by expanding the input layer , which represents the current input word , with the previous hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 4, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 134, "end": 138}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 208, "end": 213}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 140, "end": 144}, {"start": 215, "end": 219, "text": "layer"}]}]}
{"content": "Theoretically , recurrent neural networks can store relevant information from previous time steps for an arbitrarily long period of time , making it possible to learn long-term dependencies ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 16, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 26, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 33, "end": 40}]}]}
{"content": "To explain how our approach works , let us examine the operation of a simple perceptron model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "perceptron", "start": 77, "end": 86}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 88, "end": 92}]}]}
{"content": "Hence , if the weights of the feature extraction layer were initialized with 2D DCT or Gabor filter coefficients , and only the weights of the hidden and output layers were tuned during training , then the model would be equivalent to a more traditional system , and incorporating the feature extraction step into the system would be just an implementational detail ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 15, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 30, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 154, "end": 159}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 206, "end": 210}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 38, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 161, "end": 166}]}]}
{"content": "Usually , as the backpropagation algorithm guarantees only a locally optimal solution , initializing the model with weights that already provide a good solution may help the backpropagation algorithm find a better local optimum than the one found using random initial values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 17, "end": 31}, {"start": 174, "end": 188, "text": "backpropagation"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 105, "end": 109}]}]}
{"content": "We should add that the same weights are applied on each input block , so the number of weights will not change in this layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 28, "end": 34}, {"start": 87, "end": 93, "text": "weights"}]}]}
{"content": "It consisted of a hidden feature extraction layer with a linear activation function , a hidden layer with 1000 neurons with the sigmoid activation function , and an output layer containing softmax units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 25, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "linear", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 88, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "1000", "start": 106, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 128, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 136, "end": 145}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 165, "end": 170}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 189, "end": 195}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}, {"start": 95, "end": 99, "text": "layer"}, {"start": 172, "end": 176, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 64, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 75, "end": 82}, {"start": 147, "end": 154, "text": "function"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 111, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 197, "end": 201}]}]}
{"content": "The number of output neurons was set to the number of classes 39 , while the number of neurons in the input and feature extraction layers varied , depending on how many neighbouring patches were actually used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 112, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 120, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 131, "end": 136}]}]}
{"content": "The neural net was trained with random initial weights in the hidden and output layers , using standard backpropagation on 90% of the training data in semi-batch mode , while crossvalidation on the remaining , randomly selected 10% of the training set was used as the stopping criterion ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 39, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 104, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 134, "end": 141}, {"start": 239, "end": 246, "text": "training"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "semi-batch", "start": 151, "end": 160}]}, {"label": ["B-DeepLearning"], "points": [{"text": "crossvalidation", "start": 175, "end": 189}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 11, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 143, "end": 146}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 248, "end": 250}]}]}
{"content": "The \u00e2\u20ac\u02dcextreme learning machine\u00e2\u20ac\u2122 of Huang et al. also exploits this suprising fact: this learning model is practically a twolayer network , where both layers are initialized randomly , and the lowest layer is not trained at all ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 100, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "twolayer", "start": 123, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 132, "end": 138}]}]}
{"content": "In order to verify how the whisper can be recognized by the ANN , the two speakerdependent ASRs were developed with MATLAB Neural Network Toolbox ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 60, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 123, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 130, "end": 136}]}]}
{"content": "The structures of these ANNs were: 396 input nodes , 140 hidden neurons and 50 output neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 24, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 39, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "50", "start": 76, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 45, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 64, "end": 70}, {"start": 86, "end": 92, "text": "neurons"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 79, "end": 84}]}]}
{"content": "The training , development and test sets for the classification task were not selected from the corpus in a completely random manner , but instead the division respected the websites , i.e. one set of websites was denoted the training set , another \u00e2\u20ac\u201c development set and the third \u00e2\u20ac\u201c test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 15, "end": 25}, {"start": 253, "end": 263, "text": "development"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 287, "end": 290}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 27, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "test", "start": 31, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 36, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 265, "end": 267}, {"start": 292, "end": 294, "text": "set"}]}]}
{"content": "The ASR system uses a hybrid Hidden Markov Model HMM and Deep Neural Network DNN architecture and a general 550k lexicon ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 57, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 69, "end": 75}]}]}
{"content": "The DNN utilizes five hidden layers , with 1024 neurons per layer , and a learning rate of 008 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 22, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "1024", "start": 43, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 74, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 29, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 48, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 83, "end": 86}]}]}
{"content": "The ReLU function is used as the activation function of neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 44, "end": 51}]}]}
{"content": "This DNN is trained for 35 epochs using 300 h of speech recordings ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 5, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 27, "end": 32}]}]}
{"content": "Recently , neural-network based approaches , in which words are embedded into a low-dimensional space , appeared and became to be used in lexical semantic tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural-network", "start": 11, "end": 24}]}]}
{"content": "Following recent advances in artificial neural network research , the recognizer employs parametric rectified linear units PReLU , word embeddings and character-level embeddings based on gated linear units GRU ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 29, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 89, "end": 98}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PReLU", "start": 123, "end": 127}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gated", "start": 187, "end": 191}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 206, "end": 208}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 100, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 110, "end": 115}, {"start": 193, "end": 198, "text": "linear"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 117, "end": 121}, {"start": 200, "end": 204, "text": "units"}]}]}
{"content": "LSTMs are specially shaped units of artificial neural networks designed to process whole sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LSTMs", "start": 0, "end": 4}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 36, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 54, "end": 61}]}]}
{"content": "Recently , a gated linear unit GRU was proposed by Sam as an alternative to LSTM , and was shown to have similar performance , while being less computationally demanding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gated", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 31, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 76, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 26, "end": 29}]}]}
{"content": "Instead regularized averaged perceptron , we use parametric rectified linear units , character-level embeddings and dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularized", "start": 8, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 49, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 116, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "averaged", "start": 20, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 29, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 60, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 70, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 77, "end": 81}]}]}
{"content": "The input layer is connected to a hidden layer of parametric rectified linear units and the hidden layer is connected to the output layer which is a softmax layer producing probability distribution for all possible named entity classes in BILOU encoding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 34, "end": 39}, {"start": 92, "end": 97, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 50, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 125, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 149, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 10, "end": 14}, {"start": 41, "end": 45, "text": "layer"}, {"start": 99, "end": 103, "text": "layer"}, {"start": 132, "end": 136, "text": "layer"}, {"start": 157, "end": 161, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 71, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 78, "end": 82}]}]}
{"content": "The network is trained with AdaGrad and we use dropout on the hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AdaGrad", "start": 28, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 47, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 69, "end": 73}]}]}
{"content": "We implemented our neural network in Torch7 , a scientific computing framework with wide support for machine learning algorithms ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 26, "end": 32}]}]}
{"content": "We tuned most of the hyperparameters on development portion of CNEC 1.0 and used them for all other corpora ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 21, "end": 35}]}]}
{"content": "Notably , we utilize window size W = 2 , hidden layer of 200 nodes , dropout 0.5 , minibatches of size 100 and learning rate 0.02 with decay ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 41, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 69, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "minibatches", "start": 83, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 111, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 48, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 54, "end": 55}, {"start": 95, "end": 96, "text": "of"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "200", "start": 57, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 61, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.5", "start": 77, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 98, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "100", "start": 103, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 120, "end": 123}]}]}
{"content": "All reported experiments use an ensemble of 5 networks , each using different random seed , with the resulting distributions being an average of individual networks distributions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 13, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 78, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "seed", "start": 85, "end": 88}]}]}
{"content": "A work most similar to ours , also proposed neural network architecture with word embeddings and character-level embeddings ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 51, "end": 57}]}]}
{"content": "The best published WER on CMUDict at present is demonstrated in using Long Short-term Memory Recurrent Neural Networks LSTM combined with a 5-gram graphone language model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 70, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 119, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-term", "start": 75, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 86, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 93, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 103, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 110, "end": 117}]}]}
{"content": "We also use the exact same split of 90 % training data and 10 % test data as in 9 , and thus our results are directly comparable to theirs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 41, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 64, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 50, "end": 53}, {"start": 69, "end": 72, "text": "data"}]}]}
{"content": "Hierarchical softmax and related procedures that involve decomposing the output layer into classes can help with this normalization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 13, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 80, "end": 84}]}]}
{"content": "We found that 15 classes optimized perplexity values for RNNLMs with 50 and 145 hidden nodes , and 18 classes optimized perplexity values for RNNLMs with 500 nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNLMs", "start": 57, "end": 62}, {"start": 142, "end": 147, "text": "RNNLMs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 87, "end": 91}]}]}
{"content": "The learning rate \u00ce\u00b7 adaptation scheme is managed by the adaptive gradient methods ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "adaptive", "start": 57, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 66, "end": 73}]}]}
{"content": "After optimizing on the development set , \u00ce\u00b7 was fixed to 0.1 and the dimensionality of the latent space C was fixed at 45 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 24, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 36, "end": 38}]}]}
{"content": "An RNNLM with 145 hidden nodes has about the same number of parameters as CDLM and performs 0.1 perplexity points worse than CDLM ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 3, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 18, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 60, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 25, "end": 29}]}]}
{"content": "Increasing the hidden units for RNNLM to 500 , we obtain the best performing RNNLM ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 15, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 32, "end": 36}, {"start": 77, "end": 81, "text": "RNNLM"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 22, "end": 26}]}]}
{"content": "To produce better performing LMs with fewer parameters we constructed an RNNLM with 50 hidden units , which when linearly combined with CDLM CDLM+RNNLM outperforms the best RNNLM using less than half as many parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 44, "end": 53}, {"start": 208, "end": 217, "text": "parameters"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 73, "end": 77}, {"start": 173, "end": 177, "text": "RNNLM"}]}]}
{"content": "The architecture for this kind of feature extraction consists of two NNs trained towards phonetic targets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNs", "start": 69, "end": 71}]}]}
{"content": "The first-stage NN has four hidden layers with 1500 units each except the BN layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN", "start": 16, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 28, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 35, "end": 40}]}]}
{"content": "BN layer\u00e2\u20ac\u2122s size is 80 neurons and it is the third hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "80", "start": 21, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 24, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 59, "end": 63}]}]}
{"content": "Linear regression is used on all single systems for arousal and all single systems for valence except for processing video geometric features , where neural network with one hidden layer is used topology: 948\u00e2\u20ac\u201c474\u00e2\u20ac\u201c3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Linear", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 150, "end": 155}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 174, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regression", "start": 7, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 157, "end": 163}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 181, "end": 185}]}]}
{"content": "We trained a neural network with one hidden layer with topology 945\u00e2\u20ac\u201c474\u00e2\u20ac\u201c3 in this case ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 37, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}]}]}
{"content": "Our approach uses a bidirectional recurrent neural network BRNN since speech has the form of a sequential signal with complex dependencies between the different time steps in both directions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 20, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 59, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "recurrent", "start": 34, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 51, "end": 57}]}]}
{"content": "Additionally we propose an alternative layout for the BRNN and show that it performs better on that specific task ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 54, "end": 57}]}]}
{"content": "Usage of deep bidirectional GRU layers can be found in the work of Amodei et al. where , for example , up to seven bidirectional GRU layers are used and even combined with up to three convolutional layers which altogether improved the performance of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 115, "end": 127}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 184, "end": 196}]}, {"label": ["I-DeepLearning"], "points": [{"text": "bidirectional", "start": 14, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "GRU", "start": 28, "end": 30}, {"start": 129, "end": 131, "text": "GRU"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 198, "end": 203}]}]}
{"content": "An approach for speech classification solely using Convolutional Neural Networks CNN instead of recurrent ones can be seen in the work of Milde and Biemann ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 51, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 81, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 65, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 72, "end": 79}]}]}
{"content": "GRU Networks are a variation of the long short term memory LSTM networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "long", "start": 36, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 59, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "short", "start": 41, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "term", "start": 47, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "memory", "start": 52, "end": 57}]}]}
{"content": "Conventional RNN structures propagate information only forwards in time ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 13, "end": 15}]}]}
{"content": "In this context , bidirectional RNNs can be helpful by having separate layers processing the two different directions and feeding each others output into the same output layer as it is depicted in Fig ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 18, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "RNNs", "start": 32, "end": 35}]}]}
{"content": "Here , the output of the forward layer is not directly propagated towards the output layer , instead it serves as the input of the backward layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "forward", "start": 25, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 78, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backward", "start": 131, "end": 138}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 33, "end": 37}, {"start": 85, "end": 89, "text": "layer"}, {"start": 140, "end": 144, "text": "layer"}]}]}
{"content": "Deep neural networks usually come with a large amount of parameters , leaving them prone to overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 57, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 92, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 12, "end": 19}]}]}
{"content": "One straightforward way of avoiding that is using Dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 50, "end": 56}]}]}
{"content": "Those are applied by multiplying the output of each node that propagates towards a dropout layer by some random noise with each batch of training data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 83, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "noise", "start": 112, "end": 116}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 128, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 91, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 134, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 137, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 146, "end": 149}]}]}
{"content": "The amount of deactivated nodes depends on the dropout rate p which can be seen as the distribution\u00e2\u20ac\u2122s parameter in the binary case ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 47, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 104, "end": 112}]}]}
{"content": "The positive effect of better generalization capabilities can be explained in two ways ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 30, "end": 43}]}]}
{"content": "First , it essentially produces an ensemble of neural networks and therefore producing an equally averaged result over those ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ensemble", "start": 35, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 44, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 54, "end": 61}]}]}
{"content": "The RNNs are built in Python using the Keras framework ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNs", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Python", "start": 22, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Keras", "start": 39, "end": 43}]}]}
{"content": "We use two GRU layers with 128 hidden states each ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 11, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 31, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 38, "end": 43}]}]}
{"content": "For the merged BRNN those are both connected to the input and combine their results to a dropout layer with p = 04 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 15, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 89, "end": 95}]}]}
{"content": "For the sequential BRNN the first GRU layer produces another time-dependent sequence which is then fed into the backward GRU layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequential", "start": 8, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 34, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backward", "start": 112, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "BRNN", "start": 19, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 38, "end": 42}, {"start": 125, "end": 129, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "GRU", "start": 121, "end": 123}]}]}
{"content": "After each of those follows one layer of dropout with p = 04 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 41, "end": 47}]}]}
{"content": "For both types of networks we finally use one to three fully connected dense layers with , again , 128 hidden states each , eventually followed by another single-state layer which produces the output by applying the sigmoid function which is also used as the inner activation of the GRU nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 55, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 103, "end": 108}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 216, "end": 222}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 283, "end": 285}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dense", "start": 71, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 110, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 287, "end": 291}]}]}
{"content": "The remaining activation functions between each two nodes are defined as the rectifier or relu function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 14, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "relu", "start": 90, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 25, "end": 33}]}]}
{"content": "As optimizer we use Adam ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "optimizer", "start": 3, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Adam", "start": 20, "end": 23}]}]}
{"content": "In earlier stages , RMSprop has also been tried , but it clearly proved to perform worse on that learning task and also oscillated a lot , making it useless for the early stopping described in Sect. 44 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "early", "start": 165, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "stopping", "start": 171, "end": 178}]}]}
{"content": "The loss which is minimized in the training stage is the binary cross-entropy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "binary", "start": 57, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-entropy", "start": 64, "end": 76}]}]}
{"content": "The model is then trained for a maximum of 50 epochs and evaluated on the remaining 900 samples of the development-test set after each iteration ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development-test", "start": 103, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "iteration", "start": 135, "end": 143}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 120, "end": 122}]}]}
{"content": "Training is stopped when the UAR of the validation set does not increase for 10 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 40, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 51, "end": 53}]}]}
{"content": "A model checkpoint is used to keep track of the weights which have been used to reach the highest UAR on the validation set up to the last epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 2, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 48, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "UAR", "start": 98, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 109, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 139, "end": 143}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 120, "end": 122}]}]}
{"content": "Finally we obtain a measure of general performance by using the highest scoring model to predict the labels on the testing set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 80, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 115, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 123, "end": 125}]}]}
{"content": "Also , the Gaussian dropout leads to slightly higher performance than the binary one ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Gaussian", "start": 11, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 20, "end": 26}]}]}
{"content": "In most cases the network with two fully connected layers after the recurrent ones achieves the highest measures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 35, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 41, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 51, "end": 56}]}]}
{"content": "Eventually we used the sequential BRNN with two dense layers and Gaussian dropout after being trained on the sets specified in Sect. 4.4 to predict the labels of the testing set and reached a UAR of 71.03 and an accuracy of 71.30 percent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequential", "start": 23, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 48, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Gaussian", "start": 65, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 166, "end": 172}]}, {"label": ["B-DeepLearning"], "points": [{"text": "UAR", "start": 192, "end": 194}]}, {"label": ["I-DeepLearning"], "points": [{"text": "BRNN", "start": 34, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 54, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 74, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 174, "end": 176}]}]}
{"content": "Also it became clear , that there exists a variant of BRNNs which has , to our knowledge , not been researched thoroughly yet ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNNs", "start": 54, "end": 58}]}]}
{"content": "The topic of the paper is the training of deep neural networks which use tunable piecewise-linear activation functions called maxout for speech recognition tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 42, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 81, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 126, "end": 131}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 54, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 98, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 109, "end": 117}]}]}
{"content": "Maxout networks are compared to the conventional fully-connected DNNs in case of training with both crossentropy and sequence discriminative sMBR criteria ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully-connected", "start": 49, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "crossentropy", "start": 100, "end": 111}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sequence", "start": 117, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 65, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "discriminative", "start": 126, "end": 139}]}]}
{"content": "The clear advantage of maxout networks over DNNs is demonstrated when using the cross-entropy criterion on both corpora ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 44, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 80, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 30, "end": 37}]}]}
{"content": "It is also argued that maxout networks are prone to overfitting during sequence training but in some cases it can be successfully overcome with the use of the KL-divergence based regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 52, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 179, "end": 192}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 30, "end": 37}]}]}
{"content": "The greedy layerwise pretraining method became an impulse for tempestuous development of DNN training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "greedy", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 89, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layerwise", "start": 11, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pretraining", "start": 21, "end": 31}]}]}
{"content": "The pretraining results in DNN weights initialization that facilitates the subsequent finetuning and improves its quality ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 27, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 31, "end": 37}]}]}
{"content": "Nevertheless , the fully connected feedforward deep neural networks hereinafter just DNNs for brevity still remain the workhorses of the large majority of ASR systems ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 19, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 35, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 85, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 25, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "deep", "start": 47, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 59, "end": 66}]}]}
{"content": "The introduction of piecewise-linear ReLU rectified linear units activation functions made it possible to simplify and improve the optimization process during DNN training significantly ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 20, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectified", "start": 42, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 65, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 159, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ReLU", "start": 37, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 59, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 76, "end": 84}]}]}
{"content": "It was shown that DNNs with ReLU activations may be successfully learned without layerwise pretraining ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 18, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 28, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "layerwise", "start": 81, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pretraining", "start": 91, "end": 101}]}]}
{"content": "However , the fact that ReLU and its analogues are linear almost everywhere may also result in overfitting and instability of the training process ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 24, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 95, "end": 105}]}]}
{"content": "This implies the necessity of using effective regularization techniques ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 46, "end": 59}]}]}
{"content": "Dropout can be treated as an approximate way to learn the exponentially large ensemble of different neural nets with subsequent averaging ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 100, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 107, "end": 110}]}]}
{"content": "To improve efficiency of the dropout regularization a new sort of tunable piecewise-linear activation function called maxout was proposed in 9 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 29, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 74, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 118, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 37, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 91, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 102, "end": 109}]}]}
{"content": "In a short time the deep maxout networks DMN were applied to speech recognition tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 41, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "maxout", "start": 25, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 32, "end": 39}]}]}
{"content": "It was also shown that dropout for DMNs can be very effective in an annealed mode , i.e. if the dropout rate gradually decreases epoch-by-epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 23, "end": 29}, {"start": 96, "end": 102, "text": "dropout"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 35, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch-by-epoch", "start": 129, "end": 142}]}]}
{"content": "We apply DMNs to two significantly different tasks and demonstrate their clear superiority over conventional DNNs under cross-entropy CE training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 109, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 120, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CE", "start": 134, "end": 135}]}]}
{"content": "Dropout proposed in is a regularization technique for deep feedforward network training which is effective in particular for training network with a large amount of parameters on a limited size dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 25, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 54, "end": 57}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 165, "end": 174}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 194, "end": 200}]}, {"label": ["I-DeepLearning"], "points": [{"text": "feedforward", "start": 59, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 71, "end": 77}]}]}
{"content": "In the original form of dropout it was proposed to randomly turn off half of neurons per every training example ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 24, "end": 30}]}]}
{"content": "When the neurons with piecewise-linear activation functions like ReLUx = max0 , x are used the input space is divided into multiple regions where data is linearly transformed by the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 22, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLUx", "start": 65, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 39, "end": 48}]}]}
{"content": "From this point of view using of dropout with piecewise-linear activation functions performs the more exact ensemble averaging than with activation functions of nonzero curvature such as sigmoid ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 33, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 46, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 137, "end": 146}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 187, "end": 193}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 63, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 148, "end": 156}]}]}
{"content": "The effectiveness of DNNs with ReLU neurons trained with dropout was demonstrated on many tasks from different domains ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 21, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 31, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 57, "end": 63}]}]}
{"content": "Maxout is a piecewise-linear activation function which was proposed to improve neural network training with dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 12, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 79, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 108, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 29, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 86, "end": 92}]}]}
{"content": "The advantage of the maxout activation function over ReLU is the possibility to adjust its form by means of parameters tuning although it comes at the cost of k-fold increasing of the parameters number ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 28, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 53, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 108, "end": 117}, {"start": 184, "end": 193, "text": "parameters"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "k-fold", "start": 159, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 39, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tuning", "start": 119, "end": 124}]}]}
{"content": "Maxout networks both fully-connected and convolutional demonstrated impressive results on several benchmark tasks from the computer vision domain ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 41, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}]}
{"content": "The first application of Deep Maxout Networks DMN to speech recognition task seems to be done in February and in March , where it was shown that training of DMNs can be effective even without using dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 25, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 46, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 157, "end": 160}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 198, "end": 204}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Maxout", "start": 30, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 37, "end": 44}]}]}
{"content": "Nevertheless , the training of DMNs can be successfully combined with dropout regularization as it was shown here ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 31, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 70, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 78, "end": 91}]}]}
{"content": "There it was proposed to use not the conventional dropout where the dropout rate is constant during the entire training but the annealed dropout AD which consists of the gradually decreasing dropout rate according to the linear schedule ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 50, "end": 56}, {"start": 68, "end": 74, "text": "dropout"}, {"start": 191, "end": 197, "text": "dropout"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "annealed", "start": 128, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 137, "end": 143}]}]}
{"content": "We do not compare Maxout + AD to ReLU + AD because , in our experience , AD training of ReLU networks does not provide significant WER reduction it is also observed in the previous paper whilist carefully tuned sigmoidal DNNs with L2 weight decay often outperform ReLU DNNs with dropout and other types of regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 18, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 33, "end": 36}, {"start": 88, "end": 91, "text": "ReLU"}, {"start": 264, "end": 267, "text": "ReLU"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoidal", "start": 211, "end": 219}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 231, "end": 232}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 269, "end": 272}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 279, "end": 285}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 306, "end": 319}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 221, "end": 224}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weight", "start": 234, "end": 239}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 241, "end": 245}]}]}
{"content": "When the model was trained the low-rank factorization based on SVD of the last hidden layer was performed and the model was fine-tuned to provide bottleneck features , hereinafter SDBNs Speaker-Dependent BottleNeck ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 9, "end": 13}, {"start": 114, "end": 118, "text": "model"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 79, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 86, "end": 90}]}]}
{"content": "In our first attempts we did not use regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 37, "end": 50}]}]}
{"content": "Since we observed that the cross-validation value of the sMBR criterion either increases or remains constant epoch-by-epoch we concluded that the model is overfit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 27, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch-by-epoch", "start": 109, "end": 122}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 146, "end": 150}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfit", "start": 155, "end": 161}]}]}
{"content": "So to improve ST performance an effective regularization is required ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 42, "end": 55}]}]}
{"content": "We tried to use the L1 and L2 penalty as well as dropout and F-smoothing to make ST work on Switchboard , however none of these approaches succeeded in overfitting reduction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "L1", "start": 20, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 27, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 49, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 152, "end": 162}]}]}
{"content": "We considered the use of maxout activation functions for training deep neural networks as acoustic models for ASR ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 66, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 32, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 43, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 71, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 78, "end": 85}]}]}
{"content": "Using two English speech corpora namely CHiME Challenge 2015 dataset and Switchboard we demonstrated that in case of training with the cross-entropy criterion Deep Maxout Networks DMN are superior to conventional fully-connected feedforward sigmoidal DNNs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 61, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 135, "end": 147}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 159, "end": 162}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 180, "end": 182}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 229, "end": 239}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Maxout", "start": 164, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 171, "end": 178}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sigmoidal", "start": 241, "end": 249}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 251, "end": 254}]}]}
{"content": "For the same layer sizes the number of DMN parameters is larger than that of DNN but the increase in DNN layer sizes is unable to provide the comparable accuracy gain ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "layer", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 39, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 43, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 77, "end": 79}, {"start": 101, "end": 103, "text": "DNN"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 19, "end": 23}, {"start": 111, "end": 115, "text": "sizes"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 105, "end": 109}]}]}
{"content": "We also found that sequence discriminative training of maxout networks is prone to overfitting which can be reduced with the use of KLD-regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequence", "start": 19, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 83, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "KLD-regularization", "start": 132, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "discriminative", "start": 28, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 62, "end": 69}]}]}
{"content": "The performance of the developed models was examined in terms of the Area Under the Curve AUC derived from the ROC curves as well as Pearson coefficient R between the predicted and the actual efficacy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 33, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 90, "end": 92}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ROC", "start": 111, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "curves", "start": 115, "end": 120}]}]}
{"content": "Associative Neural Networks ASNN approach and fragment descriptors were used to build the models ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Associative", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ASNN", "start": 28, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 90, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 12, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 19, "end": 26}]}]}
{"content": "In three layers neural networks , each neuron in the initial layer corresponded to one molecular descriptor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 53, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 23, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 61, "end": 65}]}]}
{"content": "Hidden layer contained from three to six neurons , whereas the output layer contained one for STL and FN or 11 MTL neurons , corresponding to the number of simultaneously treated properties ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Hidden", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "six", "start": 37, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 7, "end": 11}, {"start": 70, "end": 74, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 41, "end": 47}]}]}
{"content": "Each model was validated using external fivefold cross-validation procedure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 5, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fivefold", "start": 40, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 49, "end": 64}]}]}
{"content": "Supervised learning is usually achieved in what are called feed-forward NNs that process data in several layers consisting of varying numbers of nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feed-forward", "start": 59, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "NNs", "start": 72, "end": 74}]}]}
{"content": "These nodes are organized as input nodes , several layers of hidden nodes , and output nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 61, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 35, "end": 39}, {"start": 68, "end": 72, "text": "nodes"}, {"start": 87, "end": 91, "text": "nodes"}]}]}
{"content": "A sliding window covers 60 nucleotides , which is calculated to 240 input units to the neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 68, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 87, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 74, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 94, "end": 100}]}]}
{"content": "The neural network structure is a standard three layer feedforward neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 55, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}, {"start": 74, "end": 80, "text": "network"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 67, "end": 72}]}]}
{"content": "This kind of neural network has several names , such as multilayer perceptrons MLP , feed forward neural network , and backpropagation neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}, {"start": 135, "end": 140, "text": "neural"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 56, "end": 65}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 79, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feed", "start": 85, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 119, "end": 133}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}, {"start": 105, "end": 111, "text": "network"}, {"start": 142, "end": 148, "text": "network"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 67, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "forward", "start": 90, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 98, "end": 103}]}]}
{"content": "There are two output units corresponding to the donor and acceptor splice sites , 128 hidden layer units and 240 input units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 86, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 113, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 21, "end": 25}, {"start": 119, "end": 123, "text": "units"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 93, "end": 97}]}]}
{"content": "The 240 input units were used since the orthogonal input scheme uses four inputs each nucleotide in the window ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 8, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 14, "end": 18}]}]}
{"content": "The neural network program code was reused from a previous study , and in this code the number of hidden units was hard coded and optimized for 128 hidden units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 98, "end": 103}, {"start": 148, "end": 153, "text": "hidden"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 105, "end": 109}, {"start": 155, "end": 159, "text": "units"}]}]}
{"content": "There is also a bias signal added to the hidden layer and the output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 41, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 48, "end": 52}, {"start": 69, "end": 73, "text": "layer"}]}]}
{"content": "The activation function is a standard sigmoid function , shown in Eq. 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 4, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 38, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 15, "end": 22}]}]}
{"content": "The \u00ce\u00b2 values for the sigmoid functions are 0.1 for both the hidden layer activation and the output layer activation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 22, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 61, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 93, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 68, "end": 72}, {"start": 100, "end": 104, "text": "layer"}]}]}
{"content": "When doing forward calculations and backpropagation , the sigmoid function is called repetitively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 36, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 58, "end": 64}]}]}
{"content": "A fast and effective evaluation of the sigmoid function can improve the overall performance considerably ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 39, "end": 45}]}]}
{"content": "To improve the performance of the sigmoid function , a precalculated table for the exponential function is used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 34, "end": 40}]}]}
{"content": "There is no momentum used in the training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 12, "end": 19}]}]}
{"content": "We have not implemented any second order methods to help the convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 61, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 80, "end": 86}]}]}
{"content": "The neural network training is done using standard backpropagation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 51, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}]}
{"content": "The training was done in three sessions , and for each session we chose separate , but constant , learning rates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 98, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rates", "start": 107, "end": 111}]}]}
{"content": "The learning rate , \u00ce\u00b7 , was chosen to be 0.2 , 0.1 , and 0.02 , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}]}
{"content": "The shown indicators are computed using a neural network which has been trained for about 80 epochs , with a learning rate of 02 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 42, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 93, "end": 98}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 109, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 118, "end": 121}]}]}
{"content": "The best performing neural network , achieved a correlation coefficient of 0552 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 20, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 27, "end": 33}]}]}
{"content": "The MGN works in this case as a discrete time cellular neural network and the training is based on stochastic gradient descent as described in the paper ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 99, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 62, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 110, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 119, "end": 125}]}]}
{"content": "The training of a TPNN is based on a combination of stochastic gradient descend and back propagation with several improvements that make the training of the shared weights feasible and that were reported in detail in the context of training CNNs for pattern recognition ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 52, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "back", "start": 84, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 241, "end": 244}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 63, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descend", "start": 72, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "propagation", "start": 89, "end": 99}]}]}
{"content": "In stochastic gradient descent , the true gradient is approximated by the gradient of the loss function which is evaluated on a single training sample ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 3, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 90, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 14, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 23, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 95, "end": 102}]}]}
{"content": "Weight decay is a regularization method that penalizes large weights in the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Weight", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 18, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 61, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 7, "end": 11}]}]}
{"content": "The weight decay penalty term causes the insignificant weights to converge to zero ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 55, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 11, "end": 15}]}]}
{"content": "The parameter \u00c2\u00b5 is controlling the stepsize of the gradient descent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 4, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stepsize", "start": 36, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 52, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 61, "end": 67}]}]}
{"content": "The initial step size is already small around \u00c2\u00b5 = 0.01 and it is decreased after each training epoch with a constant factor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "step", "start": 12, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 96, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 17, "end": 20}]}]}
{"content": "This is necessary to achieve a slow convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 36, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 55, "end": 61}]}]}
{"content": "A sweep through the whole data set is called one epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 26, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 31, "end": 33}]}]}
{"content": "Up to 1000 epochs are necessary to train the TPNN in a typical experimental setup 20 amino acids in the alphabet , peptides of length 5 to 10 , 30-50 training samples from measurements as a starting set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 11, "end": 16}]}]}
{"content": "The slow convergence of the error is a consequence of the relative small stepsize in the gradient descent , but it is necessary in order to get overall convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 9, "end": 19}, {"start": 152, "end": 162, "text": "convergence"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stepsize", "start": 73, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 89, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 171, "end": 177}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 98, "end": 104}]}]}
{"content": "It is well known , that neural network ensembles perform better in terms of generalisation than single models would do ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalisation", "start": 76, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 103, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ensembles", "start": 39, "end": 47}]}]}
{"content": "An ensemble of TPNNs consists of several single TPNN models that are trained on randomly chosen subsets of the training data and the training starts with random weight initializations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 111, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 161, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 120, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "initializations", "start": 168, "end": 182}]}]}
{"content": "Two analytical models have been used to fit the data set to a prognostic index: a piecewise linear model Cox regression , also known as proportional hazards , and a flexible model consisting of Partial Logistic Artificial Neural Networks regularised with Automatic Relevance Determination PLANN-ARD ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 48, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 211, "end": 220}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 53, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 222, "end": 227}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 229, "end": 236}]}]}
{"content": "If these features are stored in a vector f = f1...fh , and if we represent the i-th residue in the sequence as ri , then f is obtained as: where N h is a non-linear function , which we implement by a two-layered feedforward Neural Network with h non-linear output units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 34, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 212, "end": 222}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 257, "end": 262}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 224, "end": 229}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 231, "end": 237}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 264, "end": 268}]}]}
{"content": "The number of free parameters in the overall N1-NN can be controlled by: the number of units in the hidden layer of the sequence-to-feature network N h N H f the number of hidden units in the feature-to-output network N o , N H o the number of hidden states in the feature vector f , which is also the number of output units in the sequence-to-feature network , Nf ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 19, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "N1-NN", "start": 45, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}, {"start": 172, "end": 177, "text": "hidden"}, {"start": 244, "end": 249, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 265, "end": 271}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 312, "end": 317}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 179, "end": 183}, {"start": 319, "end": 323, "text": "units"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 251, "end": 256}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vector", "start": 273, "end": 278}]}]}
{"content": "Each training is conducted by 10 fold-cross validation , i.e. 10 different sets of training runs are performed in which a different tenth of the overall set is reserved for testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fold-cross", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 44, "end": 53}]}]}
{"content": "The training set is used to learn the free parameters of the network by gradient descent , while the validation set is used to choose model and hyperparameters network size and architecture , i.e. N H f Nf and N H o ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 43, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 72, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 101, "end": 110}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 134, "end": 138}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 144, "end": 158}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 160, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 13, "end": 15}, {"start": 112, "end": 114, "text": "set"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 81, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 168, "end": 171}]}]}
{"content": "For each fold the three networks for the best architecture are ensemble averaged and evaluated on the corresponding test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 116, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 121, "end": 123}]}]}
{"content": "Training is performed by gradient descent on the error , which we model as the relative entropy between the target class and the output of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 25, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 34, "end": 40}]}]}
{"content": "The overall output of the network output layer of N o is implemented as a softmax function , while all internal squashing functions are implemented as hyperbolic tangents ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 74, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperbolic", "start": 151, "end": 160}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tangents", "start": 162, "end": 169}]}]}
{"content": "Training terminates when either the walltime on the server is reached 6 days for fungi and plants , 10 days for animals or the epoch limit is reached 40k for fungi and plants and 20k for animals ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 127, "end": 131}]}]}
{"content": "The gradient is updated 360 times for each epoch or once every 2-6 examples , depending on the set , and the examples are shuffled between epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 43, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 139, "end": 144}]}]}
{"content": "The learning rate is halved every time a reduction of the error is not observed for more than 50 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 97, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}]}
{"content": "Both predictors are assessed by ten-fold cross-validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ten-fold", "start": 32, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 41, "end": 56}]}]}
{"content": "The type of neural network with which we performed the experiments was Multilayer Perceptron MLP , because the results obtained with other types of networks were not satisfactory and they tended to be slower than MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 55, "end": 65}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 71, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 93, "end": 95}, {"start": 213, "end": 215, "text": "MLP"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 19, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 82, "end": 91}]}]}
{"content": "The preparatory steps for conducting the experiments consisted in determining: a the size of the training and testing set b the error function c the number of hidden units d the activation functions for the hidden and the output neurons e the minimum and maximum values for weight decay ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 41, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 110, "end": 116}]}, {"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 128, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 159, "end": 164}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 178, "end": 187}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 222, "end": 227}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 274, "end": 279}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 118, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 134, "end": 141}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 166, "end": 170}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 189, "end": 197}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 229, "end": 235}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 281, "end": 285}]}]}
{"content": "For the hidden neurons were used as activation functions identity , logistic , tanh and exponential and the same ones were chosen for the output neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 8, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 36, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tanh", "start": 79, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 138, "end": 143}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 15, "end": 21}, {"start": 145, "end": 151, "text": "neurons"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 47, "end": 55}]}]}
{"content": "The error functions used were sum of squares and cross entropy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sum", "start": 30, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 10, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 34, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "squares", "start": 37, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 55, "end": 61}]}]}
{"content": "The minimum and the maximum number of hidden units were chosen differently for each experiment because we conducted several experiments which had different number of inputs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 38, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 84, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 124, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 45, "end": 49}]}]}
{"content": "The larger the number of hidden units in a neural network model the stronger the model is , the more capable the network is to model complex relationships between the inputs and the target variables ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 43, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 58, "end": 62}, {"start": 81, "end": 85, "text": "model"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 50, "end": 56}]}]}
{"content": "The optimal number of hidden units is minimum 1/10 of the number of training cases and maximum 1/5 , but we have varied this interval ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 22, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 29, "end": 33}]}]}
{"content": "The use of decay weights for hidden layer and output layer was preferred in order to prevent overfitting , thereby potentially improving generalization performance of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 29, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 93, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 137, "end": 150}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 36, "end": 40}, {"start": 53, "end": 57, "text": "layer"}]}]}
{"content": "Weight decay or weight elimination are often used in MLP training and aim to minimize a cost function which penalizes large weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Weight", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 16, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 53, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cost", "start": 88, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 124, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 7, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 93, "end": 100}]}]}
{"content": "These techniques tend to result in networks with smaller weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 57, "end": 63}]}]}
{"content": "The minimum chosen weight decay was 0.0001 and the maximum 0001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 26, "end": 30}]}]}
{"content": "To perform the experiments the data were split in a training 67 % and a testing dataset 33 % ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 15, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 72, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 80, "end": 86}]}]}
{"content": "As error functions , we tested the cross entropy and the sum of squares ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 3, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 35, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sum", "start": 57, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 9, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 41, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 61, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "squares", "start": 64, "end": 70}]}]}
{"content": "The weight decays in both the hidden and the output layer varied between 0.0001 and 0001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 45, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decays", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 52, "end": 56}]}]}
{"content": "The values of the CV criterion Table 1 show a better performance in correspondence to a choice of 6 hidden units and decay parameter equal to 0.01 , that we therefore used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 117, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 123, "end": 131}]}]}
{"content": "Ten-fold cross-validation obtained by averaging over five fits for various values of the number of hidden units H and of the decay parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Ten-fold", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 125, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 9, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 131, "end": 139}]}]}
{"content": "Scheme of a multilayer perceptron with the layer of inputs x1...xp , t , the layer of hidden units z1...zH and the output layer , here represented by the unique response h ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 12, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 86, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 115, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 23, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 93, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 122, "end": 126}]}]}
{"content": "Additionally , one of the authors recently took advantage of a deep learning algorithm , built on a multilayer autoencoder neural network , that lead to interesting prediction results in reference ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 63, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "autoencoder", "start": 111, "end": 121}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 123, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 68, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 130, "end": 136}]}]}
{"content": "We use the July 2009 version of the datasets for analyzing and selecting hyper-parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 36, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 73, "end": 88}]}]}
{"content": "Autoencoder neural networks were trained using the free GPU-accelerated software package Torch7 using stochastic gradient descent with a learning rate of 0.01 for 25 iterations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Autoencoder", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "software", "start": 72, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 102, "end": 111}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 137, "end": 144}]}, {"label": ["B-DeepLearning"], "points": [{"text": "iterations", "start": 166, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 19, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "package", "start": 81, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Torch7", "start": 89, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 113, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 122, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 146, "end": 149}]}]}
{"content": "L2 regularization was used on all weights , which were initialized randomly from the uniform distribution ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 0, "end": 1}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 34, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 3, "end": 16}]}]}
{"content": "The hidden unit function is a Sigmoid ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sigmoid", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 11, "end": 14}]}]}
{"content": "The neural network is a modified version of General Regression Neural Network GRNN used as a classification tool ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 44, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 78, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 52, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 70, "end": 76}]}]}
{"content": "In order to perform the barcode sequences classification , we introduce a modified version of General Regression Neural Network GRNN that use , alternatively , a function derived from Jaccard distance and fractional distance instead of the euclidean one to compare learned prototypes against test sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 94, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 128, "end": 131}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 102, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 113, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 120, "end": 126}]}]}
{"content": "The proposed method is based on two modified versions of the General Regression Neural Network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 61, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 69, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 87, "end": 93}]}]}
{"content": "The General Regression Neural Network is a neural network created for regression i.e. the approximation of a dependent variable y given a set of sample x , y , where x is the independent variable ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 43, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regression", "start": 70, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 12, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 23, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 50, "end": 56}]}]}
{"content": "Classification results , in terms of accuracy , precision and recall scores , have been compared with both the GRNN algorithm using J-function and the SVM classifier ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 37, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 48, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recall", "start": 62, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 111, "end": 114}]}]}
{"content": "Simple spiking neural network models such as Integrate and fire models without bio-realistic features were simulated in the older generation GPUs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["I-DeepLearning"], "points": [{"text": "spiking", "start": 7, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 15, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 22, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 30, "end": 35}]}]}
{"content": "All above described methods require to set two hyper-parameters: \u00ce\u00bb and \u00ce\u00b1 controlling the sparsity and the network influence , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters:", "start": 47, "end": 63}]}]}
{"content": "Deep learning neural networks are capable to extract significant features from raw data , and to use these features for classification tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 5, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 21, "end": 28}]}]}
{"content": "In this work we present a deep learning neural network for DNA sequence classification based on spectral sequence representation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 26, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 31, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 47, "end": 53}]}]}
{"content": "The framework is tested on a dataset of 16S genes and its performances , in terms of accuracy and F1 score , are compared to the General Regression Neural Network , already tested on a similar problem , as well as naive Bayes , random forest and support vector machine classifiers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 29, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 85, "end": 92}]}, {"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 129, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 137, "end": 146}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 148, "end": 153}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 155, "end": 161}]}]}
{"content": "The obtained results demonstrate that the deep learning approach outperformed all the other classifiers when considering classification of small sequence fragment 500 bp long ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 42, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 47, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "approach", "start": 56, "end": 63}]}]}
{"content": "Among the deep learning architecture , it is usually comprised the LeNet-5 network , or Convolutional Neural Network CNN , a neural network that is inspired by the visual system\u00e2\u20ac\u2122s structure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 10, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 67, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 88, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 117, "end": 119}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 125, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 15, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 102, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 109, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 132, "end": 138}]}]}
{"content": "In this work we want to understand if the convolutional network is capable to identify and to use these features for sequence classification , outperforming the classifiers proposed in the past ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 42, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 56, "end": 62}]}]}
{"content": "The one used in this work is a modified version of the LeNet-5 network introduced by LeCun et al. in and it is implemented using the python Theano package for deep learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 55, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 159, "end": 162}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 164, "end": 171}]}]}
{"content": "The modified LeNet-5 proposed network is made of two lower layers of convolutional and max-pooling processing elements , followed by two traditional fully connected Multi Layer Perceptron MLP processing layers , so that there are 6 processing layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 13, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 69, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 87, "end": 97}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 149, "end": 153}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multi", "start": 165, "end": 169}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 188, "end": 190}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 155, "end": 163}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Layer", "start": 171, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 177, "end": 186}]}]}
{"content": "The max-pooling is a non-linear down-sampling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 4, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "down-sampling", "start": 32, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 46, "end": 50}]}]}
{"content": "The first layer of convolutional kernels , named kernel 0 , is made of L = 10 kernels of dimension 5 so that n = 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 19, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 49, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 78, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 33, "end": 39}]}]}
{"content": "From a spectral representation vector made of 1024 components this layer produces 10 vectors of 1024 dimensions that the pooling layer reduces to the 10 feature maps of 510 dimensions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 31, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vectors", "start": 85, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pooling", "start": 121, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 129, "end": 133}]}]}
{"content": "These vectors are the input for the second convolutional layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vectors", "start": 6, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 43, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 57, "end": 61}]}]}
{"content": "The second layer of kernel kernel 1 is made of L = 20 kernels of dimension 5 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "layer", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 27, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 54, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 17, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernel", "start": 20, "end": 25}]}]}
{"content": "In both cases the max-pooling layer has dimension 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 18, "end": 28}]}]}
{"content": "Convolution and maxpooling are usually considered together and they are represented in the lower part of Fig. 2 as two highly connected blocks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolution", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxpooling", "start": 16, "end": 25}]}]}
{"content": "The two upper level layers corresponds to a traditional fully-connected MLP: the first layer of the MLP operates on the total number of output from the lower level the output is flattened to a 1-D vector and the total number of units in the Hidden Layer is 500 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP:", "start": 72, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 100, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "1-D", "start": 193, "end": 195}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Hidden", "start": 241, "end": 246}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vector", "start": 197, "end": 202}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Layer", "start": 248, "end": 252}]}]}
{"content": "In the second case , the ten-fold cross validation scheme was repeated considering as test set the sequence fragments of shorter size , 500 bp long , obtained randomly extracting 500 consecutive nucleotides from the original full length sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 34, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 40, "end": 49}]}]}
{"content": "The CNN 134 has been run considering two different kernels sizes: kernel 0 = kernel 1 = 5 in the first run kernel 0 = 25 , kernel 1 = 15 in the second run ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 51, "end": 57}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 66, "end": 71}, {"start": 77, "end": 82, "text": "kernel"}, {"start": 107, "end": 112, "text": "kernel"}, {"start": 123, "end": 128, "text": "kernel"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes:", "start": 59, "end": 64}]}]}
{"content": "In both configurations the training phase has been run for 200 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 27, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 36, "end": 40}]}]}
{"content": "The spectral representation is obtained as k-mers frequencies along the sequences the CNN belongs to the so called deep learning algorithms ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 86, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 115, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 120, "end": 127}]}]}
{"content": "We designed and tested some topic modeling techniques , and we took advantage of a deep neural network approach ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 83, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 88, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 95, "end": 101}]}]}
{"content": "A convolutional neural net learns to classify patches as salient long looks or not ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 2, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 23, "end": 25}]}]}
{"content": "Two output neurons were connected to the reinitialized layer , then training followed on augmented 800 \u00c3\u2014 800 px patches for 10000 iterations in Caffe ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "800", "start": 99, "end": 101}]}, {"label": ["B-DeepLearning"], "points": [{"text": "iterations", "start": 131, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 103, "end": 104}]}, {"label": ["I-DeepLearning"], "points": [{"text": "800", "start": 106, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "px", "start": 110, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "patches", "start": 113, "end": 119}]}]}
{"content": "Arguments of rkhs$new. define initial values of the functions and the initial value of the l 2 norm weighting parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "l", "start": 91, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "2", "start": 93, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "norm", "start": 95, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weighting", "start": 100, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 110, "end": 118}]}]}
{"content": "When new data is added , the ANN needs to be retrained in order to achieve good performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 29, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "retrained", "start": 45, "end": 53}]}]}
{"content": "The set of selected features are processed to accomplish the voxel classification by means of a Multilevel Artificial Neural Network MANN , which assures various computational advantages ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multilevel", "start": 96, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MANN", "start": 133, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Artificial", "start": 107, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 118, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 125, "end": 131}]}]}
{"content": "Then we discuss the issues concerning the minimum requirements that an artificial neural network ANN should fulfill in order that it would be capable of expressing the categories and mappings between them , underlying the MES ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 71, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 97, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 82, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 89, "end": 95}]}]}
{"content": "First generation networks studied the McCulloch-Pitts neuron , or perceptrons , using digital inputs and outputs , usually binary ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "perceptrons", "start": 66, "end": 76}]}]}
{"content": "Multi-layer perceptrons with a single hidden layer were able to compute any binary function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multi-layer", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 38, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 12, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 45, "end": 49}]}]}
{"content": "Second generation networks had neurons with activation functions , usually some logistic function like the sigmoid , with continuous inputs and outputs , having probabilistic weights in the synapses , and learning algorithms based on gradient descent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 44, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "logistic", "start": 80, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 107, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 175, "end": 181}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 234, "end": 241}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 55, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 89, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 243, "end": 249}]}]}
{"content": "Third generation networks are based on spiking or pulsating neurons , that incorporate recent neurobiology discoveries , modeling the neuron as a dynamic electrical system with bifurcation in its equilibrium , usually following some variant of the Hodgkin-Huxley model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "spiking", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "or", "start": 47, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pulsating", "start": 50, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 60, "end": 66}]}]}
{"content": "The network was trained with a 100-sentence database and was generated by a Kohonen network with 60 x 60 neurons and a decreasing neighborhood ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "100-sentence", "start": 31, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "60", "start": 97, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "was", "start": 12, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "trained", "start": 16, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 44, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "x", "start": 100, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "60", "start": 102, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 105, "end": 111}]}]}
{"content": "It is needed for instance , when channels used in training and testing have different lengths or if the duration of training and testing simulation is different for some reason ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 50, "end": 57}, {"start": 116, "end": 123, "text": "training"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 59, "end": 61}, {"start": 125, "end": 127, "text": "and"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 63, "end": 69}, {"start": 129, "end": 135, "text": "testing"}]}]}
{"content": "This gives an accuracy rate of 0627 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 14, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 23, "end": 26}]}]}
{"content": "The accuracy of the system is calculated as the total number of correctly classified objects 1094 divided by the total number of objects 1627 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 4, "end": 11}]}]}
{"content": "Neural networks are generally better at discriminating between classes , as is shown here ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "discriminating", "start": 40, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "between", "start": 55, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "classes", "start": 63, "end": 69}]}]}
{"content": "The whole data set was divided into different parts in order to get different validation data sets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 10, "end": 13}, {"start": 89, "end": 92, "text": "data"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 15, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 94, "end": 97}]}]}
{"content": "EXP-1 , which contains all data from the first clinical experiment with 2500 measurements ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "contains", "start": 14, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "clinical", "start": 47, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "all", "start": 23, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 27, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "experiment", "start": 56, "end": 65}]}]}
{"content": "Accuracy is computed as the percentage of cases correctly classified ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Accuracy", "start": 0, "end": 7}]}]}
{"content": "If yes , with probability 0.76 this is classified as a cancerous regions , otherwise , the region is certainly cancerous ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "classified", "start": 39, "end": 48}]}]}
{"content": "Results show that on both data sets the certainty for the correctly classified lesions is higher lower entropy than for the incorrectly classified lesions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 26, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 31, "end": 34}]}]}
{"content": "According to [ 25 ] , artificial neural network research evolved into three generations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 22, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 33, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 40, "end": 46}]}]}
{"content": "Under normal circumstances , however , accuracy is determined by the error rate of the inferred rule ie , the percentage of actions incorrectly classified as either legitimate or illegitimate calculated over the entire uniformly-sampled set of action possibilities within the a priori motor space ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 39, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 69, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "uniformly-sampled", "start": 219, "end": 235}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 75, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 237, "end": 239}]}]}
{"content": "As a result , the data set is optimally classified into two classes , targets and non targets following a validation process , and an average accuracy metric is then calculated ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 18, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "classified", "start": 40, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 142, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 23, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "into", "start": 51, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "two", "start": 56, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "classes", "start": 60, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "metric", "start": 151, "end": 156}]}]}
{"content": "This set was divided into 2 groups: training 320 thermographies in each class and testing 80 thermographies in reach class ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "set", "start": 5, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "was", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "divided", "start": 13, "end": 19}]}]}
{"content": "Therefore , a total of 2.411 images of healthy patients and 534 images of diseased patients were used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 29, "end": 34}, {"start": 64, "end": 69, "text": "images"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "healthy", "start": 39, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 47, "end": 54}, {"start": 83, "end": 90, "text": "patients"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "diseased", "start": 74, "end": 81}]}]}
{"content": "A balanced randomized selection was performed on 500 healthy patients and 500 sick patients ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "healthy", "start": 53, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 61, "end": 68}, {"start": 83, "end": 90, "text": "patients"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sick", "start": 78, "end": 81}]}]}
{"content": "80% 400 Healthy and 400 Sick were allocated to training and testing , that is , the processes of extracting the attributes and optimizing the weights of the filters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Healthy", "start": 8, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sick", "start": 24, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 47, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 142, "end": 148}]}, {"label": ["B-DeepLearning"], "points": [{"text": "filters", "start": 157, "end": 163}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 56, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 60, "end": 66}]}]}
{"content": "20% 100 thermographies in each class were reserved for blind validation and establishing the final predictive accuracy of these architectures using a database of images that was not used for learning training and testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "reserved", "start": 42, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 110, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 162, "end": 167}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 200, "end": 207}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 51, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "blind", "start": 55, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 61, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 209, "end": 211}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 213, "end": 219}]}]}
{"content": "Data for the neural network comes from a simulation experiment ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Data", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 52, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 5, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}]}]}
{"content": "Before giving more in depth description of these data , it is good to acknowledge that the input of a neural network consists of several cell positions and output is the velocity of that cell at a particular time ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 91, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 97, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "a", "start": 100, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 102, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 109, "end": 115}]}]}
{"content": "Data for this study were recorded every 0.001 s once in 1000 simulation steps , which results in almost 9 200 records for each cell ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Data", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recorded", "start": 25, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "simulation", "start": 61, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "records", "start": 110, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 5, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "this", "start": 9, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "study", "start": 14, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "every", "start": 34, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.001", "start": 40, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "s", "start": 46, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "steps", "start": 72, "end": 76}]}]}
{"content": "Training and testing sets used for neural network are extracted from simulations which differ only in initial seeding of the cells ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Training", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 35, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 102, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 42, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "seeding", "start": 110, "end": 116}]}]}
{"content": "As was already mentioned , the input for the neural network are the positions of the cell center and the output is the velocity of this cell at given position ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 31, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 37, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 41, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 45, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 52, "end": 58}]}]}
{"content": "The learning set for our neural network consist of the simulation output which also includes redundant data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "redundant", "start": 93, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 13, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 17, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "our", "start": 21, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 25, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 32, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 103, "end": 106}]}]}
{"content": "We processed the data in the following steps ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "processed", "start": 3, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 13, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 17, "end": 20}]}]}
{"content": "We loaded the data from the experiments and checked their correctness ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 28, "end": 38}]}]}
{"content": "Next step was the normalization of the data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "normalization", "start": 18, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 32, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 35, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 39, "end": 42}]}]}
{"content": "Normalization of data is common in neural networks. ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Normalization", "start": 0, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 14, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 17, "end": 20}]}]}
{"content": "We made the desired pairs of the inputs and the outputs from the normalized data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "normalized", "start": 65, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 76, "end": 79}]}]}
{"content": "We work with two types of the input tensor in the form of a 3\u00e2\u02c6\u2019dimensional array ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 36, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "in", "start": 43, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 46, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "form", "start": 50, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 55, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "a", "start": 58, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "3\u00e2\u02c6\u2019dimensional", "start": 60, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "array", "start": 76, "end": 80}]}]}
{"content": "The horizontal layers of the tensor represent different cells , where the cell we are focused on is on the top of the tensor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 29, "end": 34}, {"start": 118, "end": 123, "text": "tensor"}]}]}
{"content": "For our tensor we used zero padding of width p ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 8, "end": 13}]}]}
{"content": "Thus the dimension of this tensor is: ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dimension", "start": 9, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 19, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "this", "start": 22, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensor", "start": 27, "end": 32}]}]}
{"content": "This input type for the neural network set by parameter called spatial tensor is inspired by the image processing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 46, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "spatial", "start": 63, "end": 69}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 97, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensor", "start": 71, "end": 76}]}]}
{"content": "We obtain a small tensor A by determining if the cell is present or not , in each of the disc x \u00c3\u2014 disc y \u00c3\u2014 disc z channel areas ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 18, "end": 23}]}]}
{"content": "Since this input is orthogonal the neuron networks gives us more accurate output values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neuron", "start": 35, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "is", "start": 17, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "orthogonal", "start": 20, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 42, "end": 49}]}]}
{"content": "Each tensor records the position of the centre of one cell ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 5, "end": 10}]}]}
{"content": "Then the input tensor consists of small tensors An...A1 , Bn...B1 in this order ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 9, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tensors", "start": 40, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensor", "start": 15, "end": 20}]}]}
{"content": "The small tensor An is at the top horizontal layer of the input tensor and B1 is at the bottom layer of the input tensor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 10, "end": 15}, {"start": 64, "end": 69, "text": "tensor"}, {"start": 114, "end": 119, "text": "tensor"}]}]}
{"content": "Therefore the dimension of the input tensor is: ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dimension", "start": 14, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 24, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 27, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 31, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensor", "start": 37, "end": 42}]}]}
{"content": "We also used the gaussian kernel ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gaussian", "start": 17, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernel", "start": 26, "end": 31}]}]}
{"content": "Hence , the dimension of the input tensor for the neural network was 16 \u00c3\u2014 16 \u00c3\u2014 24 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensor", "start": 35, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 57, "end": 63}]}]}
{"content": "Data for the input tensors are selected as follows ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Data", "start": 0, "end": 3}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 5, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 13, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tensors", "start": 19, "end": 25}]}]}
{"content": "Besides using convolution or fully connected layers , we also used a relatively new type of layers , the dense convolution layers , introduced in [ 7 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 14, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 105, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 35, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 45, "end": 50}, {"start": 123, "end": 128, "text": "layers"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 111, "end": 121}]}]}
{"content": "There were 6 different architectures of neural networks , see the Table 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 47, "end": 54}]}]}
{"content": "First four experiments , named Experiment 0 to Experiment 3 have no spatial tensor input with the same parameters , but mutually different networks architectures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 11, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Experiment", "start": 31, "end": 40}, {"start": 47, "end": 56, "text": "Experiment"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 76, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 103, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "networks", "start": 139, "end": 146}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architectures", "start": 148, "end": 160}]}]}
{"content": "The network architectures for experiment pairs 2 and 4 and for 3 and 5 are the same ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 30, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architectures", "start": 12, "end": 24}]}]}
{"content": "Comparing the difference between the target and resulted values given from neural networks , we observed that networks with spatial tensor parameter Experiments 4 to 7 give better results than for no spatial tensor parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 75, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tensor", "start": 132, "end": 137}, {"start": 208, "end": 213, "text": "tensor"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 139, "end": 147}, {"start": 215, "end": 223, "text": "parameter"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Experiments", "start": 149, "end": 159}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 82, "end": 89}]}]}
{"content": "The Experiment 7 , where the network has the most layers gave us the best result ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Experiment", "start": 4, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "where", "start": 19, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 25, "end": 27}, {"start": 41, "end": 43, "text": "the"}, {"start": 65, "end": 67, "text": "the"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 29, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "has", "start": 37, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "most", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gave", "start": 57, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "us", "start": 62, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "best", "start": 69, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "result", "start": 74, "end": 79}]}]}
{"content": "In this section we report on a set of experiments performed on a large data set of nanopore FASTQ files ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 38, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 71, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 76, "end": 78}]}]}
{"content": "The data set is comprised of 336 different files , with sizes ranging from 7.2 KB to 3.5 GB , including reads that are up to hundreds of thousands base-pair long ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 9, "end": 11}]}]}
{"content": "The total size of the data set amounts to 114.2 GB and the dynamic range of quality scores is 7 bits ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 22, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 27, "end": 29}]}]}
{"content": "Among these methods , the credibility of the back propagation neural network is up to 98%. ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "credibility", "start": 26, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 38, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 41, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "back", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "propagation", "start": 50, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 69, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "is", "start": 77, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "up", "start": 80, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "to", "start": 83, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "98%.", "start": 86, "end": 89}]}]}
{"content": "To address the two issues , in this paper we propose to use a deep ResNet structure with Convolutional Block Attention Module CBAM , in order to extract richer and finer features from pathological images ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 67, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 89, "end": 101}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CBAM", "start": 126, "end": 129}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 197, "end": 202}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Block", "start": 103, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Attention", "start": 109, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Module", "start": 119, "end": 124}]}]}
{"content": "Existing deep learning approaches for Breast Cancer BC histology images classification task include cell nuclei segmentation [ 1415 ] and the patch-wise classification [ 1216 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 65, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 14, "end": 21}]}]}
{"content": "In order to classify the BC pathological images steadily and accurately , in this paper , we adopt an improved ResNet architecture to extract local and global features from pathological images and perform end-to-end training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 41, "end": 46}, {"start": 186, "end": 191, "text": "images"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 111, "end": 116}]}]}
{"content": "ResNet [ 17 ] is a recently popular CNN architecture , it prevents vanishing gradient by using the residual connection , which allows the network architecture to be deeper to obtain richer and more abstract features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 36, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 138, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architecture", "start": 146, "end": 157}]}]}
{"content": "To this end , we use Convolutional Block Attention Module CBAM [ 18 ] to enhance the performance of the ResNet ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 21, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CBAM", "start": 58, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 104, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Block", "start": 35, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Attention", "start": 41, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Module", "start": 51, "end": 56}]}]}
{"content": "To evaluate the effectiveness of our method , we conduct a series of experiments on the publicly available BreakHis dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 69, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BreakHis", "start": 107, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 116, "end": 122}]}]}
{"content": "The BreakHis dataset , a benchmark proposed by [ 11 ] for the BC histological images classification , consists of 7909 breast histopathological images from 82 patients ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BreakHis", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 78, "end": 83}, {"start": 144, "end": 149, "text": "images"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 159, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 13, "end": 19}]}]}
{"content": "We use the deeper ResNet CNN architecture to extract richer and more abstract features of patients\u00e2\u20ac\u2122 BC tissue ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 18, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 25, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients\u00e2\u20ac\u2122", "start": 90, "end": 100}]}]}
{"content": "We use CABM to refine the tissue features extracted from each layer of ResNet , which can improve the classification performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CABM", "start": 7, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 71, "end": 76}]}]}
{"content": "We have significantly improved the accuracy of classification on the publicly available BreakHis Dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BreakHis", "start": 88, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Dataset", "start": 97, "end": 103}]}]}
{"content": "Fortunately , Breast cancer histopathology database provided 7909 2480 benign and 5429 malignant samples microscopic images of breast tumor which collected from 82 patients by surgery [ 11 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Breast", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 117, "end": 122}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 164, "end": 171}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cancer", "start": 21, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "histopathology", "start": 28, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 43, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "provided", "start": 52, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "7909", "start": 61, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "2480", "start": 66, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "benign", "start": 71, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 78, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5429", "start": 82, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "malignant", "start": 87, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "samples", "start": 97, "end": 103}]}]}
{"content": "AlexNet is a typical deep learning model , which [ 22 ] achieves a winning top-5 test error rate of 15.3% , which is 10.9% lower than the second one who uses SIFT [ 23 ] and FVS [ 24 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 21, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 81, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 26, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 35, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "error", "start": 86, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 92, "end": 95}]}]}
{"content": "The Breast Cancer Histopathological Image Classification BreakHis dataset composes of 7909 microscopic images of breast tumor tissue ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Image", "start": 36, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 66, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 103, "end": 108}]}]}
{"content": "They are collected from 82 patients in different magnifying factors include 40\u00c3\u2014 , 100\u00c3\u2014 , 200\u00c3\u2014 , and 400\u00c3\u2014 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 27, "end": 34}]}]}
{"content": "The BreakHis dataset contains 2480 benign and 5429 malignant samples 700 \u00c3\u2014 460 pixels , 3-channel RGB , 8-bit depth in each channel , PNG format ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BreakHis", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2480", "start": 30, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "benign", "start": 35, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 42, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5429", "start": 46, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "malignant", "start": 51, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "samples", "start": 61, "end": 67}]}]}
{"content": "In this work , we use the ResNet with 50 layers , which can be represented by ResNet-50 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ResNet", "start": 26, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ResNet-50", "start": 78, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "with", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "50", "start": 38, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 41, "end": 46}]}]}
{"content": "Then we add CBAM to each block of the ResNet-50 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CBAM", "start": 12, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ResNet-50", "start": 38, "end": 46}]}]}
{"content": "The original top layer of ResNet-50 is replaced by a global average-pooling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ResNet-50", "start": 26, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "average-pooling", "start": 60, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 76, "end": 80}]}]}
{"content": "A dropout of 0.3 is also used after the fully connected layer for helping reduce overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 2, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 40, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 81, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 10, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.3", "start": 13, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 46, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 56, "end": 60}]}]}
{"content": "The loss is categorical cross-entropy and the optimizer is Stochastic Gradient Descent SGD ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "optimizer", "start": 46, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Stochastic", "start": 59, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SGD", "start": 87, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "is", "start": 9, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "categorical", "start": 12, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-entropy", "start": 24, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Gradient", "start": 70, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Descent", "start": 79, "end": 85}]}]}
{"content": "We set the learning rate at 0001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 11, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 20, "end": 23}]}]}
{"content": "We pre-train our model on the ImageNet dataset , and then fine-tune our model on the BreakHis dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-train", "start": 3, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 17, "end": 21}, {"start": 72, "end": 76, "text": "model"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ImageNet", "start": 30, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BreakHis", "start": 85, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 39, "end": 45}, {"start": 94, "end": 100, "text": "dataset"}]}]}
{"content": "We retrospectively evaluate our model on the data of 142 patients with 305 lesions and 70 no-lesion volumes , and it significantly outperforms the comparison methods with the sensitivity of 92.1% with 1.92 false positives per volume ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 32, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 57, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 175, "end": 185}]}]}
{"content": "The model is based on the standard UNet segmentation architecture with a down-sampling and upsampling path , where all the simply stacked convolutional layers are replaced with residual blocks to prevent gradient vanishing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "UNet", "start": 35, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 138, "end": 150}]}, {"label": ["B-DeepLearning"], "points": [{"text": "residual", "start": 177, "end": 184}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 204, "end": 211}]}, {"label": ["I-DeepLearning"], "points": [{"text": "segmentation", "start": 40, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architecture", "start": 53, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 152, "end": 157}]}, {"label": ["I-DeepLearning"], "points": [{"text": "blocks", "start": 186, "end": 191}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vanishing", "start": 213, "end": 221}]}]}
{"content": "We combine spatial information into different layers of UNet to reduce the false positive rate ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "UNet", "start": 56, "end": 59}]}]}
{"content": "Finally , considering the blurred boundaries between lesion and no-lesions , we adopt the sub-hard mining strategy in loss function , which only computes the loss of the specific samples to update the parameter of the network , in order to ensure the network learn correct information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 118, "end": 121}, {"start": 158, "end": 161, "text": "loss"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 201, "end": 209}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 123, "end": 130}]}]}
{"content": "To validate the effectiveness of the proposed framework , we conduct extensive experiments on an ABUS dataset of 142 patients\u00e2\u20ac\u2122 images with 375 volumes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 79, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ABUS", "start": 97, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patients\u00e2\u20ac\u2122", "start": 117, "end": 127}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 129, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 102, "end": 108}]}]}
{"content": "We introduce the spatial information into different layers of the segmentation network , which significantly reduce the false positive rate ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "spatial", "start": 17, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "reduce", "start": 109, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "information", "start": 25, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "into", "start": 37, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "different", "start": 42, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 116, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "false", "start": 120, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "positive", "start": 126, "end": 133}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 135, "end": 138}]}]}
{"content": "Our method is based on UNet ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "UNet", "start": 23, "end": 26}]}]}
{"content": "The image resolution reduces by half after max pooling along the downsampling path , while the resolution doubles via deconvolution operation along the upsampling path ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 43, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "downsampling", "start": 65, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deconvolution", "start": 118, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "upsampling", "start": 152, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "path", "start": 78, "end": 81}, {"start": 163, "end": 166, "text": "path"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "operation", "start": 132, "end": 140}]}]}
{"content": "Finally , a softmax layer is used to transform the result into a two-class problem ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 12, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 20, "end": 24}]}]}
{"content": "We improve UNet by replacing the simply stacked convolutional layers with residual blocks to prevent vanishing gradient ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "UNet", "start": 11, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 48, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "residual", "start": 74, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vanishing", "start": 101, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "blocks", "start": 83, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 111, "end": 118}]}]}
{"content": "Furthermore , the attention skip connection is included to make the model pay more attention to useful spatial areas and improve the ability of localization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "attention", "start": 18, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 68, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "skip", "start": 28, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connection", "start": 33, "end": 42}]}]}
{"content": "As is shown in Fig. 3 , it contains two 3 \u00c3\u2014 3 \u00c3\u2014 3 convolutional layers with stride 1 , and the same padding is used to ensure the output has the same size with the input ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3", "start": 40, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 78, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "same", "start": 97, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 42, "end": 43}, {"start": 47, "end": 48, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "3", "start": 45, "end": 45}, {"start": 50, "end": 50, "text": "3"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolutional", "start": 52, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "padding", "start": 102, "end": 108}]}]}
{"content": "The reception field of two successive 3 \u00c3\u2014 3 \u00c3\u2014 3 convolutional layers is the same with that of a 5\u00c3\u20145\u00c3\u20145 convolutional layer but with fewer parameters to be computed ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3", "start": 38, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "5\u00c3\u20145\u00c3\u20145", "start": 98, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 141, "end": 150}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 40, "end": 41}, {"start": 45, "end": 46, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "3", "start": 43, "end": 43}, {"start": 48, "end": 48, "text": "3"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolutional", "start": 50, "end": 62}, {"start": 106, "end": 118, "text": "convolutional"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 64, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 120, "end": 124}]}]}
{"content": "Since the extreme imbalance between normal regions and lesion regions , the model tends to predict most areas as normal regions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 76, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tends", "start": 82, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "to", "start": 88, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "predict", "start": 91, "end": 97}]}]}
{"content": "To normally represent the actual shape and size of lesions , we firstly normalize the pixel spacing of the ABUS volume in each direction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "normalize", "start": 72, "end": 80}]}]}
{"content": "Our proposed framework is implemented with PyTorch and trained on NVIDIA Tesla M40 GPU ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "PyTorch", "start": 43, "end": 49}]}]}
{"content": "During training , in order to alleviate the computational complexity in each batch and increase the augmentation , we randomly crop the ABUS images into small patches with the size of 160 \u00c3\u2014 80 \u00c3\u2014 160 , 70% of which are lesion-centered ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 141, "end": 146}]}, {"label": ["B-DeepLearning"], "points": [{"text": "patches", "start": 159, "end": 165}]}, {"label": ["I-DeepLearning"], "points": [{"text": "with", "start": 167, "end": 170}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 172, "end": 174}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 176, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 181, "end": 182}]}, {"label": ["I-DeepLearning"], "points": [{"text": "160", "start": 184, "end": 186}, {"start": 197, "end": 199, "text": "160"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 188, "end": 189}, {"start": 194, "end": 195, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "80", "start": 191, "end": 192}]}]}
{"content": "In addition , we adopt various data augmentations , e.g. , contrast adjustment , rotation , cropping , flipping , especially the elastic transform [ 26 ] , which are widely used in small medical image dataset to increase the data diversity ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 31, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 195, "end": 199}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentations", "start": 36, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 201, "end": 207}]}]}
{"content": "Adam optimizer is used to train the whole network , and the learning rate is set as 1e-4 , and training is stopped when the model has similar performance on training and validation dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Adam", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 60, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 124, "end": 128}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 170, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "optimizer", "start": 5, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 69, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 181, "end": 187}]}]}
{"content": "Then the skip attention is used to identify the image regions and prune the irrelevant feature responses so as to preserve only the activation relevant to this segmentation task ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 48, "end": 52}]}]}
{"content": "The architecture of AssCNV23\u00e2\u20ac\u2122s network contains four convolutional layers intersected with three max pooling layers , three fully connected layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 55, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 99, "end": 101}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 126, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 69, "end": 74}, {"start": 111, "end": 116, "text": "layers"}, {"start": 142, "end": 147, "text": "layers"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 103, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 132, "end": 140}]}]}
{"content": "The detected results were compared to the benchmark data , and the performance of each tool was evaluated by Precision Pre , Sensitivity Sen , F1-score F1 and Matthews correlation coefficient MCC ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Precision", "start": 109, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sensitivity", "start": 125, "end": 135}]}, {"label": ["B-DeepLearning"], "points": [{"text": "F1-score", "start": 143, "end": 150}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Matthews", "start": 159, "end": 166}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MCC", "start": 192, "end": 194}]}, {"label": ["I-DeepLearning"], "points": [{"text": "correlation", "start": 168, "end": 178}]}, {"label": ["I-DeepLearning"], "points": [{"text": "coefficient", "start": 180, "end": 190}]}]}
{"content": "They used Gold-standard dataset , SCOP version 1.75 and 6SSE , 5SSE , 4SSE and 3SSE [ 10 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Gold-standard", "start": 10, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 24, "end": 30}]}]}
{"content": "Indeed , this work will focus on the development of a diagnosis support system , in terms of its knowledge representation and reasoning procedures , under a formal framework based on Logic Programming , complemented with an approach to computing centered on Artificial Neural Networks , to evaluate ACS predisposing and the respective Degree-of-Confidence that one has on such a happening ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "diagnosis", "start": 54, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 258, "end": 267}]}, {"label": ["I-DeepLearning"], "points": [{"text": "support", "start": 64, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "system", "start": 72, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 269, "end": 274}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 276, "end": 283}]}]}
{"content": "In this study 627 patients admitted on the emergency department were considered , during the period of three months from February to May of 2013 , with suspect of ACS ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "patients", "start": 18, "end": 25}]}]}
{"content": "The gender distribution was 48% and 52% for female and male , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gender", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "48%", "start": 28, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "distribution", "start": 11, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 32, "end": 34}, {"start": 51, "end": 53, "text": "and"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "52%", "start": 36, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 40, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "female", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "male", "start": 55, "end": 58}]}]}
{"content": "Seventeen variables were selected allowing one to have a multivariable dataset with 627 records ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 71, "end": 77}]}]}
{"content": "To ensure statistical significance of the attained results , 20 twenty experiments were applied in all tests ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "20", "start": 61, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "twenty", "start": 64, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "experiments", "start": 71, "end": 81}]}]}
{"content": "In each simulation , the available data was randomly divided into two mutually exclusive partitions , i.e. , the training set with 67% of the available data and , the test set with the remaining 33% of the cases ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 113, "end": 120}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 167, "end": 170}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 122, "end": 124}, {"start": 172, "end": 174, "text": "set"}]}]}
{"content": "In the other layers we used the sigmoid function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 32, "end": 38}]}]}
{"content": "As the output function in the preprocessing layer it was used the identity one ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 7, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "identity", "start": 66, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 14, "end": 21}]}]}
{"content": "Table 2 shows that the model accuracy was 96.9% for the training set 410 correctly classified in 423 and 94.6% for test set 193 correctly classified in 204 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 23, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 29, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 56, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 65, "end": 67}]}]}
{"content": "Thus , the predictions made by the ANN model are satisfactory , attaining accuracies close to 95% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 35, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracies", "start": 74, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 39, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "close", "start": 85, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "to", "start": 91, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "95%", "start": 94, "end": 96}]}]}
{"content": "Huang and Liao [ 45 ] introduced a comprehensive study to investigate the capability of the probabilistic neural networks PNN associated with a feature selection method , the so-called signal-to-noise statistic , in cancer classification ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "probabilistic", "start": 92, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PNN", "start": 122, "end": 124}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cancer", "start": 216, "end": 221}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 106, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 113, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "classification", "start": 223, "end": 236}]}]}
{"content": "As an illustrated in Figure 1.6a , the entire data-set of all 88 experiments was first quality filtered 1 and then the dimensionality was further reduced by principal component analysis PCA to 10 PC projections 2 , from the original 6567 expression values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 65, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dimensionality", "start": 119, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "principal", "start": 157, "end": 165}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PCA", "start": 186, "end": 188}]}, {"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 193, "end": 194}]}, {"label": ["I-DeepLearning"], "points": [{"text": "was", "start": 134, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "further", "start": 138, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "reduced", "start": 146, "end": 152}]}, {"label": ["I-DeepLearning"], "points": [{"text": "component", "start": 167, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "analysis", "start": 177, "end": 184}]}, {"label": ["I-DeepLearning"], "points": [{"text": "PC", "start": 196, "end": 197}]}, {"label": ["I-DeepLearning"], "points": [{"text": "projections", "start": 199, "end": 209}]}]}
{"content": "Next , the 25 test experiments were set aside and the 63 training experiments were randomly partitioned into 3 groups 3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 14, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 57, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "experiments", "start": 19, "end": 29}, {"start": 66, "end": 76, "text": "experiments"}]}]}
{"content": "ANN models were then calibrated using for each sample the 10 PC values as input and the cancer category as output 5 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 58, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cancer", "start": 88, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 4, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "PC", "start": 61, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "values", "start": 64, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "as", "start": 71, "end": 72}, {"start": 104, "end": 105, "text": "as"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 74, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "category", "start": 95, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 107, "end": 112}]}]}
{"content": "For each model , the calibration was optimized with 100 iterative cycles epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 9, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 73, "end": 78}]}]}
{"content": "In addition , there was no sign of over-training: if the models begin to learn features in the training set , which are not present in the validation set , this would result in an increase in the error for the validation at that point and the curves would no longer remain parallel ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "over-training:", "start": 35, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 95, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 139, "end": 148}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 104, "end": 106}, {"start": 150, "end": 152, "text": "set"}]}]}
{"content": "A standard approach to neural network training is the use of backpropagation to optimize the weight assignments for a fixed neural network topology ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 61, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 93, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "assignments", "start": 100, "end": 110}]}]}
{"content": "Gene selection using Feed Forward Back Propagation Neural Network as a classifier is illustrated in Figure 17 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Feed", "start": 21, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Forward", "start": 26, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Back", "start": 34, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Propagation", "start": 39, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 58, "end": 64}]}]}
{"content": "The new technique is based on Switching Neural Networks SNN , learning machines that assign a relevance value to each input variable , and adopts Recursive Feature Addition RFA for performing gene selection ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Switching", "start": 30, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SNN", "start": 56, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 47, "end": 54}]}]}
{"content": "After that , authors classify the microarray data sets with a Fuzzy Neural Network FNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 45, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Fuzzy", "start": 62, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "FNN", "start": 83, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 50, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 68, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 75, "end": 81}]}]}
{"content": "Recently , Marylyn et al. [ 74 ] took a serious attempt to introduce a genetic programming neural network GPNN as a method for optimizing the architecture of a neural network to improve the identification of genetic and gene-environment combinations associated with a disease risk ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "genetic", "start": 71, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 106, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 160, "end": 165}]}, {"label": ["I-DeepLearning"], "points": [{"text": "programming", "start": 79, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 91, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 98, "end": 104}, {"start": 167, "end": 173, "text": "network"}]}]}
{"content": "This empirical studies suggest GPNN has excellent power for identifying gene-gene and gene-environment interactions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 31, "end": 34}]}]}
{"content": "Using simulated dataauthors show that GPNN has higher power to identify gene-gene and gene-environment interactions than SLR and CART ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 38, "end": 41}]}]}
{"content": "These include an independent variable input set , a list of mathematical functions , a fitness function , and finally the operating parameters of the GP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fitness", "start": 87, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 132, "end": 141}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 95, "end": 102}]}]}
{"content": "These operating parameters include number of demes or populations , population size , number of generations , reproduction rate , crossover rate , mutation rate , and migration [ 90 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 16, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 35, "end": 40}, {"start": 86, "end": 91, "text": "number"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "population", "start": 68, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "reproduction", "start": 110, "end": 121}]}, {"label": ["B-DeepLearning"], "points": [{"text": "crossover", "start": 130, "end": 138}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mutation", "start": 147, "end": 154}]}, {"label": ["B-DeepLearning"], "points": [{"text": "migration", "start": 167, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 42, "end": 43}, {"start": 93, "end": 94, "text": "of"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "demes", "start": 45, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "or", "start": 51, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "populations", "start": 54, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 79, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "generations", "start": 96, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 123, "end": 126}, {"start": 140, "end": 143, "text": "rate"}, {"start": 156, "end": 159, "text": "rate"}]}]}
{"content": "Here , we will train the GPNN on 9/10 of the data to develop an NN model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "train", "start": 15, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN", "start": 64, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 21, "end": 23}, {"start": 41, "end": 43, "text": "the"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "GPNN", "start": 25, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "on", "start": 30, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "9/10", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 38, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 67, "end": 71}]}]}
{"content": "Another work introduced by Alison et al [ 74 ] which developed a grammatical evolution neural network GENN approach that accounts for the drawbacks of GPNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "grammatical", "start": 65, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GENN", "start": 102, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 151, "end": 154}]}, {"label": ["I-DeepLearning"], "points": [{"text": "evolution", "start": 77, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 87, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 94, "end": 100}]}]}
{"content": "They also , compare the performance of GENN to GPNN , a traditional Back-Propagation Neural Network BPNN and a random search algorithm ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GENN", "start": 39, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 47, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Back-Propagation", "start": 68, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BPNN", "start": 100, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 111, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 85, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 92, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "search", "start": 118, "end": 123}]}]}
{"content": "GENN outperforms both BPNN and the random search , and performs at least as well as GPNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GENN", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BPNN", "start": 22, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 35, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GPNN", "start": 84, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "search", "start": 42, "end": 47}]}]}
{"content": "Liang and Kelemen [ 60 ] proposed a Time Lagged Recurrent Neural Network with trajectory learning for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Time", "start": 36, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 215, "end": 225}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Lagged", "start": 41, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 48, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 58, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 65, "end": 71}]}]}
{"content": "We used a fuzzy neural network FNN proposed earlier for cancer classification ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fuzzy", "start": 10, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "FNN", "start": 31, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cancer", "start": 56, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 23, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "classification", "start": 63, "end": 76}]}]}
{"content": "In this work we used three well-known microarray databases , i.e. , the lymphoma data set , the small round blue cell tumor SRBCT data set , and the ovarian cancer data set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "microarray", "start": 38, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "lymphoma", "start": 72, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "small", "start": 96, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ovarian", "start": 149, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "databases", "start": 49, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 81, "end": 84}, {"start": 130, "end": 133, "text": "data"}, {"start": 164, "end": 167, "text": "data"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 86, "end": 88}, {"start": 135, "end": 137, "text": "set"}, {"start": 169, "end": 171, "text": "set"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "round", "start": 102, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "blue", "start": 108, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cell", "start": 113, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tumor", "start": 118, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "SRBCT", "start": 124, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cancer", "start": 157, "end": 162}]}]}
{"content": "Our result shows the FNN classifier not only improves the accuracy of cancer classification problem but also helps biologists to find a better relationship between important genes and development of cancers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "FNN", "start": 21, "end": 23}]}]}
{"content": "An adaptive learning rate provides the network with higher convergence speed and learning performance , i.e. , accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "adaptive", "start": 3, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 59, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 12, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 21, "end": 24}]}]}
{"content": "Here we use a heuristic for tuning the learning rate \u00ce\u00b7 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 39, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 48, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00ce\u00b7", "start": 53, "end": 54}]}]}
{"content": "If the error undergoes five consecutive reductions , increase \u00ce\u00b7 by 5% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "increase", "start": 53, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00ce\u00b7", "start": 62, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "by", "start": 65, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5%", "start": 68, "end": 69}]}]}
{"content": "If the error undergoes three consecutive combinations of one increase and one reduction , decrease \u00ce\u00b7 by 5% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "decrease", "start": 90, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00ce\u00b7", "start": 99, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "by", "start": 102, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5%", "start": 105, "end": 106}]}]}
{"content": "Since we dynamically update the learning rate , initial value for \u00ce\u00b7 becomes insignificant as long as it is not too large ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 32, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "\u00ce\u00b7", "start": 66, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 41, "end": 44}]}]}
{"content": "For the SRBCT data , the FNN needs only 8 genes to obtain the same accuracy , i.e. , 100% , while the well-known evolutionary algorithm reported by Deutsch [ 8 ] used 12 genes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "FNN", "start": 25, "end": 27}]}]}
{"content": "The network configuration , as shown in Figure 4 , is composed of two layers one hidden and one output ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "composed", "start": 54, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one", "start": 77, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "configuration", "start": 12, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 63, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "two", "start": 66, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 70, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 81, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 88, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "one", "start": 92, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 96, "end": 101}]}]}
{"content": "The hidden layer consistes on five neurons , while the output layer has only one neuron ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "five", "start": 30, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one", "start": 77, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 11, "end": 15}, {"start": 62, "end": 66, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 35, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neuron", "start": 81, "end": 86}]}]}
{"content": "Sigmoid type of neurons have been employed in both layers , the hidden and the output layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Sigmoid", "start": 0, "end": 6}]}]}
{"content": "The algorithm used for training process was the Scaled Conjugate Gradient and performance is optimized by the function Cross-Entropy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Scaled", "start": 48, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Cross-Entropy", "start": 119, "end": 131}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Conjugate", "start": 55, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Gradient", "start": 65, "end": 72}]}]}
{"content": "In this paper we describe use of neural network for ECG signal prediction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 33, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 40, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 48, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ECG", "start": 52, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "signal", "start": 56, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "prediction", "start": 63, "end": 72}]}]}
{"content": "A training signal figure 4 , red part consists of 620 samples ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "620", "start": 50, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "samples", "start": 54, "end": 60}]}]}
{"content": "The test signal is also 10 second long 1000 samples ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "1000", "start": 39, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "samples", "start": 44, "end": 50}]}]}
{"content": "We used the log-sigmoid transfer function logsig in each layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "log-sigmoid", "start": 12, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "logsig", "start": 42, "end": 47}]}]}
{"content": "The learning rate was set to value 0.05 and the tolerable error to 510-5 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}]}
{"content": "The said algorithm is based on Levenberg-Marquardt optimization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Levenberg-Marquardt", "start": 31, "end": 49}]}]}
{"content": "In this study a novel neural network-based method for peptide identification is proposed ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 22, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "peptide", "start": 54, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network-based", "start": 29, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "identification", "start": 62, "end": 75}]}]}
{"content": "The presented here Qual method uses for classification purposes a fully connected , feed-forward multilayer neural network [ 6 ] with 6 input nodes corresponding to the spectral features , one hidden layer , and one output node ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 66, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feed-forward", "start": 84, "end": 95}]}, {"label": ["B-DeepLearning"], "points": [{"text": "6", "start": 134, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one", "start": 189, "end": 191}, {"start": 212, "end": 214, "text": "one"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 72, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "multilayer", "start": 97, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 108, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 115, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 136, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 142, "end": 146}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 193, "end": 198}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 200, "end": 204}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 216, "end": 221}]}, {"label": ["I-DeepLearning"], "points": [{"text": "node", "start": 223, "end": 226}]}]}
{"content": "The number of nodes in the hidden layer was determined by cross-validation during training with the back-propagation Leveberg-Marquadt algorithm ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "back-propagation", "start": 100, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Leveberg-Marquadt", "start": 117, "end": 133}]}]}
{"content": "The final neural network model included a hidden layer with 8 nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 10, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 42, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 25, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "with", "start": 55, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "8", "start": 60, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 62, "end": 66}]}]}
{"content": "The training set consisted of 184 653 tandem spectra acquired with an high resolution LTQ\u00e2\u20ac\u201c FTICR mass spectrometer and searched against a target/decoy database with the X!Tandem engine [ 3 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 13, "end": 15}]}]}
{"content": "The expected neural network output values for positive and negative examples were equal to 1 and 0 , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 28, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "values", "start": 35, "end": 40}]}]}
{"content": "To recognize the performed activity the backpropagation neural network with one hidden layer was used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 40, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one", "start": 76, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 56, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 63, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 87, "end": 91}]}]}
{"content": "We conduct experiments on the newly proposed COSMIC CNA dataset , which contains 25 types of cancer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 11, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "COSMIC", "start": 45, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "CNA", "start": 52, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 56, "end": 62}]}]}
{"content": "We obtained 4731 corn seed RGB images consisting of 952 haploid and 3779 diploid seeds from several different proprietary maize inbred lines ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RGB", "start": 27, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "images", "start": 31, "end": 36}]}]}
{"content": "We train our network using the image dataset that was randomly split into 4021 training 809 haploid and 3212 diploid seeds and 710 test 143 haploids and 567 diploids images with 20% haploids in both sets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 31, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 166, "end": 171}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 37, "end": 43}]}]}
{"content": "Input images of the corn seeds are convolved with 16 filter kernels in each convolutional layer , followed by two fully connected layers and an output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 6, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "filter", "start": 53, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 76, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 114, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 60, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 90, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 120, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 130, "end": 135}]}]}
{"content": "In this paper , we propose a Convolutional neural network CNN as a tool to improve efficiency and accuracy of Osteosarcoma tumor classification into tumor classes viable tumor , necrosis vs non-tumor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 29, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 58, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 98, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 43, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 50, "end": 56}]}]}
{"content": "The proposed CNN architecture contains five learned layers: three convolutional layers interspersed with max pooling layers for feature extraction and two fully-connected layers with data augmentation strategies to boost performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 13, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "five", "start": 39, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "three", "start": 60, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 105, "end": 107}]}, {"label": ["B-DeepLearning"], "points": [{"text": "two", "start": 151, "end": 153}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 183, "end": 186}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learned", "start": 44, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers:", "start": 52, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolutional", "start": 66, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 80, "end": 85}, {"start": 117, "end": 122, "text": "layers"}, {"start": 171, "end": 176, "text": "layers"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 109, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "fully-connected", "start": 155, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 188, "end": 199}]}]}
{"content": "The convolution filters are applied to small patches of the input image to detect and extract image features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 4, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 66, "end": 70}, {"start": 94, "end": 98, "text": "image"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "filters", "start": 16, "end": 22}]}]}
{"content": "Our neural network architecture combines features of AlexNet [ 9 ] and LeNet [ 10 ] to develop a fast and accurate slide classification system ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 53, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LeNet", "start": 71, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}]}
{"content": "The goal of this paper is to utilize CNN to identify the four regions of interest Fig. 2 , namely , 1 Viable tumor , 2 Coagulative necrosis , 3 fibrosis or osteoid , and 4 Non tumor Bone , cartilage ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 37, "end": 39}]}]}
{"content": "Spanhol et al. [ 15 ] developed on existing AlexNet for different segmentation and classification tasks in breast cancer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 44, "end": 50}]}]}
{"content": "In this paper , we propose a deep learning approach capable of assigning tumor classes viable tumor , necrosis vs non-tumor directly to input slides in osteosarcoma , a type of cancer with significantly more variability in tumor description ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 29, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 34, "end": 41}]}]}
{"content": "We extend the successful Alexnet proposed by Krizhevsky see [ 9 ] and LeNet network architectures introduced by LeCun see [ 10 ] which uses gradient based learning with back propogation algorithm ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Alexnet", "start": 25, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LeNet", "start": 70, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 76, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 140, "end": 147}]}, {"label": ["B-DeepLearning"], "points": [{"text": "back", "start": 169, "end": 172}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architectures", "start": 84, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "based", "start": 149, "end": 153}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 155, "end": 162}]}, {"label": ["I-DeepLearning"], "points": [{"text": "propogation", "start": 174, "end": 184}]}]}
{"content": "The typical CNN architecture for image classification consists of a series of convolution filters paired with pooling layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 12, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 33, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 78, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pooling", "start": 110, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "filters", "start": 90, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 118, "end": 123}]}]}
{"content": "We develop on existing proven networks LeNet and AlexNet because finding a successful network configuration for a given problem can be a difficult challenge given the total number of possible configurations that can be defined ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LeNet", "start": 39, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 49, "end": 55}]}]}
{"content": "The data augmentation methods to reduce over-fitting on image data as described by Krizhevsky [ 9 ] has been proclaimed for its success rate in various object recognition applications ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 40, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 56, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 9, "end": 20}]}]}
{"content": "We start with a simple 3 layer network INPUT - CONVOLUTION - MAX POOL - MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3", "start": 23, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "INPUT", "start": 39, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 25, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "-", "start": 45, "end": 45}, {"start": 59, "end": 59, "text": "-"}, {"start": 70, "end": 70, "text": "-"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "CONVOLUTION", "start": 47, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "MAX", "start": 61, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "POOL", "start": 65, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "MLP", "start": 72, "end": 74}]}]}
{"content": "INPUT 128 \u00c3\u2014 128 \u00c3\u2014 3 will hold the raw pixel values of the image , i.e. an image of width 128 , height 128 , and with three color channels RGB ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "INPUT", "start": 0, "end": 4}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 60, "end": 64}, {"start": 76, "end": 80, "text": "image"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "128", "start": 6, "end": 8}, {"start": 13, "end": 15, "text": "128"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 10, "end": 11}, {"start": 17, "end": 18, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "3", "start": 20, "end": 20}]}]}
{"content": "This may result in volume such as 124 \u00c3\u2014 124 \u00c3\u2014 4 for 4 filters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "124", "start": 34, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "filters", "start": 56, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 38, "end": 39}, {"start": 45, "end": 46, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "124", "start": 41, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "4", "start": 48, "end": 48}]}]}
{"content": "MAX POOL layer will down-sample along the spatial dimensions width , height , resulting in volume 62 \u00c3\u2014 62 \u00c3\u2014 4 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MAX", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "62", "start": 98, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "POOL", "start": 4, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 9, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 101, "end": 102}, {"start": 107, "end": 108, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "62", "start": 104, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "4", "start": 110, "end": 110}]}]}
{"content": "MLP layer will compute the class scores , resulting in volume of size 1 \u00c3\u2014 1 \u00c3\u2014 4 , where each of the 4 numbers correspond to a class score for the 4 tumor regions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "1", "start": 70, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 4, "end": 8}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 72, "end": 73}, {"start": 77, "end": 78, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "1", "start": 75, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "4", "start": 80, "end": 80}]}]}
{"content": "The different layers in the network are 3 Convolution layer C , 3 Sub-Sampling layer P , and 2 fully connected multi-level perceptrons M ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolution", "start": 42, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sub-Sampling", "start": 66, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 95, "end": 99}]}, {"label": ["B-DeepLearning"], "points": [{"text": "multi-level", "start": 111, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 54, "end": 58}, {"start": 79, "end": 83, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 101, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 123, "end": 133}]}]}
{"content": "We worked with different number of hidden layers to define the best output in terms of tumor identification and computational resources needed see Table 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 35, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 42, "end": 47}]}]}
{"content": "The detailed architecture of the five level CNN for tumor classification is shown in Fig. 3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 44, "end": 46}]}]}
{"content": "Our architecture combines the simplicity of Lenet architecture with the data augmentation methods used by AlexNet architecture ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Lenet", "start": 44, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 72, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 106, "end": 112}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 77, "end": 88}]}]}
{"content": "The lower 3 layers are comprised of alternating convolution and max-pooling layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 48, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 60, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "max-pooling", "start": 64, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 76, "end": 81}]}]}
{"content": "The first convolution layer has filter size 5 \u00c3\u2014 5 used to detect low level features like edges which is followed by a max pooling layer of scale 2 to down-sample the data. ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 10, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "filter", "start": 32, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 119, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 22, "end": 26}, {"start": 131, "end": 135, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 39, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5", "start": 44, "end": 44}, {"start": 49, "end": 49, "text": "5"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 46, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 123, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 137, "end": 138}]}, {"label": ["I-DeepLearning"], "points": [{"text": "scale", "start": 140, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "2", "start": 146, "end": 146}]}]}
{"content": "This data is then sent to second layer of 5 \u00c3\u2014 5 filters to detect higher order features like texture and spatial connectivity followed by a max-pooling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "5", "start": 42, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 141, "end": 151}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 44, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "5", "start": 47, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "filters", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 153, "end": 157}]}]}
{"content": "The last convolution layer uses a filter of size 3 \u00c3\u2014 3 and max-pooling size 2 for down- sampling to generate more higher order features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 9, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "filter", "start": 34, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 60, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 21, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 41, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 44, "end": 47}, {"start": 72, "end": 75, "text": "size"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "3", "start": 49, "end": 49}, {"start": 54, "end": 54, "text": "3"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 51, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "2", "start": 77, "end": 77}]}]}
{"content": "The upper 2 layers are fully-connected multi-level perceptron MLP neural network hidden layer + logistic regression ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully-connected", "start": 23, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 62, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 66, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 81, "end": 86}]}, {"label": ["B-DeepLearning"], "points": [{"text": "logistic", "start": 96, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "multi-level", "start": 39, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 51, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 73, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 88, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regression", "start": 105, "end": 114}]}]}
{"content": "The second layer of the MLP is the output layer consisting of four neurons see Table 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 24, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 35, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "four", "start": 62, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 42, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 67, "end": 73}]}]}
{"content": "The sum of the output probabilities from the MLP is 1 , ensured by the use of Softmax algorithm as the activation function in the output layer of the MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 45, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Softmax", "start": 78, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 103, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 130, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 114, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 137, "end": 141}]}]}
{"content": "The convolution and max pooling layers are feature extractors and the MLP is the classifier ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 4, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 20, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 70, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 24, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 32, "end": 37}]}]}
{"content": "The easiest and most common method to reduce overfitting of data is to artificially augment the dataset using label-preserving transformations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 45, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 96, "end": 102}]}]}
{"content": "We use two distinct data augmentation techniques both of which allow transformed images to be produced from the original images with very little computation , so the transformed images do not need to be stored on disk ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 81, "end": 86}, {"start": 121, "end": 126, "text": "images"}, {"start": 178, "end": 183, "text": "images"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 25, "end": 36}]}]}
{"content": "The second technique for data augmentation alters the intensities of the RGB channels in training images [ 9 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 25, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 89, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 30, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "images", "start": 98, "end": 103}]}]}
{"content": "We perform Principal component analysis PCA on the set of RGB pixel values throughout the training set and then , for each training image , we add the following quantity to each RGB image pixel i.e. , Ixy = [ IR xy , IG xy , IB xy ] T : [ p1 , p2 , p3}{\u00ce\u00b11\u00ce\u00bb1 , \u00ce\u00b12\u00ce\u00bb2 , \u00ce\u00b13\u00ce\u00bb3 ] T where pi and \u00ce\u00bbi are the i-th eigenvector and eigenvalue of the 3 \u00c3\u2014 3 covariance matrix of RGB pixel values , respectively , and \u00ce\u00b1i is a random variable drawn from a Gaussian with mean 0 and standard deviation 01 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Principal", "start": 11, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PCA", "start": 40, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 90, "end": 97}, {"start": 123, "end": 130, "text": "training"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RGB", "start": 178, "end": 180}]}, {"label": ["I-DeepLearning"], "points": [{"text": "component", "start": 21, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "analysis", "start": 31, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 99, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "image", "start": 132, "end": 136}, {"start": 182, "end": 186, "text": "image"}]}]}
{"content": "Data augmentation helps alleviate over-fitting by considerably increasing the amount of training data , removing rotation dependency and making the training images invariant to changes in the color brightness and intensity through PCA ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Data", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 34, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 88, "end": 95}, {"start": 148, "end": 155, "text": "training"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "augmentation", "start": 5, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 97, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "images", "start": 157, "end": 162}]}]}
{"content": "The network is trained with stochastic gradient descent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 28, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 39, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 48, "end": 54}]}]}
{"content": "We initialized all weights with 0 mean by assigning them small , random and unique numbers from 10\u00e2\u02c6\u20192 standard deviation Gaussian random numbers , so that each layer calculates unique updates and integrate themselves as different units of the full network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 19, "end": 25}]}]}
{"content": "Each case consists of an average of 25 individual svs images representing different sections of the microscopic slide ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 54, "end": 59}]}]}
{"content": "The 256 \u00c3\u2014 256 patches limited the CNN due to memory issues and the 64 \u00c3\u2014 64 patch size had very low accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "256", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 35, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "64", "start": 68, "end": 69}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 101, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 8, "end": 9}, {"start": 71, "end": 72, "text": "\u00c3\u2014"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "256", "start": 11, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "patches", "start": 15, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "64", "start": 74, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "patch", "start": 77, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 83, "end": 86}]}]}
{"content": "Hence we decided on a 128 \u00c3\u2014 128 patch size ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "128", "start": 22, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "\u00c3\u2014", "start": 26, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "128", "start": 29, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "patch", "start": 33, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 39, "end": 42}]}]}
{"content": "This resulted in about 5000 image patches in the dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 28, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 49, "end": 55}]}]}
{"content": "Only 60% patches were used for training , and 20% data was used as validation set , the remaining 20% data was use for test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 67, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 119, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 78, "end": 80}, {"start": 124, "end": 126, "text": "set"}]}]}
{"content": "The architecture was developed in JAVA using dl4j deep learning for java libraries [ 1 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dl4j", "start": 45, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 50, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 55, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "for", "start": 64, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "java", "start": 68, "end": 71}]}]}
{"content": "The training data was fed to the network in batch sizes of 100 to utilize parallelism and improve the network efficiency ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 44, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 50, "end": 54}]}]}
{"content": "The objective of the network was to classify the input images tiles into one of the four regions viable tumor , coagulative necrosis , osteoid or fibrosis , non-tumor mentioned before ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "images", "start": 55, "end": 60}]}]}
{"content": "The performance of the neural network was monitored by assessing the error rate on the validation set , once the error rate saturated after 10 epochs , training was stopped ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 69, "end": 73}, {"start": 113, "end": 117, "text": "error"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 87, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 143, "end": 148}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 75, "end": 78}, {"start": 119, "end": 122, "text": "rate"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 98, "end": 100}]}]}
{"content": "Our implementation gives F1-score of 0.86 and an accuracy of 084 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "F1-score", "start": 25, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 49, "end": 56}]}]}
{"content": "In this section we present and compare the qualitative output of three architectures: AlexNet , Lenet , and our proposed architecture ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 86, "end": 92}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Lenet", "start": 96, "end": 100}]}]}
{"content": "We find that the running time of Lenet is fastest but the accuracy and precision of our proposed architecture is better than both AlexNet and Lenet see Table 3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Lenet", "start": 33, "end": 37}, {"start": 142, "end": 146, "text": "Lenet"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 58, "end": 65}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 71, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AlexNet", "start": 130, "end": 136}]}]}
{"content": "We can continue to explore different architectures and strategies for the training of a neural network by changing the hyper-parameters or pre-processing the input data like using the LAB color space instead of RGB space or by augmenting the results of initial segmentation otsu segmentation in the input data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 88, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 119, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 158, "end": 162}, {"start": 299, "end": 303, "text": "input"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 95, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 164, "end": 167}, {"start": 305, "end": 308, "text": "data"}]}]}
{"content": "This can be done by applying the full convolution neural network to generate color coded likelihood maps for the pathologists ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 38, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 57, "end": 63}]}]}
{"content": "As far as the authors are aware , this is the first paper describing the applicability of convolutional neural networks for diagnostic analysis of osteosarcoma ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 90, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "analysis", "start": 135, "end": 142}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 104, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 111, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 144, "end": 145}]}, {"label": ["I-DeepLearning"], "points": [{"text": "osteosarcoma", "start": 147, "end": 158}]}]}
{"content": "In this paper , we propose a new computational method for predicting DTIs from drug molecular structure and protein sequence by using the stacked autoencoder of deep learning which can adequately extracts the raw data information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "autoencoder", "start": 146, "end": 156}]}]}
{"content": "The experimental results of 5-fold cross-validation indicate that the proposed method achieves superior performance on golden standard datasets enzymes , ion channels , GPCRs and nuclear receptors with accuracy of 0.9414 , 0.9116 , 0.8669 and 0.8056 , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "5-fold", "start": 28, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "golden", "start": 119, "end": 124}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 202, "end": 209}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 35, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "standard", "start": 126, "end": 133}]}, {"label": ["I-DeepLearning"], "points": [{"text": "datasets", "start": 135, "end": 142}]}]}
{"content": "In the experiment , we make the predictions on the golden standard drug-target interactions datasets involving enzymes , ion channels , GPCRs and nuclear receptors ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 7, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "golden", "start": 51, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 92, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "standard", "start": 58, "end": 65}]}]}
{"content": "In this experiment , we used the chemical structure of the molecular substructure fingerprints from PubChem database ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 8, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PubChem", "start": 100, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 108, "end": 115}]}]}
{"content": "It defines an 881 dimensional binary vector to represent the molecular substructure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 37, "end": 42}]}]}
{"content": "Stacked Auto-Encoder SAE is a popular depth learning model , which uses autoencoders as building blocks to create deep neural network [ 17 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Stacked", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SAE", "start": 21, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 53, "end": 57}]}, {"label": ["B-DeepLearning"], "points": [{"text": "autoencoders", "start": 72, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 119, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Auto-Encoder", "start": 8, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 126, "end": 132}]}]}
{"content": "The auto-encoder AE can be considered as a special neural network with one input layer , one hidden layer and one output layer , as shown in Fig. 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 4, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 51, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one", "start": 71, "end": 73}, {"start": 89, "end": 91, "text": "one"}, {"start": 110, "end": 112, "text": "one"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 58, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 75, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 81, "end": 85}, {"start": 100, "end": 104, "text": "layer"}, {"start": 121, "end": 125, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 93, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 114, "end": 119}]}]}
{"content": "The combination of multiple auto-encoders together constitutes the stacked auto-encoders , which has the characteristics of deep learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoders", "start": 28, "end": 40}, {"start": 75, "end": 87, "text": "auto-encoders"}]}]}
{"content": "Figure 2 shows the structure of the stacked auto-encoder with h-level auto-encoders which are trained in the layer-wise and bottom-up manner ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 44, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "auto-encoders", "start": 70, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "layer-wise", "start": 109, "end": 118}]}]}
{"content": "The activation function is usually the sigmoid function or tanh function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 4, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 39, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tanh", "start": 59, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 15, "end": 22}]}]}
{"content": "In this paper , we set up a 3 layer auto-encoder , and use the rotation forest as the final classifier ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3", "start": 28, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 30, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder", "start": 36, "end": 47}]}]}
{"content": "In this paper , we use 5-fold cross-validation to assess the predictive ability of our model in the golden standard datasets involving enzymes , ion channels , GPCRs and nuclear receptors ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "5-fold", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 87, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "golden", "start": 100, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 30, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "standard", "start": 107, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "datasets", "start": 116, "end": 123}]}]}
{"content": "The proposed model performs well in the golden standard datasets: enzymes , ion channels , GPCRs and nuclear receptors ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "golden", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "standard", "start": 47, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "datasets:", "start": 56, "end": 64}]}]}
{"content": "Table 1 lists the experimental results on the enzyme dataset , it yielded an accuracy of 0.9414 , sensitivity of 0.9555 , precision of 0.9293 , MCC of 0.8832 and AUC of 09425 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "enzyme", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 77, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 98, "end": 108}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 122, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MCC", "start": 144, "end": 146}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 162, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 53, "end": 59}]}]}
{"content": "The highest accuracy of the five models reached 0.9462 , and the lowest also reached 09385 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 12, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 33, "end": 38}]}]}
{"content": "The accuracy , sensitivity , precision , MCC and AUC of cross-validation are 0.9116 , 0.9569 , 0.8778 , 0.8271 and 0.9107 , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 15, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 29, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MCC", "start": 41, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 49, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 56, "end": 71}]}]}
{"content": "The average accuracy , sensitivity , precision , MCC and AUC are 0.8669 , 0.8164 , 0.9102 , 0.7396 and 0.8743 , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 12, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 23, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 37, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MCC", "start": 49, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 57, "end": 59}]}]}
{"content": "The highest accuracy of the five models reached 09331 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 12, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 33, "end": 38}]}]}
{"content": "We achieved an accuracy of 0.8056 , sensitivity of 0.7627 , precision of 0.8410 , MCC of 0.6188 and AUC of 08176 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 15, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 36, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 60, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MCC", "start": 82, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 100, "end": 102}]}]}
{"content": "Figures 4 , 5 , 6 and 7 show the ROC curves obtained on the enzymes , ion channels , GPCRs and nuclear receptors datasets by the proposed method ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ROC", "start": 33, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "nuclear", "start": 95, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "curves", "start": 37, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "receptors", "start": 103, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "datasets", "start": 113, "end": 120}]}]}
{"content": "The results of the comparison show that the stacked auto-encoder combined with the rotation forest classifier can improve the prediction ability on the golden standard datasets Table 6 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 52, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "golden", "start": 152, "end": 157}]}, {"label": ["I-DeepLearning"], "points": [{"text": "standard", "start": 159, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "datasets", "start": 168, "end": 175}]}]}
{"content": "CAST uses Multi-Layer Perceptron MLP , a type of artificial neural network ANN that is often applied to classification problems [ 17 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multi-Layer", "start": 10, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 33, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 49, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 75, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 22, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 60, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 67, "end": 73}]}]}
{"content": "The retina training set is comprised of 5891 pixel examples ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "retina", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "5891", "start": 40, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 11, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 20, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pixel", "start": 45, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "examples", "start": 51, "end": 58}]}]}
{"content": "The accuracy of the training set was tested using Weka Explorer , which calculated the accuracy of correctly classifying pixels to be 96.4013% [ 19 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 4, "end": 11}, {"start": 87, "end": 94, "text": "accuracy"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 20, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 29, "end": 31}]}]}
{"content": "The network used consisted of 97 input nodes , 1 hidden layer with 49 nodes , and 2 output nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "97", "start": 30, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "1", "start": 47, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2", "start": 82, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 33, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 39, "end": 43}, {"start": 70, "end": 74, "text": "nodes"}, {"start": 91, "end": 95, "text": "nodes"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 49, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 56, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "with", "start": 62, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "49", "start": 67, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 84, "end": 89}]}]}
{"content": "The network was trained using ten-fold cross validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ten-fold", "start": 30, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross", "start": 39, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 45, "end": 54}]}]}
{"content": "The learning method used was the standard method of backpropagation gradient decent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 52, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 68, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decent", "start": 77, "end": 82}]}]}
{"content": "In order to develop CAST we utilized an artificial neural network ANN and tested its ability against fast marching ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 40, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 66, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 58, "end": 64}]}]}
{"content": "Classifiers such as perceptrons , the basic component of neural networks , have been found useful for identifying membranes in EM images [ 6 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "perceptrons", "start": 20, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 130, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 64, "end": 71}]}]}
{"content": "One kind of neural network that has been widely used for image segmentation is the Convolutional Neural Network CNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 57, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 83, "end": 95}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 112, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 19, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 97, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 104, "end": 110}]}]}
{"content": "ANNs require significant computing power , which can be an important factor if the data set is large ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 83, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 88, "end": 90}]}]}
{"content": "Use of ANNs has become common in image processing due to generally high performance , and our results do not differ in that we also find classification by ANN to provide superior performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 7, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 33, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 155, "end": 157}]}]}
{"content": "The artificial neural network examines each pixel in the selected area ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 4, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 15, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 22, "end": 28}]}]}
{"content": "This allows the neural network to make an accurate classification despite low contrasts and fragmented boundaries ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 23, "end": 29}]}]}
{"content": "Outlines made using the neural network are subjectively much more accurate than those made using fast marching ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}]}
{"content": "Expand Area works in three dimensions [ 21 ] , but the current neural network only works in two dimensions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 70, "end": 76}]}]}
{"content": "If part of the structure of interest shifted out of the area being examined by the neural network somewhere in the stack , the results produced by the network would be off ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 83, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 90, "end": 96}]}]}
{"content": "One possible solution would be to move the region being examined by the neural network on a slice-to-slice basis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 72, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 79, "end": 85}]}]}
{"content": "These words are filtered by the NLTK toolkit [ 9 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NLTK", "start": 32, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "toolkit", "start": 37, "end": 43}]}]}
{"content": "In contrast to the one-hot encoding , the NNLM generates word vectors with low and dense vector , it is easier to capture the word\u00e2\u20ac\u2122s property ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNLM", "start": 42, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vectors", "start": 62, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 89, "end": 94}]}]}
{"content": "Mikolov et al. [ 12 ] extended the Bengio\u00e2\u20ac\u2122 work and built a new neural network language learning model , Word2Ve , using different learning methods: Continues Bag of Words CBOW and Skip-gram ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 66, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 99, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Word2Ve", "start": 107, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 73, "end": 79}]}]}
{"content": "The reason that we use the Word2Vec due to it consume less time than Benhio\u00e2\u20ac\u2122s NNLM while training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Word2Vec", "start": 27, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NNLM", "start": 80, "end": 83}]}]}
{"content": "We applied the Word2Vec to generate the good quality of word vectors through training the whole data set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Word2Vec", "start": 15, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vectors", "start": 61, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 96, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 101, "end": 103}]}]}
{"content": "In order to explore the performances of different combinations of the nine features , we trained 5621000 pattern recognition neural networks [ 11 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "5621000", "start": 97, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pattern", "start": 105, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "recognition", "start": 113, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 125, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 132, "end": 139}]}]}
{"content": "In this article , we study the impacts of encoding schemes on several variant algorithms in auto-encoders by applying deep neural networks to gene annotation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoders", "start": 92, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 118, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 123, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 130, "end": 137}]}]}
{"content": "Four variant auto-encoder algorithms include orthodox auto-encoder , denoising auto-encoder , hidden-layer denoising auto-encoder and double denoising auto-encoder. ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 13, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "orthodox", "start": 45, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "denoising", "start": 69, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden-layer", "start": 94, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "double", "start": 134, "end": 139}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder", "start": 54, "end": 65}, {"start": 79, "end": 90, "text": "auto-encoder"}, {"start": 117, "end": 128, "text": "auto-encoder"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "denoising", "start": 107, "end": 115}, {"start": 141, "end": 149, "text": "denoising"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder.", "start": 151, "end": 163}]}]}
{"content": "The data sets are the standard benchmark from fruitfly.org for predicting gene splicing sites on human genome sequences [ 16 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "predicting", "start": 63, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 9, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gene", "start": 74, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "splicing", "start": 79, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sites", "start": 88, "end": 92}]}]}
{"content": "The data set I is the Acceptor locations containing 6877 sequences with 90 features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "6877", "start": 52, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "90", "start": 72, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sequences", "start": 57, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "features", "start": 75, "end": 82}]}]}
{"content": "The data set II is the Donor locations including 6246 sequences with 15 features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "6246", "start": 49, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "15", "start": 69, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sequences", "start": 54, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "features", "start": 72, "end": 79}]}]}
{"content": "The data set of cleaned 269 genes is divided into a test and a training data set [ 16 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "data", "start": 4, "end": 7}, {"start": 72, "end": 75, "text": "data"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 9, "end": 11}, {"start": 77, "end": 79, "text": "set"}]}]}
{"content": "Denoising auto-encoder seems not fit to the application of DNA structure prediction because corrupted input data DNA features at each location may have a high dependency with others such that denoising makes the prediction messed ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Denoising", "start": 0, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNA", "start": 59, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 102, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder", "start": 10, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "structure", "start": 63, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "prediction", "start": 73, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 108, "end": 111}]}]}
{"content": "Compared with the performance of input-layer denoising auto-encoder in Fig. 2c , d , in hidden-layer denoising auto-encoder model , corrupting some nodes on hidden layers makes a less impact than corrupting nodes on input layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input-layer", "start": 33, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden-layer", "start": 88, "end": 99}]}, {"label": ["B-DeepLearning"], "points": [{"text": "denoising", "start": 101, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 124, "end": 128}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 157, "end": 162}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 216, "end": 220}]}, {"label": ["I-DeepLearning"], "points": [{"text": "denoising", "start": 45, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder", "start": 55, "end": 66}, {"start": 111, "end": 122, "text": "auto-encoder"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 164, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 222, "end": 226}]}]}
{"content": "Figure 2g , h shows the performance of double denoising auto-encoder ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "double", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "denoising", "start": 46, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "auto-encoder", "start": 56, "end": 67}]}]}
{"content": "On the other side , it shows that auto-encoder method without denoising is better than other three variants ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 34, "end": 45}]}]}
{"content": "The over-fitting issues occur in Denoising Auto-encoder more frequently than others ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 4, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Denoising", "start": 33, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Auto-encoder", "start": 43, "end": 54}]}]}
{"content": "The over-fitting occurrence is correlated to the auto-encoder algorithm and the encoding schemes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 4, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "auto-encoder", "start": 49, "end": 60}]}]}
{"content": "The DAE shows the poorest performance among the autoencoder algorithms ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "autoencoder", "start": 48, "end": 58}]}]}
{"content": "In case study section given below , we demonstrate the application of a Neural Network for identifying multimarker panels for breast cancer based on liquid chromatography tandem mass spectrometry LC/MS/MS proteomics profiles ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 72, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 79, "end": 85}]}]}
{"content": "Lancashire et al. 24 described with recent literature that artificial neural networks can cope with highly dimensional , complex datasets such as those generated by protein mass spectrometry and DNA microarray experiments , and can also be used to solve problems , such as disease classification and identification of biomarkers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 59, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 129, "end": 136}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 210, "end": 220}]}, {"label": ["B-DeepLearning"], "points": [{"text": "disease", "start": 273, "end": 279}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 70, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 77, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "classification", "start": 281, "end": 294}]}]}
{"content": "We used a data analysis method based on a Feed-Forward Neural Network FFNN to identify multiprotein biomarker panels , with which we are capable of separating plasma samples regarding reference and cancerous with high predictive performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Feed-Forward", "start": 42, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "FFNN", "start": 70, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 55, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 62, "end": 68}]}]}
{"content": "According to an area-under-the-curve AUC criterion the optimal combination of a panel of five markers was determined , using both a twovariable output encoding scheme and a single-variable encoding scheme ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "area-under-the-curve", "start": 16, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 37, "end": 39}]}]}
{"content": "We compared the Receiver Operating Characteristics ROC performance and verified that the best five-marker panel performed well in both training dataset and test dataset , achieving more than 82.5% in sensitivity and specificity ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Receiver", "start": 16, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ROC", "start": 51, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 135, "end": 142}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 156, "end": 159}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sensitivity", "start": 200, "end": 210}]}, {"label": ["B-DeepLearning"], "points": [{"text": "specificity", "start": 216, "end": 226}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Operating", "start": 25, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Characteristics", "start": 35, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 144, "end": 150}, {"start": 161, "end": 167, "text": "dataset"}]}]}
{"content": "The algorithm NNPP Neural Network Promoter Prediction , which is discussed in Section 10.5 , was developed for the Berkeley Drosophila Genome Project BDGP , and uses time-delay neural network architecture and recognition of specific sequence patterns to predict promoters see Figure 10.14 for details ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNPP", "start": 14, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 19, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "time-delay", "start": 166, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 26, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Promoter", "start": 34, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Prediction", "start": 43, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 177, "end": 182}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 184, "end": 190}]}]}
{"content": "Each sequence position is represented by four input units , each representing one base , with a total of 51 bases in the input layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "four", "start": 41, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 121, "end": 125}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 46, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 52, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 127, "end": 131}]}]}
{"content": "Each TATA-box hidden unit has inputs from 15 consecutive bases , and the Inr hidden units also receive signals from 15 consecutive bases ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 14, "end": 19}, {"start": 77, "end": 82, "text": "hidden"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 21, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 84, "end": 88}]}]}
{"content": "The same weighting scheme is applied to all the TATA-box hidden units and similarly with another set of weights for the Inr hidden units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 57, "end": 62}, {"start": 124, "end": 129, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "set", "start": 97, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 64, "end": 68}, {"start": 131, "end": 135, "text": "units"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 101, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 104, "end": 110}]}]}
{"content": "The four layers of squares underneath the sequence represent the input layer units and encode the sequence in a very simple way ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 65, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 71, "end": 75}]}]}
{"content": "Each of the units in the two hidden layers the TATA and Inr layers receives input from a consecutive set of 15 bases ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 29, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 36, "end": 41}]}]}
{"content": "A variety of measures , such as the weight matrix score for TATA-box signal and simple measurements such as the GC content and the distance between different signals , are used to assign values to the units in an input layer of a neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 213, "end": 217}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 230, "end": 235}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 219, "end": 223}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 237, "end": 243}]}]}
{"content": "These feed via two hidden layers to an output layer unit , which predicts the presence of RNA polymerase II promoters in the sequence window ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 19, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 26, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 46, "end": 50}]}]}
{"content": "The donor sites are modeled with a set of 10 networks , each with 23 bases in the input layer represented by four units each , as in Figure 10.14 , a hidden layer of 10 units , and a single output layer unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 42, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 82, "end": 86}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 150, "end": 155}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 190, "end": 195}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 45, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 88, "end": 92}, {"start": 157, "end": 161, "text": "layer"}, {"start": 197, "end": 201, "text": "layer"}]}]}
{"content": "The 10 networks were separately trained from different starting points , and their outputs from 0 to 1 averaged to get the final score ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 4, "end": 5}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}]}
{"content": "A similar set of 10 networks was used to score acceptor sites , except that these have a 61-base input window and 15 hidden layer units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 17, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 117, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 20, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 124, "end": 128}]}]}
{"content": "The set of neural networks used to predict coding regions in NetPlantGene has as output a score for each base of its likelihood of being in a coding region ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 18, "end": 25}]}]}
{"content": "A neural network predicts the three secondary structure types a , b , and coil using real numbers from the output units that make up the output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 2, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 137, "end": 142}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 9, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 144, "end": 148}]}]}
{"content": "The first network is a sequence-to-structure network whose input represents the local alignment , residue conservation , and additionally long-range sequence information see Figure 1122 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequence-to-structure", "start": 23, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 45, "end": 51}]}]}
{"content": "The output of the first network forms the input to the second neural network , a structure-to-structure network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 62, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "structure-to-structure", "start": 81, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 69, "end": 75}, {"start": 104, "end": 110, "text": "network"}]}]}
{"content": "NNSSP Neural Net Nearest Neighbor is a neural network development of a statistical and knowledge-based program , SSP , which , like PREDATOR , used the nearestneighbor approach ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNSSP", "start": 0, "end": 4}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 6, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Net", "start": 13, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Nearest", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neighbor", "start": 25, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 46, "end": 52}]}]}
{"content": "NNSSP has an advanced scoring system that combines sequence similarity , local sequence information , and knowledge-based information on b-turns and the amino- and carboxy-terminal properties of a-helices and b-strands ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNSSP", "start": 0, "end": 4}]}]}
{"content": "Schematic representation of the double neural network architecture frequently used in protein secondary structure prediction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 39, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "protein", "start": 86, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 46, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architecture", "start": 54, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "secondary", "start": 94, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "structure", "start": 104, "end": 112}]}, {"label": ["I-DeepLearning"], "points": [{"text": "prediction", "start": 114, "end": 123}]}]}
{"content": "The three hidden layer units receive the same signals from the input layer , but apply different weights to them ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 10, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 63, "end": 67}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 97, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 17, "end": 21}, {"start": 69, "end": 73, "text": "layer"}]}]}
{"content": "The first hidden layer will contain 39 units for a 13-residue window , and these send signals to units in a second hidden layer , which then signals the output layer see Figure 1226 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 10, "end": 15}, {"start": 115, "end": 120, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "39", "start": 36, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 153, "end": 158}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 17, "end": 21}, {"start": 122, "end": 126, "text": "layer"}, {"start": 160, "end": 164, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 39, "end": 43}]}]}
{"content": "The communication from the first hidden layer to the second has a repeat pattern intended to mimic the helical structure , and has been colored for clarity ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 33, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 40, "end": 44}]}]}
{"content": "The output layer Oi has three units , corresponding to predicting a-helix , b-strand , or coil for residue i ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "three", "start": 24, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 11, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 30, "end": 34}]}]}
{"content": "These feed signals to a hidden layer of 15 units that in turn send signals to the output layer of seven units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 24, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "15", "start": 40, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 82, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "seven", "start": 98, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 31, "end": 35}, {"start": 89, "end": 93, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 43, "end": 47}, {"start": 104, "end": 108, "text": "units"}]}]}
{"content": "The initial neural network has a sevenstate output , namely a-helix H , b-strand E , coil C , and the single residue at the beginning and end of a helix or strand Hb , He , Eb , and Ee , respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sevenstate", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 19, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "output", "start": 44, "end": 49}]}]}
{"content": "A 15-residue sequence window is used with 20 input layer units per residue representing the PSSM and an additional spacer unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "20", "start": 42, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "input", "start": 45, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 51, "end": 55}]}]}
{"content": "The neural network used is a standard feed-forward model as described in Section 12.4 , with six hidden layer units and two output units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feed-forward", "start": 38, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 97, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 124, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 51, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 104, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 110, "end": 114}, {"start": 131, "end": 135, "text": "units"}]}]}
{"content": "Indeed , this work will focus on the development of a diagnosis support system , in terms of its knowledge representation and reasoning procedures , under a formal framework based on Logic Programming , complemented with an approach to computing centered on Artificial Neural Networks , to evaluate ACS predisposing and the respective Degree-of-Confidence that one has on such a happening ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 258, "end": 267}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 269, "end": 274}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 276, "end": 283}]}]}
{"content": "Each base pair A , C , T , G was denoted as a one-hot vector [ 1 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 ] , [ 0 , 0 , 1 , 0 ] and [ 0 , 0 , 0 , 1 ] respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 54, "end": 59}]}]}
{"content": "For example , if we used DNase-seq data with 101 bp , the vector was of size 1 x 101 for a sample ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 58, "end": 63}]}]}
{"content": "The CNN consists of a convolutional layer , a max-pooling layer , a fully connected layer , a dropout layer [ 28 ] and an output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 22, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 46, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 68, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 94, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 122, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 36, "end": 40}, {"start": 58, "end": 62, "text": "layer"}, {"start": 84, "end": 88, "text": "layer"}, {"start": 102, "end": 106, "text": "layer"}, {"start": 129, "end": 133, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 74, "end": 82}]}]}
{"content": "DNN consists of one or two full connection layers , a dropout layer after each full connection layer and an output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 27, "end": 30}, {"start": 79, "end": 82, "text": "full"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 54, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 108, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connection", "start": 32, "end": 41}, {"start": 84, "end": 93, "text": "connection"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 43, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 62, "end": 66}, {"start": 95, "end": 99, "text": "layer"}, {"start": 115, "end": 119, "text": "layer"}]}]}
{"content": "For CNN models , we vary the number of kernels , the size of kernel window , and the number of neurons in the full connection layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 29, "end": 34}, {"start": 85, "end": 90, "text": "number"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "size", "start": 53, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 110, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 8, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 36, "end": 37}, {"start": 58, "end": 59, "text": "of"}, {"start": 92, "end": 93, "text": "of"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernel", "start": 61, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 95, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connection", "start": 115, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 126, "end": 130}]}]}
{"content": "For DNN models , we vary the number of layers , and the number of neural in each full connection layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 29, "end": 34}, {"start": 56, "end": 61, "text": "number"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 81, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 8, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 36, "end": 37}, {"start": 63, "end": 64, "text": "of"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connection", "start": 86, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 97, "end": 101}]}]}
{"content": "For training , we used the cross-entropy as the loss function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 27, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 48, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 53, "end": 60}]}]}
{"content": "Given this loss function and different hyper-parameters see below , the models were trained using the standard error back-propagation algorithm and AdaDetla method [ 29 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 11, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 39, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 72, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "standard", "start": 102, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AdaDetla", "start": 148, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 16, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "error", "start": 111, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "back-propagation", "start": 117, "end": 132}]}]}
{"content": "We set each model for 100 epochs and 128 mini-batch size and validated the model after each epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 12, "end": 16}, {"start": 75, "end": 79, "text": "model"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "100", "start": 22, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mini-batch", "start": 41, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "epochs", "start": 26, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 52, "end": 55}]}]}
{"content": "Then the early-stop trick was used to stop training as the error on validation set is higher than the last four epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "early-stop", "start": 9, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stop", "start": 38, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 68, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 112, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 43, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 79, "end": 81}]}]}
{"content": "The best model was chosen according to the accuracy on the validation set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 9, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 43, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 59, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 70, "end": 72}]}]}
{"content": "For KNN , LR , RF , we implemented these baselines using the python based scikit-learn package ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "scikit-learn", "start": 74, "end": 85}]}]}
{"content": "We used python and Keras framework to train neural networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "python", "start": 8, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Keras", "start": 19, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 51, "end": 58}]}]}
{"content": "We used python and skcikit-learn to train conventional machine learning methods [ 30 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "python", "start": 8, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "skcikit-learn", "start": 19, "end": 31}]}]}
{"content": "We used different sample lengths to train the CNN models and different hyper-parameters for each length ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sample", "start": 18, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 46, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 71, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "lengths", "start": 25, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 50, "end": 55}]}]}
{"content": "For each length , we selected the results of best hyper-parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 50, "end": 65}]}]}
{"content": "Performance evaluation of CNN with respect to sample length and model structure using HMS data in terms of the distribution of AUCs across 256 experiments ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 26, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sample", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 64, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUCs", "start": 127, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 143, "end": 153}]}, {"label": ["I-DeepLearning"], "points": [{"text": "length", "start": 53, "end": 58}]}]}
{"content": "The effect of kernel number ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "number", "start": 21, "end": 26}]}]}
{"content": "The effect of neuron number ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neuron", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "number", "start": 21, "end": 26}]}]}
{"content": "The effect of kernel window size ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "window", "start": 21, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 28, "end": 31}]}]}
{"content": "The effect of sample length and DNN model structure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sample", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 32, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "length", "start": 21, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 36, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "structure", "start": 42, "end": 50}]}]}
{"content": "The performance comparison of DNN versus CNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 30, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 41, "end": 43}]}]}
{"content": "First , more convolutional kernels could also improve the prediction performance Fig. 2B ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 13, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 27, "end": 33}]}]}
{"content": "However , when more than 64 kernels were used , the improvement seemed to be saturated for the 256 experiments Fig. 2B ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 28, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 99, "end": 109}]}]}
{"content": "Second , more neurons in the full connection layer of CNN could improve the prediction performance Fig. 2C ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 29, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 54, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connection", "start": 34, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 45, "end": 49}]}]}
{"content": "We observe that small kernel window size achieves better performance than using large ones Fig. 2D while big kernel window size usually used in sequence-based CNN models ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 22, "end": 27}, {"start": 109, "end": 114, "text": "kernel"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 159, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "window", "start": 29, "end": 34}, {"start": 116, "end": 121, "text": "window"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 36, "end": 39}, {"start": 123, "end": 126, "text": "size"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 163, "end": 168}]}]}
{"content": "We find that deeper neural networks and longer sample length work better too for DNN Fig. 2E ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 20, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 81, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 27, "end": 34}]}]}
{"content": "However , the performance of DNN is still slightly worse than that of CNN , indicating the importance of combining convolution operation with HMS data Fig. 2F ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 29, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 70, "end": 72}]}]}
{"content": "We conducted leave-one-feature-out feature selection experiments to train the CNN models by using merely four histone modifications data with the same hyperparameters in previous section ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 53, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 78, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 151, "end": 165}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 82, "end": 87}]}]}
{"content": "For model architectures , more convolutional kernels could also improve the prediction performance Fig. 4B ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 31, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 45, "end": 51}]}]}
{"content": "Thus , no matter what the data type is , the additional kernels are beneficial to enhance power in extracting features and improve model performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 56, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 131, "end": 135}]}]}
{"content": "By changing the number of neurons in the last dense layer of CNN , we can see that models with more hidden neurons achieve better performance Fig. 4C ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 16, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 46, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 61, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 83, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 23, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 26, "end": 32}, {"start": 107, "end": 113, "text": "neurons"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 52, "end": 56}]}]}
{"content": "We also see that CNN models with small and large kernel window sizes 4 and 24 achieve almost the same performance for different sample lengths Fig. 4D ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 17, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 49, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 21, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "window", "start": 56, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 63, "end": 67}]}]}
{"content": "This suggests that kernel window sizes 4 and 24 could not distinctly influence DHS data information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "window", "start": 26, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 33, "end": 37}]}]}
{"content": "For comparison with CNN , we also trained DNN using different sequence lengths and hyper-parameters for the DHS data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 20, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 42, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 83, "end": 98}]}]}
{"content": "Moreover , the performance of DNN is slightly worse than that of CNN , indicating the importance of combining convolution operation with DHS data Fig. 4F ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 30, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 65, "end": 67}]}]}
{"content": "In both HMS and DHS cases , CNN perform significantly better than conventional classifiers in term of the distribution of AUCs across 256 experiments Fig. 5 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 28, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUCs", "start": 122, "end": 125}]}, {"label": ["B-DeepLearning"], "points": [{"text": "experiments", "start": 138, "end": 148}]}]}
{"content": "With 3D convolution kernel in 3DCNN could learn more spatial information than the standard 2D convolutional neural networks with the 2D kernels ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3D", "start": 5, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 30, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 108, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2D", "start": 133, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 8, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernel", "start": 20, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 115, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 136, "end": 142}]}]}
{"content": "Our 3DCNN is a seven layers deep neural network consists the layers of 3D convolution , batch normalization , and Softmax ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "seven", "start": 15, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3D", "start": 71, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 88, "end": 92}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Softmax", "start": 114, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 21, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "deep", "start": 28, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 33, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 40, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 74, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "normalization", "start": 94, "end": 106}]}]}
{"content": "As the input is a high-resolution 3D MRI data , 3DCNN is applied deep convolution layers with small kernel numbers to overcome the overfitting problem ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 48, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 70, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 100, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 131, "end": 141}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 82, "end": 87}]}]}
{"content": "Meanwhile , 3DMaxPooling is introduced to dimension reduction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DMaxPooling", "start": 12, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dimension", "start": 42, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "reduction", "start": 52, "end": 60}]}]}
{"content": "Except for the output layer , all the layers are 3D convolution layers , which have a much stronger nonlinear mapping power and feature extraction capacity than traditional 2D convolution neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 15, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3D", "start": 49, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2D", "start": 173, "end": 174}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 22, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 52, "end": 62}, {"start": 176, "end": 186, "text": "convolution"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 64, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 188, "end": 193}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 195, "end": 201}]}]}
{"content": "At last , due to the 3D high dimension in MRI data , all the parameters and convolution kernels in 3DCNN are small to prevent the overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 61, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 76, "end": 86}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 99, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 130, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 88, "end": 94}]}]}
{"content": "There are 80 samples in our MRI dataset which consists 40 BECT instances and 40 control instances ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "80", "start": 10, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MRI", "start": 28, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "samples", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 32, "end": 38}]}]}
{"content": "Therefore , we construct a traditional 2D convolution neural network 2DCNN as the baseline model for BECT prediction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "2D", "start": 39, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 54, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 69, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 91, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 42, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 61, "end": 67}]}]}
{"content": "The convolution kernel sizes of 2DCNN are set the same 3DCNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernel", "start": 16, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 32, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 55, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 23, "end": 27}]}]}
{"content": "And the stride of 2DCNN is modified to get a full convolution architecture which reduces the number of weights and overcomes the overfitting problem ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 8, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 18, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 103, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 129, "end": 139}]}]}
{"content": "However , 3DCNN extends one more dimension as the data channel dimension in the input layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 10, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 80, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 86, "end": 90}]}]}
{"content": "The detailed comparison of 2DCNN and 3DCNN are shown in Table 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 27, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 37, "end": 41}]}]}
{"content": "We evaluate 3DCNN on our MRI dataset in five-fold cross-validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 12, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MRI", "start": 25, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "five-fold", "start": 40, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 29, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 50, "end": 65}]}]}
{"content": "The whole 80-cases dataset is divided into training dataset 54 instances and testing dataset 16 instances , and the evaluation criterion is the prediction accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 19, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 43, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "54", "start": 60, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 77, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "16", "start": 93, "end": 94}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 155, "end": 162}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 52, "end": 58}, {"start": 85, "end": 91, "text": "dataset"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "instances", "start": 63, "end": 71}, {"start": 96, "end": 104, "text": "instances"}]}]}
{"content": "The learning rate is set to 1e \u00e2\u02c6\u2019 4 under Adam optimizer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Adam", "start": 43, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "optimizer", "start": 48, "end": 56}]}]}
{"content": "Table 2 shows the performance of 3DCNN and 2DCNN on the test dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 33, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 43, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 56, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 61, "end": 67}]}]}
{"content": "The result shows 3DCNN achieving a prediction accuracy of 89.80% on the five-fold cross-validation evaluation , which outperforms the 2DCNN baseline model 8250% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "3DCNN", "start": 17, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 46, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "five-fold", "start": 72, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "2DCNN", "start": 134, "end": 138}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 149, "end": 153}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 82, "end": 97}]}]}
{"content": "Finally , the dataset for model contains 32 independent variables , 1 dependent variable , and 1 time index ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 14, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 26, "end": 30}]}]}
{"content": "We test 4 different LSTM models: large/small sample size that the predictor variables come from air pollution , temperature change , and google trends datasets large/small sample size that only use google trends data as predictor variables ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 151, "end": 158}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models:", "start": 25, "end": 31}]}]}
{"content": "The number of neurons in the input layer was equal to the number of input data points for each attribute ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 29, "end": 33}, {"start": 68, "end": 72, "text": "input"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 11, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 14, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 35, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 74, "end": 77}]}]}
{"content": "The optimal number of neurons in the hidden layer was determined through experimentation for minimizing the error at the best epoch for each network individually ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 12, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 37, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 126, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 19, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 22, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}]}]}
{"content": "An upper limit for the total number of weight connections was set to half of the total number of input vectors to avoid overfitting , as suggested previously Andrea and Kalayeh , 1991 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "number", "start": 29, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 97, "end": 101}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 120, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 36, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weight", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connections", "start": 46, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vectors", "start": 103, "end": 109}]}]}
{"content": "During predictions , the network was fed with new data from the sequences that were not part of training set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 96, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 105, "end": 107}]}]}
{"content": "At this threshold value of Pad , the Matthew's correlation coefficient was observed to be highest 094 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Matthew's", "start": 37, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "correlation", "start": 47, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "coefficient", "start": 59, "end": 69}]}]}
{"content": "The first neural network NN1 predicts the residue contact number on the basis of the PSSM weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 10, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 25, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 17, "end": 23}]}]}
{"content": "The structure of the NN1 is as follows ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 21, "end": 23}]}]}
{"content": "The PSSM values are the input parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 24, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 30, "end": 39}]}]}
{"content": "The complete vector of the input data for the NN1 thus composed of 2/z + 1 x 1 = 11 x 21 =231 parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 27, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 46, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 94, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 33, "end": 36}]}]}
{"content": "Two internal layers with 50 and 3 neurons were implied for the NN1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Two", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "3", "start": 32, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 63, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "internal", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 13, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 34, "end": 40}]}]}
{"content": "A single number was at the NN1 output , the predicted value of the contact number of the rth residue ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 27, "end": 29}]}]}
{"content": "The neural network at the second level NN2 was built as follows ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN2", "start": 39, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}]}
{"content": "The contact numbers predicted by NN1 served as its input data 41 values were estimated for each position , namely , the predicted contact numbers at the /th and rth \u00c2\u00b1 20 positions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN1", "start": 33, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 51, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 57, "end": 60}]}]}
{"content": "In this way , 42 parameters were at the NN2 input the NN2 contained one internal layer of 10 neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "42", "start": 14, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "NN2", "start": 40, "end": 42}, {"start": 54, "end": 56, "text": "NN2"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "10", "start": 90, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 17, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 93, "end": 99}]}]}
{"content": "Training and testing of the algorithm was implemented on a sample of the 234 monomeric protein chains with known spatial structures extracted from the PDB database Berman et al. , 2000 and belonging to different protein fold types according to the SCOP classification Andreeva et al. , 2004 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Training", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PDB", "start": 151, "end": 153}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 155, "end": 162}]}]}
{"content": "The neural network was trained on the training sample by the backpropagation algorithm Rumelhart et al. , 1986 , with 50 epochs , momentum of 0.9 , and learning rate of 001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 61, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "50", "start": 118, "end": 119}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 130, "end": 137}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 152, "end": 159}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "epochs", "start": 121, "end": 126}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 139, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.9", "start": 142, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 161, "end": 164}]}]}
{"content": "The prediction accuracy was estimated on the testing data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 15, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 45, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 53, "end": 56}]}]}
{"content": "The second level neural network NN2 enables the improvement of prediction accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 17, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 74, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 24, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "NN2", "start": 32, "end": 34}]}]}
{"content": "NN2 was introduced to additionally take into consideration the interdependence of the contact number of residues neighboring along the protein chain ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN2", "start": 0, "end": 2}]}]}
{"content": "The NN2 introduces , although slight , yet regular improvement in prediction accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN2", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 77, "end": 84}]}]}
{"content": "Similarly to VL-XT , a neural network with a fully connected hidden layer of ten neurons was trained on the specific datasets and it outputs a value for the central amino acid in the window ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 45, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ten", "start": 77, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 117, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 51, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "hidden", "start": 61, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 68, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 81, "end": 87}]}]}
{"content": "Using an original approach , RONN Regional Order Neural Network [ 82 ] recognizes disordered segments based on their similarity to well-characterized prototype sequences with known disordered status ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RONN", "start": 29, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Regional", "start": 34, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Order", "start": 43, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 49, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 56, "end": 62}]}]}
{"content": "The resulting homology scores are converted into distances and are used to train a modified version of radial basis function networks called a bio-basis function neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bio-basis", "start": 143, "end": 151}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 153, "end": 160}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 162, "end": 167}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 169, "end": 175}]}]}
{"content": "Seven physical parameters describing the physicochemical properties of the residue: a steric parametergraph shape index , hydrophobicity , volume , polarizability , isoelectric point , helix probability , and sheet probability ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "physical", "start": 6, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 15, "end": 24}]}]}
{"content": "Two hidden layers , each with 71 nodes , were used for a preprocessing network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "71", "start": 30, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 33, "end": 37}]}]}
{"content": "Probability predictions from this network were further refined using a filter network with a single hidden layer of 51 nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "51", "start": 116, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 119, "end": 123}]}]}
{"content": "Training and testing for all neural networks considered here was preformed on the SPINE dataset [ 18 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Training", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 29, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SPINE", "start": 82, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 36, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 88, "end": 94}]}]}
{"content": "In general the networks used to predict \u00cf\u02c6 angles were composed of two hidden layers , each with 51 nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 71, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "51", "start": 97, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 78, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 100, "end": 104}]}]}
{"content": "Ten fold cross validation will be used to judge the accuracy of the predictions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Ten", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 52, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "fold", "start": 4, "end": 7}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross", "start": 9, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 15, "end": 24}]}]}
{"content": "We use a bipolar activation function given by f x = tanh\u00ce\u00b1x , with \u00ce\u00b1 = 0.2 , momentum of 0.4 , and the back-propagation method with relatively slow learning learning rate 0.001 to optimize the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 17, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 78, "end": 85}]}, {"label": ["B-DeepLearning"], "points": [{"text": "back-propagation", "start": 104, "end": 119}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 158, "end": 165}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 194, "end": 200}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 28, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 87, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "0.4", "start": 90, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 167, "end": 170}]}]}
{"content": "To determine the quality of the prediction we use the Mean Absolute Error MAE in degrees , Pearson\u00e2\u20ac\u2122s correlation coefficient pc , and the probability that the predicted and native angles are separated by less than 10% Q10p ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Mean", "start": 54, "end": 57}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MAE", "start": 74, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Absolute", "start": 59, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Error", "start": 68, "end": 72}]}]}
{"content": "We use 10-fold cross-validations [ 35 ] to estimate the accuracy over the set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "10-fold", "start": 7, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validations", "start": 15, "end": 31}]}]}
{"content": "To test for possible overfit issues we take secondary structure predictions based on the weights trained for the first cross-validation fold and compare angle prediction between the folds ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfit", "start": 21, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 89, "end": 95}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 119, "end": 134}]}]}
{"content": "Firpi et al. 2010 developed a method called CSI-ANN based on a time-delayed neural network TDNN framework to predict enhancers in HeLa and Human CD4+ T cells ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "time-delayed", "start": 63, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "TDNN", "start": 91, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 76, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 83, "end": 89}]}]}
{"content": "A model particularly well suited for the tasks of interest is the Bidirectional Recurrent Neural Network BRNN [ 6 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 2, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Bidirectional", "start": 66, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 105, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 80, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 90, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 97, "end": 103}]}]}
{"content": "The model they used is a fully connected Multi-Layer Perceptron MLP with one hidden layer [ 4 ] Haykin ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 25, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multi-Layer", "start": 41, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 64, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 77, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 31, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 53, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 84, "end": 88}]}]}
{"content": "They randomly weighted each input and used back-propagation [ 4 ] to train the system on a PDB file chosen from the PDB ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "back-propagation", "start": 43, "end": 58}]}]}
{"content": "Moreover , owing to the rapid growth of the PDB database , the training sets for ANNs have also increased ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "PDB", "start": 44, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 63, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 81, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 48, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 72, "end": 75}]}]}
{"content": "Since these artificial intelligence methods are based on an empirical risk-minimization principle , they have some disadvantages , for example , finding the local minimal instead of global minimal having low convergence rate more prone to overfitting and when the size of fault samples is limited , it might cause poor generalization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "local", "start": 157, "end": 161}]}, {"label": ["B-DeepLearning"], "points": [{"text": "global", "start": 182, "end": 187}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 208, "end": 218}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 239, "end": 249}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 319, "end": 332}]}, {"label": ["I-DeepLearning"], "points": [{"text": "minimal", "start": 163, "end": 169}, {"start": 189, "end": 195, "text": "minimal"}]}]}
{"content": "As a collaborative filter , we use General Regression Neural Network GRNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 35, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 69, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 43, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 54, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 61, "end": 67}]}]}
{"content": "In the literature , GRNN was shown to be effective in noisy environments as it deals with sparse data effectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sparse", "start": 90, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 97, "end": 100}]}]}
{"content": "The method makes use of RDKit , an open-source cheminformatics software , allowing the incorporation of any global molecular such as molecular charge and local such as atom type information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RDKit", "start": 24, "end": 28}]}]}
{"content": "We evaluate the method on the Side Effect Resource SIDE v4.1 dataset and show that it significantly outperforms another recently proposed method based on deep convolutional neural networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Side", "start": 30, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 159, "end": 171}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Effect", "start": 35, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Resource", "start": 42, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "SIDE", "start": 51, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "v4.1", "start": 56, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 61, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 173, "end": 178}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 180, "end": 187}]}]}
{"content": "Surprisingly , while being one of the best performing methods on all other datasets in [ 19 ] their presented DCNN performed very poorly on the SIDER dataset and were outperformed by traditional methods such as random forest and logistic regression ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "datasets", "start": 75, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DCNN", "start": 110, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SIDER", "start": 144, "end": 148}]}, {"label": ["B-DeepLearning"], "points": [{"text": "logistic", "start": 229, "end": 236}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 150, "end": 156}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regression", "start": 238, "end": 247}]}]}
{"content": "Several authors overcome this flaw by either using a recurrent neural network RNN or a DCNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 53, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 78, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DCNN", "start": 87, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 70, "end": 76}]}]}
{"content": "Wallach et al. [ 17 ] for example apply a three dimensional DCNN on the spatial structure of molecules ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DCNN", "start": 60, "end": 63}]}]}
{"content": "Due to the conventions of the field , F will be defined as the leaky ReLU function [ 12 ] in the rest of this paper ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "leaky", "start": 63, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ReLU", "start": 69, "end": 72}]}]}
{"content": "The H in Eq. 5 represents an arbitrary activation function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 39, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 50, "end": 57}]}]}
{"content": "In this paper the Leaky ReLU function will be used for Hl except in the final layer where the sigmoid function will be used instead ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Leaky", "start": 18, "end": 22}]}, {"label": ["B-DeepLearning"], "points": [{"text": "final", "start": 72, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 94, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ReLU", "start": 24, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 78, "end": 82}]}]}
{"content": "The sigmoid function is selected here since we want the output of our model to be in the range of 0 to 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 70, "end": 74}]}]}
{"content": "To train and test our proposed model we use the publicly available SIDER dataset presented in [ 19 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 31, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SIDER", "start": 67, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 73, "end": 79}]}]}
{"content": "This dataset contains information on molecules , marketed as medicines , and their recorded side effects ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 5, "end": 11}]}]}
{"content": "The data originates from the SIDER database ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SIDER", "start": 29, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "database", "start": 35, "end": 42}]}]}
{"content": "The original data consist of 1430 molecules and 5868 different types of side effects ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "1430", "start": 29, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "5868", "start": 48, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "molecules", "start": 34, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "different", "start": 53, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "types", "start": 63, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 69, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "side", "start": 72, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "effects", "start": 77, "end": 83}]}]}
{"content": "However , in the dataset presented by Wu et al. [ 19 ] , similar side effects are grouped together , leaving the dataset with 28 groups of side effects ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dataset", "start": 17, "end": 23}, {"start": 113, "end": 119, "text": "dataset"}]}]}
{"content": "RDKit is also used to extract information about the atoms , bonds and molecules , such as chirality and molecular weight ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RDKit", "start": 0, "end": 4}]}]}
{"content": "The model described earlier in Sect. 3 is implemented in Theano [ 15 ] and with a network architecture that is as similar as possible as the architecture presented by Wu et al. [ 19 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Theano", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 82, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architecture", "start": 90, "end": 101}]}]}
{"content": "Two convolutional layers , each with 64 neurons , will therefore be used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Two", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "64", "start": 37, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolutional", "start": 4, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 18, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 40, "end": 46}]}]}
{"content": "These layers are then followed by a single fully connected layer , with 128 neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 43, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "128", "start": 72, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 49, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 59, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 76, "end": 82}]}]}
{"content": "Between each layer in our model we use a 10% dropout rate [ 14 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 26, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 45, "end": 51}]}]}
{"content": "The models are trained by minimizing the cross entropy error ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "models", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 41, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "error", "start": 55, "end": 59}]}]}
{"content": "This is achieved by optimizing the values of all free parameters Wl and V l using the ADAM optimization algorithm [ 7 ] ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 54, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ADAM", "start": 86, "end": 89}]}]}
{"content": "Each network is trained for 200 epochs using a batch size of 20 examples ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "200", "start": 28, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 47, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "epochs", "start": 32, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 53, "end": 56}]}]}
{"content": "The performance of our model on the presented experimental set-ups is measured as the mean AUC-ROC value , averaged over all trials and all target variables ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 23, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC-ROC", "start": 91, "end": 97}]}]}
{"content": "The network mainly consists of three convolution blocks and two fully-connected blocks , as well as flattening and concatenating layers that bridge the two types of blocks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "three", "start": 31, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "two", "start": 60, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 37, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "blocks", "start": 49, "end": 54}, {"start": 80, "end": 85, "text": "blocks"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "fully-connected", "start": 64, "end": 78}]}]}
{"content": "Each convolution block is composed of a convolution layer with ReLU activation , followed by a max pooling layer , and a dropout layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 5, "end": 15}, {"start": 40, "end": 50, "text": "convolution"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 63, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "block", "start": 17, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 52, "end": 56}]}]}
{"content": "The first convolution layer filters the input of size 1280 \u00c3\u2014 1024 \u00c3\u2014 2 using 16 kernels of size 10 \u00c3\u2014 10 with a horizontal stride of 10 pixels and a vertical stride of 8 pixels ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 10, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 81, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 124, "end": 129}, {"start": 159, "end": 164, "text": "stride"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 22, "end": 26}]}]}
{"content": "The result image of size 128\u00c3\u2014128\u00c3\u201416 is then shrunk to 64\u00c3\u201464\u00c3\u201416 in the following max pooling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "image", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 84, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 88, "end": 94}]}]}
{"content": "The second convolution layer filters it using 32 kernels of size 3 \u00c3\u2014 3 with a stride of 2 pixels ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 11, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 49, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 79, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 23, "end": 27}]}]}
{"content": "Through a similar process , the input of the third convolution layer becomes of size 16\u00c3\u201416\u00c3\u201432 and at the end of the block , the input is flattened to a vector of length 1024 from 4 \u00c3\u2014 4 \u00c3\u2014 64 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 51, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 154, "end": 159}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 63, "end": 67}]}]}
{"content": "The vector is then concatenated with the second input , a vector of eight metrics , and delivered to a fully-connected layer of 512 neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 4, "end": 9}, {"start": 58, "end": 63, "text": "vector"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully-connected", "start": 103, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "512", "start": 128, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 119, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 132, "end": 138}]}]}
{"content": "Finally , the vector of length 512 goes through a sigmoid function to generate the output ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "vector", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 50, "end": 56}]}]}
{"content": "We conducted the self-assembly experiment for four hours and recorded images of size 1280 \u00c3\u2014 1024 at 1 fps , leading to over 14000 pairs of front and right images ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 31, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 70, "end": 75}, {"start": 156, "end": 161, "text": "images"}]}]}
{"content": "In total , 725 positive real pairs , 2475 negative real pairs , 12000 positive synthetic pairs , and 12000 negative synthetic pairs are utilized in our experiment ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "experiment", "start": 152, "end": 161}]}]}
{"content": "Note that the presented accuracies are the mean accuracy over 10-fold cross validation except the case when only the synthetic data is used as a training set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "10-fold", "start": 62, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 145, "end": 152}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross", "start": 70, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 76, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 154, "end": 156}]}]}
{"content": "Our algorithm reaches an accuracy of 93.2% when it is trained with labeled real images ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 25, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 80, "end": 85}]}]}
{"content": "It justifies our utilization of synthetic data , as it recorded 88.7% accuracy when only the real data are used as a training set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 70, "end": 77}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 117, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 126, "end": 128}]}]}
{"content": "Our classifier also shows promising results with synthetic images , reaching an accuracy of 86% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "images", "start": 59, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 80, "end": 87}]}]}
