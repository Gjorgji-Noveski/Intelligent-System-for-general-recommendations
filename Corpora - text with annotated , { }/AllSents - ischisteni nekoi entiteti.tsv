The	O
initial	O
learning	B-DeepLearning
rate	I-DeepLearning
was	O
set	O
to	O
0.0005	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
Deep	O
Voice	O
3	O
is	O
a	O
fully	O
convolutional	B-DeepLearning
architecture	O
for	O
speech	O
synthesis	O
.	O
Its	O
character-to-spectrogram	O
architecture	O
enables	O
fully	O
parallel	O
computation	O
and	O
the	O
training	O
is	O
much	O
faster	O
than	O
at	O
the	O
RNN	B-DeepLearning
architectures	O
.	O
Those	O
features	O
are	O
in	O
a	O
key	O
,	O
value	O
form	O
and	O
they	O
are	O
fed	O
into	O
the	O
attention-based	B-DeepLearning
decoder	I-DeepLearning
.	O
The	O
hidden	B-DeepLearning
layers	I-DeepLearning
of	O
the	O
decoder	B-DeepLearning
are	O
fed	O
into	O
the	O
third	O
converter	O
layer	O
,	O
which	O
is	O
capable	O
of	O
predicting	O
the	O
acoustic	O
features	O
for	O
waveform	O
synthesis	O
.	O
The	O
training	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
model	I-DeepLearning
is	O
followed	O
by	O
validation	O
,	O
which	O
means	O
that	O
each	O
10K	B-DeepLearning
steps	I-DeepLearning
are	O
evaluated	O
before	O
the	O
training	O
process	O
proceeds	O
.	O
The	O
evaluation	O
is	O
done	O
on	O
external	O
,	O
unknown	O
sentences	O
that	O
provide	O
insight	O
into	O
the	O
advancement	O
of	O
the	O
learnt	O
dependencies	O
between	O
the	O
dataset	B-DeepLearning
and	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
weights	B-DeepLearning
.	O
Deep	O
Voice	O
3	O
suggested	O
that	O
parameters	B-DeepLearning
worked	O
perfectly	O
for	O
our	O
model	B-DeepLearning
,	O
thus	O
we	O
used	O
the	O
same	O
hyperparameters	B-DeepLearning
without	O
increasing	O
the	O
demand	O
due	O
to	O
our	O
resource	O
limitations	O
.	O
The	O
model	B-DeepLearning
started	O
to	O
produce	O
an	O
intelligible	O
,	O
understandable	O
,	O
and	O
partially	O
human-like	O
speech	O
after	O
50	B-DeepLearning
K	I-DeepLearning
steps	I-DeepLearning
,	O
as	O
observed	O
from	O
the	O
figure	O
.	O
Loss	B-DeepLearning
function	I-DeepLearning
is	O
a	O
metric	O
that	O
refers	O
to	O
the	O
accuracy	B-DeepLearning
of	O
the	O
prediction	O
.	O
The	O
main	O
objective	O
is	O
to	O
minimize	O
the	O
model	B-DeepLearning
errors	O
or	O
minimize	O
the	O
loss	B-DeepLearning
function	I-DeepLearning
.	O
In	O
our	O
case	O
,	O
the	O
loss	B-DeepLearning
functions	I-DeepLearning
behaves	O
in	O
a	O
desired	O
manner	O
,	O
it	O
gradually	O
decreases	O
,	O
converging	B-DeepLearning
to	O
a	O
value	O
of	O
0.1731	O
after	O
162.2	B-DeepLearning
K	I-DeepLearning
steps	I-DeepLearning
in	O
four	O
days	O
and	O
11	O
h	O
of	O
training	O
.	O
Learning	B-DeepLearning
rate	I-DeepLearning
plays	O
a	O
vital	O
role	O
in	O
minimizing	O
the	O
loss	B-DeepLearning
function	I-DeepLearning
.	O
The	O
gradient	B-DeepLearning
norm	I-DeepLearning
that	O
is	O
presented	O
in	O
the	O
same	O
figure	O
calculates	O
the	O
L2	B-DeepLearning
norm	I-DeepLearning
of	O
the	O
gradients	B-DeepLearning
of	O
the	O
last	B-DeepLearning
layer	I-DeepLearning
of	O
the	O
Deep	B-DeepLearning
learning	I-DeepLearning
network	I-DeepLearning
.	O
It	O
is	O
an	O
indicator	O
showing	O
whether	O
the	O
weights	B-DeepLearning
of	O
the	O
Deep	B-DeepLearning
learning	I-DeepLearning
network	I-DeepLearning
are	O
properly	O
updated	O
.	O
This	O
problem	O
affects	O
the	O
upper	O
layers	O
of	O
the	O
Deep	B-DeepLearning
learning	I-DeepLearning
network	I-DeepLearning
,	O
making	O
it	O
really	O
hard	O
for	O
the	O
network	O
to	O
learn	O
and	O
tune	B-DeepLearning
the	I-DeepLearning
parameters	I-DeepLearning
.	O
In	O
such	O
case	O
,	O
the	O
model	B-DeepLearning
is	O
unstable	O
and	O
it	O
is	O
not	O
able	O
to	O
learn	O
from	O
data	O
,	O
since	O
the	O
accumulation	O
of	O
large	O
error	B-DeepLearning
gradients	I-DeepLearning
during	O
the	O
training	O
process	O
result	O
in	O
very	O
large	O
updates	O
in	O
the	O
Deep	B-DeepLearning
learning	I-DeepLearning
model	I-DeepLearning
weights	I-DeepLearning
.	O
By	O
principles	O
of	O
transfer	B-DeepLearning
learning	I-DeepLearning
we	O
tried	O
to	O
fine-tune	O
the	O
Russian	O
TTS	B-DeepLearning
model	I-DeepLearning
.	O
So	O
as	O
to	O
automatically	O
and	O
accurately	O
classify	O
the	O
FCGR	O
encoded	O
data	O
,	O
we	O
experimentally	O
compared	O
Multilayer	B-DeepLearning
Perceptron	I-DeepLearning
Artificial	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
MLP-ANN	B-DeepLearning
,	O
Support	O
Vector	O
Machine	O
SVM	O
and	O
Naïve	O
Bayes	O
NB	O
,	O
which	O
are	O
frontline	O
pattern	O
recognition	O
tools	O
in	O
machine	B-DeepLearning
learning	I-DeepLearning
.	O
The	O
MLP-ANN	B-DeepLearning
contains	O
64	B-DeepLearning
neurons	I-DeepLearning
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
64	O
element	O
FCGR	O
,	O
6	B-DeepLearning
neurons	I-DeepLearning
in	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
5	O
mutation	O
classes	O
and	O
1	O
normal	O
class	O
and	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
with	O
the	O
neurons	O
experimentally	O
varied	O
from	O
10	O
to	O
100	O
.	O
The	O
result	O
obtained	O
by	O
varying	O
the	O
neurons	O
in	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
is	O
reported	O
in	O
Sect	O
3	O
.	O
Deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
DNN	B-DeepLearning
or	O
deep	B-DeepLearning
learning	I-DeepLearning
models	I-DeepLearning
,	O
were	O
proved	O
to	O
be	O
able	O
to	O
extract	O
automatically	O
useful	O
features	O
from	O
input	O
patterns	O
.	O
Under	O
this	O
framework	O
,	O
Long	B-DeepLearning
Short-Term	I-DeepLearning
Memory	I-DeepLearning
LSTM	B-DeepLearning
is	O
a	O
recurrent	B-DeepLearning
unit	I-DeepLearning
that	O
reads	O
a	O
sequence	O
one	O
step	O
at	O
a	O
time	O
and	O
can	O
exploit	O
long	O
range	O
relations	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
DNN	B-DeepLearning
model	I-DeepLearning
for	O
nucleosome	O
identification	O
on	O
sequences	O
from	O
three	O
different	O
species	O
.	O
Recently	O
deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
or	O
deep	B-DeepLearning
learning	I-DeepLearning
models	I-DeepLearning
,	O
were	O
proved	O
to	O
be	O
able	O
to	O
automatically	O
extract	O
useful	O
features	O
from	O
input	O
patterns	O
with	O
no	O
a	O
priori	O
information	O
.	O
The	O
two	O
main	O
categories	O
of	O
deep	B-DeepLearning
neural	I-DeepLearning
models	I-DeepLearning
are	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
CNN	B-DeepLearning
and	O
Recurrent	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
RNN	B-DeepLearning
.	O
CNNs	B-DeepLearning
are	O
characterized	O
by	O
an	O
initial	B-DeepLearning
layer	I-DeepLearning
of	O
convolutional	B-DeepLearning
filters	I-DeepLearning
,	O
followed	O
by	O
a	O
non	O
Linearity	O
,	O
a	O
sub-sampling	O
,	O
and	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
which	O
realized	O
the	O
final	O
classification	O
.	O
Conversely	O
,	O
in	O
this	O
work	O
we	O
want	O
to	O
avoid	O
the	O
feature	O
extraction	O
step	O
in	O
order	O
to	O
fully	O
exploit	O
the	O
capabilities	O
of	O
DNNs	B-DeepLearning
,	O
making	O
use	O
of	O
a	O
convolutional	B-DeepLearning
layer	I-DeepLearning
for	O
extracting	O
features	O
from	O
local	O
sequences	O
of	O
nucleotides	O
,	O
and	O
an	O
LSTM	B-DeepLearning
to	O
take	O
into	O
account	O
longer-range	O
positional	O
information	O
.	O
Another	O
important	O
component	O
of	O
a	O
deep	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
is	O
the	O
max-pooling	B-DeepLearning
layer	O
,	O
that	O
usually	O
follows	O
the	O
recurrent	O
or	O
convolutional	B-DeepLearning
layers	I-DeepLearning
in	O
the	O
computation	O
flow	O
.	O
The	O
dropout	B-DeepLearning
layer	I-DeepLearning
randomly	O
sets	O
to	O
zero	O
the	O
output	O
from	O
the	O
preceding	O
layer	O
during	O
training	O
,	O
with	O
a	O
probability	O
p	O
given	O
as	O
a	O
fixed	O
parameter	B-DeepLearning
.	O
When	O
p	O
=	O
0.5	O
,	O
it	O
is	O
equivalent	O
to	O
training	O
2|W|	O
networks	O
with	O
shared	O
parameters	B-DeepLearning
,	O
where	O
|W|	O
is	O
the	O
number	O
of	O
neurons	O
subject	O
to	O
dropout	B-DeepLearning
.	O
This	O
results	O
in	O
a	O
strong	O
regularization	B-DeepLearning
effect	O
,	O
which	O
helps	O
in	O
preventing	O
overfitting	B-DeepLearning
.	O
We	O
propose	O
three	O
kind	O
of	O
architectures	O
,	O
obtained	O
by	O
the	O
composition	O
of	O
six	O
kinds	O
of	O
neural	B-DeepLearning
layers:	I-DeepLearning
a	O
convolutional	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
max	B-DeepLearning
pooling	I-DeepLearning
layer	I-DeepLearning
,	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
long	B-DeepLearning
short-term	I-DeepLearning
memory	I-DeepLearning
LSTM	B-DeepLearning
layer	O
,	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
and	O
a	O
softmax	B-DeepLearning
layer	O
.	O
The	O
Max	B-DeepLearning
Pooling	I-DeepLearning
operation	O
with	O
width	O
and	O
stride	B-DeepLearning
2	O
helps	O
to	O
capture	O
the	O
most	O
salient	O
features	O
extracted	O
by	O
the	O
previous	O
layer	O
and	O
reduces	O
the	O
output	O
size	O
from	O
145	O
to	O
72	O
vectors	B-DeepLearning
.	O
The	O
Dropout	B-DeepLearning
operation	O
with	O
probability	O
p	O
=	O
0.5	O
is	O
used	O
to	O
prevent	O
overfitting	B-DeepLearning
during	O
the	O
training	B-DeepLearning
phase	I-DeepLearning
.	O
We	O
notice	O
that	O
the	O
best	O
architecture	O
is	O
the	O
CONV-LSTM-FCX2	B-DeepLearning
.	O
We	O
have	O
proposed	O
a	O
novel	O
model	B-DeepLearning
parameter	I-DeepLearning
training	O
scheme	O
based	O
on	O
the	O
concepts	O
of	O
quantum	O
computing	O
.	O
We	O
apply	O
deep	B-DeepLearning
learning	I-DeepLearning
methods	I-DeepLearning
in	O
this	O
paper	O
,	O
namely	O
we	O
use	O
convolutional	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
CNNs	B-DeepLearning
for	O
description	O
and	O
prediction	O
of	O
the	O
red	O
blood	O
cells’	O
trajectory	O
,	O
which	O
is	O
crucial	O
in	O
modeling	O
of	O
a	O
blood	O
flow	O
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
sets	I-DeepLearning
used	O
for	O
neural	B-DeepLearning
network	I-DeepLearning
are	O
extracted	O
from	O
simulations	O
which	O
differ	O
only	O
in	O
initial	O
seeding	B-DeepLearning
of	O
the	O
cells	O
.	O
Besides	O
using	O
convolution	B-DeepLearning
or	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
,	O
we	O
also	O
used	O
a	O
relatively	O
new	O
type	O
of	O
layers	O
,	O
the	O
dense	B-DeepLearning
convolution	I-DeepLearning
layers	I-DeepLearning
,	O
introduced	O
in	O
.	O
Used	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
hyperparameters	B-DeepLearning
are	O
these:	O
Weights	B-DeepLearning
are	O
initialized	O
in	O
the	O
range	O
xavier	O
,	O
bias	B-DeepLearning
is	O
set	O
to	O
0.	O
Learning	B-DeepLearning
rate	I-DeepLearning
is	O
0.0002	O
,	O
λ1	O
=	O
λ2	O
=	O
0.000001	O
,	O
dropout	B-DeepLearning
=	I-DeepLearning
0.02	I-DeepLearning
and	O
minibatch	B-DeepLearning
size	I-DeepLearning
is	O
32	O
.	O
The	O
training	B-DeepLearning
phase	I-DeepLearning
lasts	O
about	O
6	O
hours	O
for	O
more	O
complicated	O
network	B-DeepLearning
architectures	I-DeepLearning
.	O
The	O
convolution	B-DeepLearning
region	I-DeepLearning
of	O
the	O
CNNs	B-DeepLearning
includes	O
the	O
convolution	B-DeepLearning
,	O
the	O
activation	B-DeepLearning
and	O
the	O
pooling	B-DeepLearning
layers	I-DeepLearning
.	O
Different	O
techniques	O
were	O
used	O
to	O
improve	O
the	O
accuracy:	B-DeepLearning
Learning	B-DeepLearning
rate	I-DeepLearning
tuning	O
,	O
which	O
controls	O
the	O
update	O
of	O
the	O
CNNs	B-DeepLearning
weights	O
.	O
The	O
neural	B-DeepLearning
networks	I-DeepLearning
are	O
updated	O
via	O
the	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
.	O
Data	B-DeepLearning
Augmentation	I-DeepLearning
,	O
that	O
consists	O
in	O
the	O
realization	O
of	O
different	O
random	O
operations	O
of	O
rotation	O
,	O
translation	O
and	O
zoom	O
on	O
the	O
images	B-DeepLearning
to	O
avoid	O
overfitting	B-DeepLearning
and	O
improve	O
generalization	B-DeepLearning
.	O
Fine-Tuning	B-DeepLearning
,	O
that	O
is	O
based	O
on	O
defrosting	O
the	O
weights	B-DeepLearning
of	O
the	O
convolutional	B-DeepLearning
layers	I-DeepLearning
,	O
allowing	O
the	O
network	O
to	O
train	O
in	O
its	O
integrity	O
.	O
It	O
is	O
applied	O
using	O
a	O
differential	O
learning	B-DeepLearning
rate	I-DeepLearning
,	O
that	O
is	O
,	O
introducing	O
three	O
different	O
and	O
successively	O
higher	O
values	O
of	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
,	O
thus	O
taking	O
into	O
account	O
the	O
differential	O
knowledge	O
of	O
the	O
layers	O
.	O
Therefore	O
,	O
although	O
Resnet50	B-DeepLearning
provided	O
the	O
highest	O
accuracy	B-DeepLearning
is	O
less	O
stable	O
than	O
Resnet	B-DeepLearning
34	O
1.09%	O
vs	O
063%	O
.	O
Table	O
2	O
provides	O
the	O
corresponding	O
confusion	B-DeepLearning
matrices	I-DeepLearning
in	O
validation	O
.	O
With	O
respect	O
to	O
the	O
tune	O
of	O
models’	B-DeepLearning
hyper-parameters	B-DeepLearning
,	O
the	O
R	O
package	O
mlrMBO	O
was	O
used	O
to	O
perform	O
a	O
Bayesian	O
optimization	O
within	O
the	O
train	B-DeepLearning
set	I-DeepLearning
.	O
This	O
package	O
implements	O
a	O
Bayesian	O
optimization	O
of	O
black-box	O
functions	O
which	O
allows	O
to	O
find	O
faster	O
an	O
optimal	O
hyper-parameters	B-DeepLearning
setting	O
in	O
contrast	O
to	O
traditional	O
hyperparameters	B-DeepLearning
search	O
strategies	O
such	O
as	O
grid	B-DeepLearning
search	I-DeepLearning
highly	O
time	O
consuming	O
when	O
more	O
than	O
3	O
hyper-parameters	B-DeepLearning
are	O
tuned	O
or	O
random	B-DeepLearning
search	I-DeepLearning
not	O
efficient	O
enough	O
since	O
similar	O
or	O
non-sense	O
hyper-parameters	B-DeepLearning
settings	O
might	O
be	O
tested	O
.	O
Table	O
1	O
shows	O
the	O
average	O
AUC	B-DeepLearning
performance	O
,	O
standard	O
deviation	O
and	O
number	O
of	O
genes	O
retained	O
by	O
the	O
different	O
models	B-DeepLearning
tested	O
over	O
the	O
test	O
sets	O
of	O
the	O
cross-validation	B-DeepLearning
setting	O
.	O
The	O
proposed	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
CNN	B-DeepLearning
is	O
consisting	O
of	O
two	O
parallel	O
convolutional	B-DeepLearning
layers	I-DeepLearning
taking	O
as	O
inputs	O
transversal	O
,	O
coronal	O
and	O
axial	O
slices	O
acquired	O
before	O
and	O
after	O
chemotherapy	O
for	O
each	O
patient	B-DeepLearning
.	O
As	O
illustrated	O
in	O
Fig.	O
2	O
,	O
this	O
architecture	O
contains	O
two	O
similar	O
branches	O
,	O
each	O
one	O
contains	O
4	O
blocks	O
of	O
2D	B-DeepLearning
convolution	I-DeepLearning
followed	O
by	O
an	O
activation	B-DeepLearning
function	I-DeepLearning
ReLU	B-DeepLearning
and	O
a	O
Max	B-DeepLearning
pooling	I-DeepLearning
layer	O
.	O
In	O
the	O
first	O
and	O
second	O
blocks	O
,	O
32	O
kernels	B-DeepLearning
were	O
used	O
for	O
each	O
convolutional	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
parallel	O
Deep	B-DeepLearning
learning	I-DeepLearning
architecture	O
was	O
applied	O
for	O
each	O
view	O
using	O
corresponding	O
slices	O
before	O
and	O
after	O
the	O
first	O
chemotherapy	O
.	O
Consequently	O
,	O
we	O
used	O
the	O
Stochastic	B-DeepLearning
Gradient	I-DeepLearning
Descent	I-DeepLearning
SGD	B-DeepLearning
with	O
a	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
00052	O
.	O
A	O
learning	B-DeepLearning
rate	I-DeepLearning
decay	O
of	O
3.46e−5	O
was	O
used	O
to	O
schedule	O
a	O
best	O
accuracy	B-DeepLearning
.	O
To	O
compile	O
the	O
model	B-DeepLearning
,	O
a	O
categorical	O
cross	B-DeepLearning
entropy	I-DeepLearning
was	O
used	O
as	O
loss	B-DeepLearning
function	I-DeepLearning
and	O
standard	O
accuracy	O
was	O
used	O
as	O
a	O
metric	O
.	O
To	O
avoid	O
results’	O
bias	B-DeepLearning
,	O
a	O
5-Fold	O
stratified	O
cross	B-DeepLearning
validation	I-DeepLearning
with	O
AUC	B-DeepLearning
as	O
metric	O
was	O
used	O
.	O
Within	O
150	O
epochs	B-DeepLearning
with	O
5-fold	O
stratified	O
cross	B-DeepLearning
validation	I-DeepLearning
,	O
an	O
accuracy	B-DeepLearning
of	O
90.03	O
was	O
obtained	O
using	O
20%	O
of	O
3D	O
validation	B-DeepLearning
data	I-DeepLearning
.	O
An	O
overfitting	B-DeepLearning
was	O
observed	O
during	O
training	O
when	O
using	O
only	O
one	O
of	O
the	O
views	O
without	O
data	B-DeepLearning
augmentation	I-DeepLearning
.	O
Besides	O
the	O
work	O
we	O
did	O
on	O
building	O
other	O
types	O
of	O
agents	O
we	O
have	O
also	O
tried	O
to	O
explore	O
in	O
more	O
depth	O
different	O
cognitive	O
and	O
affective	O
models	O
of	O
agents	O
,	O
including	O
symbolic	O
BDI	O
models	O
as	O
well	O
as	O
neural	B-DeepLearning
network	I-DeepLearning
models	I-DeepLearning
.	O
While	O
we	O
are	O
defining	O
FSTs	O
,	O
and	O
after	O
having	O
introduced	O
the	O
use	O
of	O
HMMs	O
as	O
stochastic	O
FSTs	O
,	O
it	O
is	O
worth	O
noticing	O
that	O
regular	O
transduction	O
rules	O
can	O
also	O
be	O
implemented	O
in	O
the	O
form	O
of	O
so-called	O
multilayer	B-DeepLearning
perceptrons	I-DeepLearning
MLP	B-DeepLearning
a	O
particular	O
type	O
of	O
artificial	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
.	O
Artificial	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
are	O
based	O
on	O
simplistic	O
models	O
for	O
biological	O
neuron	O
behavior	O
and	O
the	O
interconnections	O
between	O
these	O
neurons	O
.	O
Most	O
often	O
,	O
the	O
nonlinear	O
function	O
is	O
a	O
limiter	O
or	O
a	O
sigmoid	B-DeepLearning
function	O
.	O
The	O
MLP	B-DeepLearning
is	O
the	O
far	O
most	O
widely	O
used	O
network	O
.	O
It	O
is	O
composed	O
of	O
an	O
input	B-DeepLearning
layer	I-DeepLearning
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
separated	O
by	O
one	O
or	O
more	O
hidden	B-DeepLearning
layers	I-DeepLearning
of	O
nodes	B-DeepLearning
,	O
with	O
each	O
layer	O
connected	O
to	O
the	O
next	O
layer	O
,	O
feeding	O
its	O
node	B-DeepLearning
values	O
forward	O
Fig.	O
24	O
.	O
In	O
many	O
domains	O
,	O
neural	B-DeepLearning
networks	I-DeepLearning
are	O
an	O
effective	O
alternative	O
to	O
statistical	O
methods	O
.	O
James	O
Henderson	O
has	O
identified	O
a	O
suitable	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
for	O
natural	O
language	O
parsing	O
,	O
called	O
Simple	B-DeepLearning
Synchrony	I-DeepLearning
Networks	I-DeepLearning
SSNs	B-DeepLearning
,	O
which	O
he	O
discusses	O
in	O
chapter	O
6	O
,	O
A	O
Neural	B-DeepLearning
Network	I-DeepLearning
Parser	O
that	O
Handles	O
Sparse	B-DeepLearning
Data	I-DeepLearning
.	O
Because	O
neural	B-DeepLearning
networks	I-DeepLearning
learn	O
their	O
own	O
internal	O
representations‚	O
neural	B-DeepLearning
networks	I-DeepLearning
can	O
decide	O
automatically	O
what	O
features	O
to	O
count	O
and	O
how	O
reliable	O
they	O
are	O
for	O
estimating	O
the	O
desired	O
probabilities	O
.	O
This	O
generalization	B-DeepLearning
ability	O
is	O
a	O
result	O
of	O
using	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
method	O
for	O
representing	O
sets	O
of	O
objects‚	O
called	O
Temporal	O
Synchrony	O
Variable	O
Binding	O
TSVB	O
Shastri	O
and	O
Ajjanagadde‚	O
1993	O
.	O
SRNs	B-DeepLearning
can	O
learn	O
generalizations	B-DeepLearning
over	O
positions	O
in	O
an	O
input	O
sequence	O
and	O
thus	O
can	O
handle	O
unbounded	O
input	O
sequences‚	O
which	O
has	O
made	O
them	O
of	O
interest	O
in	O
natural	O
language	O
processing	O
.	O
By	O
using	O
TSVB	O
to	O
represent	O
the	O
constituents	O
in	O
a	O
syntactic	O
structure‚	O
SSNs	B-DeepLearning
also	O
learn	O
generalizations	B-DeepLearning
over	O
structural	O
constituents	O
.	O
The	O
linguistic	O
relevance	O
of	O
this	O
class	O
of	O
generalizations	B-DeepLearning
is	O
what	O
accounts	O
for	O
the	O
fact	O
that	O
SSNs	B-DeepLearning
generalize	O
from	O
training	B-DeepLearning
set	I-DeepLearning
to	O
testing	B-DeepLearning
set	I-DeepLearning
in	O
an	O
appropriate	O
way‚	O
as	O
demonstrated	O
in	O
Section	O
4	O
.	O
In	O
this	O
section	O
we	O
briefly	O
outline	O
the	O
SSN	B-DeepLearning
architecture	O
and	O
how	O
it	O
can	O
be	O
used	O
to	O
estimate	O
the	O
parameters	B-DeepLearning
of	O
a	O
probability	O
model	O
.	O
Simple	B-DeepLearning
Recurrent	I-DeepLearning
Networks	I-DeepLearning
extend	O
MLPs	B-DeepLearning
to	O
sequences	O
by	O
using	O
the	O
hidden	O
representations	O
as	O
representations	O
of	O
the	O
network’s	O
state	O
at	O
a	O
given	O
point	O
in	O
the	O
sequence	O
.	O
Simple	B-DeepLearning
Synchrony	I-DeepLearning
Networks	I-DeepLearning
extend	O
SRNs	B-DeepLearning
by	O
computing	O
one	O
of	O
these	O
sequences	O
for	O
each	O
object	O
in	O
a	O
set	O
of	O
objects	O
.	O
The	O
most	O
important	O
feature	O
of	O
any	O
learning	O
architecture	O
is	O
how	O
it	O
generalizes	B-DeepLearning
from	O
training	B-DeepLearning
data	I-DeepLearning
to	O
testing	B-DeepLearning
data	I-DeepLearning
.	O
SRNs	B-DeepLearning
are	O
popular	O
for	O
sequence	O
processing	O
because	O
they	O
inherently	O
generalize	O
over	O
sequence	O
positions	O
.	O
Because	O
inputs	O
are	O
fed	O
to	O
an	O
SRN	B-DeepLearning
one	O
at	O
a	O
time‚	O
and	O
the	O
same	O
trained	O
parameters	B-DeepLearning
called	O
link	O
weights	O
apply	O
at	O
every	O
time‚	O
information	O
learned	O
at	O
one	O
sequence	O
position	O
will	O
inherently	O
be	O
generalized	O
to	O
other	O
sequence	O
positions	O
.	O
This	O
generalization	B-DeepLearning
ability	O
manifests	O
itself	O
in	O
the	O
fact	O
that	O
SRNs	B-DeepLearning
can	O
handle	O
arbitrarily	O
long	O
sequences	O
.	O
This	O
generalization	B-DeepLearning
ability	O
manifests	O
itself	O
in	O
the	O
fact	O
that	O
these	O
networks	O
can	O
handle	O
arbitrarily	O
many	O
constituents‚	O
and	O
therefore	O
unbounded	O
phrase	O
structure	O
trees	O
.	O
After	O
the	O
parent	O
of	O
a	O
terminal	O
is	O
chosen	O
it	O
is	O
used	O
by	O
the	O
SSN	B-DeepLearning
to	O
estimate	O
the	O
parameters	B-DeepLearning
for	O
latter	O
portions	O
of	O
the	O
sentence‚	O
including	O
latter	O
parent	O
estimates	O
.	O
We	O
also	O
bias	B-DeepLearning
the	O
network	B-DeepLearning
training	I-DeepLearning
by	O
providing	O
a	O
new-nonterminal	O
input	O
unit	O
.	O
To	O
test	O
the	O
ability	O
of	O
Simple	B-DeepLearning
Synchrony	I-DeepLearning
Networks	I-DeepLearning
to	O
handle	O
sparse	B-DeepLearning
data	I-DeepLearning
we	O
train	O
the	O
SSN	B-DeepLearning
parser	O
described	O
in	O
the	O
previous	O
section	O
on	O
a	O
relatively	O
small	O
set	O
of	O
sentences	O
and	O
then	O
test	O
how	O
well	O
it	O
generalizes	O
to	O
a	O
set	O
of	O
previously	O
unseen	O
sentences	O
.	O
This	O
process	O
can	O
be	O
continued	O
until	O
no	O
more	O
changes	O
are	O
made	O
,	O
but	O
to	O
avoid	O
over-fitting	B-DeepLearning
it	O
is	O
better	O
to	O
check	O
the	O
performance	O
of	O
the	O
network	O
on	O
a	O
validation	B-DeepLearning
set	I-DeepLearning
and	O
stop	O
training	O
when	O
the	O
performance	O
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
reaches	O
a	O
maximum	O
.	O
This	O
is	O
why	O
we	O
have	O
split	O
the	O
corpus	O
into	O
three	O
datasets	B-DeepLearning
,	O
one	O
for	O
training	O
,	O
one	O
for	O
validation	O
,	O
and	O
one	O
for	O
testing	O
.	O
This	O
technique	O
also	O
allows	O
multiple	O
versions	O
of	O
the	O
network	O
to	O
be	O
trained	O
and	O
then	O
evaluated	O
using	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
without	O
ever	O
using	O
the	O
testing	B-DeepLearning
set	I-DeepLearning
until	O
a	O
single	O
network	O
has	O
been	O
chosen	O
.	O
A	O
variety	O
of	O
hidden	B-DeepLearning
layer	I-DeepLearning
sizes	O
and	O
random	O
initial	O
weight	B-DeepLearning
seeds	I-DeepLearning
were	O
used	O
in	O
the	O
different	O
networks	O
.	O
Larger	O
hidden	B-DeepLearning
layers	I-DeepLearning
result	O
in	O
the	O
network	O
being	O
able	O
to	O
fit	O
the	O
training	O
data	O
more	O
precisely	O
,	O
but	O
can	O
lead	O
to	O
over-fitting	B-DeepLearning
and	O
therefore	O
bad	O
performance	O
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
.	O
From	O
the	O
multiple	O
networks	O
trained	O
,	O
the	O
best	O
network	O
was	O
chosen	O
on	O
the	O
basis	O
of	O
its	O
performance	O
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
and	O
this	O
one	O
network	O
was	O
used	O
in	O
testing	O
.	O
The	O
best	O
network	O
had	O
100	O
hidden	B-DeepLearning
units	I-DeepLearning
and	O
trained	O
for	O
a	O
total	O
of	O
145	O
passes	O
through	O
the	O
training	B-DeepLearning
set	I-DeepLearning
.	O
Because	O
this	O
process	O
does	O
not	O
require	O
a	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
we	O
estimate	O
these	O
parameters	B-DeepLearning
using	O
the	O
combination	O
of	O
the	O
training	B-DeepLearning
set	I-DeepLearning
and	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
.	O
This	O
generalization	B-DeepLearning
performance	O
is	O
due	O
to	O
SSNs’	B-DeepLearning
ability	O
to	O
generalize	O
across	O
constituents	O
as	O
well	O
as	O
across	O
sequence	O
positions	O
,	O
plus	O
the	O
ability	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
in	O
general	O
to	O
learn	O
what	O
input	O
features	O
are	O
important	O
as	O
well	O
as	O
what	O
they	O
imply	O
about	O
the	O
output	O
.	O
In	O
particular	O
,	O
we	O
used	O
a	O
momentum	B-DeepLearning
of	O
0.9	O
and	O
weight	B-DeepLearning
decay	I-DeepLearning
regularization	B-DeepLearning
of	O
between	O
0.1	O
and	O
0.0.	O
Both	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
and	O
the	O
weight	B-DeepLearning
decay	I-DeepLearning
were	O
decreased	O
as	O
the	O
learning	O
proceeded	O
,	O
based	O
on	O
training	B-DeepLearning
error	I-DeepLearning
and	O
validation	B-DeepLearning
error	I-DeepLearning
,	O
respectively	O
.	O
We	O
mean	O
feature	O
here	O
in	O
the	O
sense	O
of	O
features	O
which	O
can	O
be	O
computed	O
from	O
discourse	O
as	O
input	O
to	O
machine	B-DeepLearning
learning	I-DeepLearning
algorithms	O
for	O
classification	O
tasks	O
such	O
as	O
topic	O
segmentation	O
.	O
Moreover	O
,	O
generic	O
tools	O
are	O
provided	O
for	O
iterating	O
over	O
discourses	O
,	O
processing	O
them	O
,	O
and	O
extracting	O
sets	O
of	O
feature	O
values	O
at	O
regular	O
intervals	O
which	O
can	O
then	O
be	O
piped	O
directly	O
into	O
learners	O
like	O
decision	O
trees	O
,	O
neural	B-DeepLearning
nets	I-DeepLearning
or	O
support	O
vector	O
machines	O
.	O
A	O
variety	O
of	O
learning	B-DeepLearning
parameters	I-DeepLearning
were	O
explored	O
,	O
and	O
the	O
best-performing	O
parameter	B-DeepLearning
set	O
was	O
selected:	O
initial	O
Q	O
values	O
set	O
to	O
0	O
,	O
exploration	O
parameter	B-DeepLearning
ε	O
=	O
0.2	O
,	O
and	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
α	O
set	O
to	O
1/k	O
where	O
k	O
is	O
the	O
number	O
of	O
visits	O
to	O
the	O
Qs	O
,	O
a	O
being	O
updated	O
.	O
Schmid	O
1994a	O
presents	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
tagger	O
based	O
on	O
multilayer	B-DeepLearning
perceptron	I-DeepLearning
networks	I-DeepLearning
.	O
An	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consist	O
of	O
simple	O
units	O
each	O
associated	O
with	O
an	O
activation	O
value	O
and	O
directed	O
links	O
for	O
passing	O
the	O
values	O
between	O
the	O
units	O
.	O
Activation	B-DeepLearning
values	I-DeepLearning
are	O
propagated	O
from	O
input	O
to	O
output	B-DeepLearning
layers	I-DeepLearning
.	O
At	O
each	O
unit	O
,	O
the	O
input	O
activation	O
values	O
are	O
summed	O
and	O
a	O
bias	B-DeepLearning
parameter	B-DeepLearning
is	O
added	O
.	O
The	O
network	O
learns	O
by	O
adapting	O
the	O
weights	B-DeepLearning
of	O
the	O
connections	O
between	O
units	O
until	O
the	O
correct	O
output	O
is	O
produced	O
.	O
In	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
,	O
all	O
units	O
have	O
a	O
value	O
of	O
zero	O
except	O
the	O
correct	O
unit	O
tag	O
,	O
which	O
gets	O
the	O
value	O
of	O
one	O
.	O
In	O
a	O
system	O
of	O
this	O
kind	O
,	O
tagging	O
a	O
word	O
means	O
i	O
copying	O
the	O
tag	O
probabilities	O
of	O
the	O
word	O
and	O
its	O
neighbours	O
into	O
the	O
input	B-DeepLearning
units	I-DeepLearning
and	O
ii	O
propagating	O
the	O
activations	O
to	O
the	O
output	B-DeepLearning
units	I-DeepLearning
.	O
In	O
back-propagation	B-DeepLearning
learning	O
,	O
this	O
training	O
is	O
done	O
by	O
repeatedly	O
iterating	O
over	O
all	O
examples	O
,	O
comparing	O
for	O
each	O
example	O
the	O
output	O
predicted	O
by	O
the	O
network	O
random	O
at	O
first	O
to	O
the	O
desired	O
output	O
and	O
changing	O
connection	B-DeepLearning
weights	I-DeepLearning
between	O
network	O
nodes	O
in	O
such	O
a	O
way	O
that	O
performance	O
increases	O
.	O
Multilayer	B-DeepLearning
Perceptrons	I-DeepLearning
Rumelhart	O
et	O
al.	O
1986	O
are	O
the	O
most	O
popular	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
.	O
The	O
activation	B-DeepLearning
rule	I-DeepLearning
is	O
a	O
local	O
rule	O
which	O
is	O
used	O
by	O
each	O
unit	O
to	O
compute	O
its	O
activation	O
.	O
Given	O
two	O
words	O
preceding	O
context	O
and	O
89	O
categories	O
,	O
the	O
network	O
has	O
an	O
input	B-DeepLearning
layer	I-DeepLearning
of	O
178	O
units	O
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
of	O
89	O
units	O
.	O
Adding	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
to	O
a	O
two-layer	O
network	O
did	O
not	O
improve	O
performance	O
.	O
Connectionist	O
approaches	O
also	O
require	O
the	O
computation	O
of	O
fewer	O
parameters	B-DeepLearning
weights	B-DeepLearning
than	O
statistical	O
models	O
N-gram	O
probabilities	O
,	O
which	O
becomes	O
especially	O
useful	O
when	O
considering	O
a	O
wider	O
context	O
than	O
trigrams	O
.	O
On	O
the	O
theoretical	O
side	O
,	O
there	O
is	O
a	O
need	O
for	O
more	O
insight	O
into	O
the	O
differences	O
and	O
similarities	O
in	O
how	O
generalization	B-DeepLearning
is	O
achieved	O
in	O
this	O
area	O
by	O
different	O
statistical	O
and	O
machine	B-DeepLearning
learning	I-DeepLearning
techniques	O
.	O
Most	O
emphasis	O
in	O
current	O
deep	B-DeepLearning
learning	I-DeepLearning
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
based	O
automatic	O
recognition	O
of	O
speech	O
is	O
put	O
on	O
deep	B-DeepLearning
net	I-DeepLearning
architectures	O
with	O
multiple	O
sequential	O
levels	O
of	O
processing	O
.	O
Current	O
state-of-the-art	O
stochastic	O
ASR	O
systems	O
often	O
estimate	O
the	O
likelihood	O
pX|W	O
by	O
a	O
discriminatively-trained	O
multi-layer	B-DeepLearning
perceptron	I-DeepLearning
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
MLP	B-DeepLearning
.	O
The	O
rule-based	O
algorithm	O
obtained	O
60.67%	O
splitting	O
accuracy	B-DeepLearning
at	O
word	O
level	O
and	O
94.31%	O
accuracy	B-DeepLearning
,	O
within	O
the	O
word	O
,	O
at	O
split	O
level	O
.	O
The	O
hyperparameters	B-DeepLearning
found	O
to	O
optimize	O
it	O
are	O
n	O
=	O
4	O
,	O
α	O
=	O
10−5	O
,	O
marker=true	O
.	O
Recently	O
there	O
has	O
been	O
a	O
renewed	O
interest	O
in	O
applying	O
neural	B-DeepLearning
networks	I-DeepLearning
ANNs	B-DeepLearning
to	O
speech	O
recognition	O
,	O
thanks	O
to	O
the	O
invention	O
of	O
deep	B-DeepLearning
neural	I-DeepLearning
nets	I-DeepLearning
.	O
It	O
treats	O
the	O
network	O
as	O
a	O
deep	B-DeepLearning
belief	I-DeepLearning
network	I-DeepLearning
DBN	B-DeepLearning
built	O
out	O
of	O
restricted	B-DeepLearning
Bolztmann	I-DeepLearning
machines	I-DeepLearning
RBMs	B-DeepLearning
,	O
and	O
optimizes	O
an	O
energy-based	O
target	O
function	O
using	O
the	O
contrastive	O
divergence	O
CD	O
algorithm	O
.	O
As	O
for	O
the	O
third	O
method	O
,	O
it	O
is	O
different	O
from	O
the	O
two	O
above	O
in	O
the	O
sense	O
that	O
in	O
this	O
case	O
it	O
is	O
not	O
the	O
training	B-DeepLearning
algorithm	I-DeepLearning
that	O
is	O
slightly	O
modified	O
,	O
but	O
the	O
neurons	O
themselves	O
.	O
Namely	O
,	O
the	O
usual	O
sigmoid	B-DeepLearning
activation	B-DeepLearning
function	I-DeepLearning
is	O
replaced	O
with	O
the	O
rectifier	B-DeepLearning
function	I-DeepLearning
max0	O
,	O
x	O
.	O
These	O
kinds	O
of	O
neural	B-DeepLearning
units	I-DeepLearning
have	O
been	O
proposed	O
by	O
Glorot	O
et	O
al.	O
,	O
and	O
were	O
successfully	O
applied	O
to	O
image	B-DeepLearning
recognition	O
and	O
NLP	O
tasks	O
.	O
Rectified	B-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
were	O
also	O
found	O
to	O
improve	O
restricted	B-DeepLearning
Boltzmann	I-DeepLearning
machines	I-DeepLearning
.	O
It	O
has	O
been	O
shown	O
recently	O
that	O
a	O
deep	B-DeepLearning
rectifier	I-DeepLearning
network	I-DeepLearning
can	O
attain	O
the	O
same	O
phone	O
recognition	O
performance	O
as	O
that	O
for	O
the	O
pre-trained	B-DeepLearning
nets	I-DeepLearning
of	O
Mohamed	O
et	O
al.	O
4	B-DeepLearning
,	O
but	O
without	O
the	O
need	O
for	O
any	O
pre-training	B-DeepLearning
.	O
This	O
efficient	O
unsupervised	O
algorithm	O
,	O
first	O
described	O
in	O
,	O
can	O
be	O
used	O
for	O
learning	O
the	O
connection	B-DeepLearning
weights	I-DeepLearning
of	O
a	O
deep	B-DeepLearning
belief	I-DeepLearning
network	I-DeepLearning
DBN	B-DeepLearning
consisting	O
of	O
several	O
layers	O
of	O
restricted	B-DeepLearning
Boltzmann	I-DeepLearning
machines	I-DeepLearning
RBMs	B-DeepLearning
.	O
As	O
their	O
name	O
implies	O
,	O
RBMs	B-DeepLearning
are	O
a	O
variant	O
of	O
Boltzmann	B-DeepLearning
machines	I-DeepLearning
,	O
with	O
the	O
restriction	O
that	O
their	O
neurons	O
must	O
form	O
a	O
bipartite	O
graph	O
.	O
They	O
have	O
an	O
input	B-DeepLearning
layer	I-DeepLearning
,	O
representing	O
the	O
features	O
of	O
the	O
given	O
task	O
,	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
which	O
has	O
to	O
learn	O
some	O
representation	O
of	O
the	O
input	O
,	O
and	O
each	O
connection	O
in	O
an	O
RBM	B-DeepLearning
must	O
be	O
between	O
a	O
visible	B-DeepLearning
unit	I-DeepLearning
and	O
a	O
hidden	B-DeepLearning
unit	I-DeepLearning
.	O
RBMs	B-DeepLearning
can	O
be	O
trained	O
using	O
the	O
one-step	B-DeepLearning
contrastive	I-DeepLearning
divergence	I-DeepLearning
CD	B-DeepLearning
algorithm	O
described	O
in	O
6	O
.	O
It	O
is	O
a	O
simple	O
algorithm	O
where	O
first	O
we	O
train	O
a	O
network	O
with	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
to	O
full	B-DeepLearning
convergence	I-DeepLearning
using	O
backpropagation	B-DeepLearning
.	O
Then	O
we	O
replace	O
the	O
softmax	B-DeepLearning
layer	O
by	O
another	O
randomly	O
initialized	O
hidden	B-DeepLearning
layer	I-DeepLearning
and	O
a	O
new	O
softmax	B-DeepLearning
layer	O
on	O
top	O
,	O
and	O
we	O
train	O
the	O
network	O
again	O
.	O
This	O
process	O
is	O
repeated	O
until	O
we	O
reach	O
the	O
desired	O
number	O
of	O
hidden	B-DeepLearning
layers	I-DeepLearning
.	O
Seide	O
et	O
al.	O
found	O
that	O
this	O
method	O
gives	O
the	O
best	O
results	O
if	O
one	O
performs	O
only	O
a	O
few	O
iterations	B-DeepLearning
of	O
backpropagation	B-DeepLearning
in	O
the	O
pre-training	B-DeepLearning
phase	O
instead	O
of	O
training	O
to	O
full	O
convergence	B-DeepLearning
with	O
an	O
unusually	O
large	O
learn	B-DeepLearning
rate	I-DeepLearning
.	O
In	O
their	O
paper	O
,	O
they	O
concluded	O
that	O
this	O
simple	O
training	O
strategy	O
performs	O
just	O
as	O
well	O
as	O
the	O
much	O
more	O
complicated	O
DBN	B-DeepLearning
pre-training	B-DeepLearning
method	O
described	O
above	O
.	O
In	O
the	O
case	O
of	O
the	O
third	O
method	O
it	O
is	O
not	O
the	O
training	B-DeepLearning
algorithm	I-DeepLearning
,	O
but	O
the	O
neurons	O
that	O
are	O
slightly	O
modified	O
.	O
Instead	O
of	O
the	O
usual	O
sigmoid	B-DeepLearning
activation	O
,	O
here	O
we	O
apply	O
the	O
rectifier	B-DeepLearning
function	I-DeepLearning
max0	I-DeepLearning
,	O
x	O
for	O
all	O
hidden	B-DeepLearning
neurons	I-DeepLearning
.	O
There	O
are	O
two	O
fundamental	O
differences	O
between	O
the	O
sigmoid	B-DeepLearning
and	O
the	O
rectifier	B-DeepLearning
functions	I-DeepLearning
.	O
One	O
is	O
that	O
the	O
output	O
of	O
rectifier	B-DeepLearning
neurons	I-DeepLearning
does	O
not	O
saturate	O
as	O
their	O
activity	O
gets	O
higher	O
.	O
Glorot	O
et	O
al.	O
conjecture	O
that	O
this	O
is	O
very	O
important	O
in	O
explaining	O
their	O
good	O
performance	O
in	O
deep	B-DeepLearning
nets:	I-DeepLearning
because	O
of	O
this	O
linearity	O
,	O
there	O
is	O
no	O
gradient	B-DeepLearning
vanishing	I-DeepLearning
effect	O
.	O
One	O
might	O
suppose	O
that	O
this	O
could	O
harm	O
optimization	O
by	O
blocking	O
gradient	B-DeepLearning
backpropagation	I-DeepLearning
,	O
but	O
the	O
experimental	O
results	O
do	O
not	O
support	O
this	O
hypothesis	O
.	O
The	O
main	O
advantage	O
of	O
deep	B-DeepLearning
rectifier	I-DeepLearning
nets	I-DeepLearning
is	O
that	O
they	O
can	O
be	O
trained	O
with	O
the	O
standard	O
backpropagation	B-DeepLearning
algorithm	O
,	O
without	O
any	O
pre-training	B-DeepLearning
.	O
A	O
random	O
10%	O
of	O
the	O
training	B-DeepLearning
set	I-DeepLearning
was	O
held	O
out	O
for	O
validation	O
purposes	O
,	O
and	O
this	O
block	O
of	O
data	O
will	O
be	O
referred	O
to	O
as	O
the	O
’development	B-DeepLearning
set’	I-DeepLearning
.	O
It	O
contains	O
about	O
28	O
hours	O
of	O
recordings	O
,	O
from	O
which	O
22	O
hours	O
were	O
selected	O
for	O
the	O
training	B-DeepLearning
set	I-DeepLearning
,	O
2	O
hours	O
for	O
the	O
development	O
set	O
and	O
4	O
hours	O
for	O
the	O
test	B-DeepLearning
set	I-DeepLearning
.	O
In	O
the	O
case	O
of	O
the	O
DBN-based	O
pre-training	B-DeepLearning
method	O
see	O
Section	O
2.1	O
,	O
we	O
applied	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
i.e.	O
backpropagation	B-DeepLearning
training	O
with	O
a	O
mini-batch	B-DeepLearning
size	O
of	O
128	O
.	O
For	O
Gaussian-binary	O
RBMs	B-DeepLearning
,	O
we	O
ran	O
50	O
epochs	B-DeepLearning
with	O
a	O
fixed	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
0.002	O
,	O
while	O
for	O
binary-binary	O
RBMs	B-DeepLearning
we	O
used	O
30	O
epochs	B-DeepLearning
with	O
a	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
002	O
.	O
Then	O
,	O
to	O
fine-tune	O
the	O
pre-trained	B-DeepLearning
nets	O
,	O
again	O
backpropagation	B-DeepLearning
was	O
applied	O
with	O
the	O
same	O
mini-batch	B-DeepLearning
size	O
as	O
that	O
used	O
for	O
pre-training	B-DeepLearning
.	O
The	O
initial	O
learn	B-DeepLearning
rate	I-DeepLearning
was	O
set	O
to	O
0.01	O
,	O
and	O
it	O
was	O
halved	O
after	O
each	O
epoch	B-DeepLearning
when	O
the	O
error	O
on	O
the	O
development	B-DeepLearning
set	I-DeepLearning
increased	O
.	O
During	O
both	O
the	O
pretraining	B-DeepLearning
and	O
fine-tuning	B-DeepLearning
phases	O
,	O
the	O
learning	O
was	O
accelerated	O
by	O
using	O
a	O
momentum	B-DeepLearning
of	O
0.9	O
except	O
for	O
the	O
first	O
epoch	B-DeepLearning
of	O
fine-tuning	B-DeepLearning
,	O
which	O
did	O
not	O
use	O
the	O
momentum	B-DeepLearning
method	O
.	O
Turning	O
to	O
the	O
discriminative	O
pre-training	B-DeepLearning
method	O
see	O
Section	O
2.2	O
,	O
the	O
initial	O
learn	B-DeepLearning
rate	I-DeepLearning
was	O
set	O
to	O
0.01	O
,	O
and	O
it	O
was	O
halved	O
after	O
each	O
epoch	B-DeepLearning
when	O
the	O
error	O
on	O
the	O
development	B-DeepLearning
set	I-DeepLearning
increased	O
.	O
The	O
learn	B-DeepLearning
rate	I-DeepLearning
was	O
restored	O
to	O
its	O
initial	O
value	O
of	O
0.01	O
after	O
the	O
addition	O
of	O
each	O
layer	O
.	O
Furthermore	O
,	O
we	O
found	O
that	O
using	O
5	O
epochs	B-DeepLearning
of	O
backpropagation	B-DeepLearning
after	O
the	O
introduction	O
of	O
each	O
layer	O
gave	O
the	O
best	O
results	O
.	O
For	O
both	O
the	O
pre-training	B-DeepLearning
and	O
fine-tuning	B-DeepLearning
phases	O
we	O
used	O
a	O
batch	B-DeepLearning
size	I-DeepLearning
of	O
128	O
and	O
momentum	B-DeepLearning
of	O
0.8	O
except	O
for	O
the	O
first	O
epoch	B-DeepLearning
.	O
The	O
initial	O
learn	B-DeepLearning
rate	I-DeepLearning
for	O
the	O
fine-tuning	B-DeepLearning
of	O
the	O
full	O
network	O
was	O
again	O
set	O
to	O
001	O
.	O
The	O
training	O
of	O
deep	B-DeepLearning
rectifier	I-DeepLearning
nets	I-DeepLearning
see	O
Section	O
2.3	O
did	O
not	O
require	O
any	O
pre-training	B-DeepLearning
at	O
all	O
.	O
The	O
training	O
of	O
the	O
network	O
was	O
performed	O
using	O
backpropagation	B-DeepLearning
with	O
an	O
initial	O
learn	B-DeepLearning
rate	I-DeepLearning
of	O
0.001	O
and	O
a	O
batch	B-DeepLearning
size	I-DeepLearning
of	O
128	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
three	O
training	O
methods	O
performed	O
very	O
similarly	O
on	O
the	O
test	O
set	O
,	O
the	O
only	O
exception	O
being	O
the	O
case	O
of	O
five	O
hidden	B-DeepLearning
layers	I-DeepLearning
,	O
where	O
the	O
rectifier	B-DeepLearning
net	I-DeepLearning
performed	O
slightly	O
better	O
.	O
It	O
also	O
significantly	O
outperformed	O
the	O
other	O
two	O
methods	O
on	O
the	O
development	B-DeepLearning
set	I-DeepLearning
.	O
We	O
mention	O
that	O
a	O
single	O
hidden	B-DeepLearning
layer	I-DeepLearning
net	O
with	O
the	O
same	O
amount	O
of	O
weights	B-DeepLearning
as	O
the	O
best	O
deep	B-DeepLearning
net	I-DeepLearning
yielded	O
237%	O
.	O
Similar	O
to	O
the	O
TIMIT	O
tests	O
,	O
2048	B-DeepLearning
neurons	I-DeepLearning
were	O
used	O
for	O
each	O
hidden	B-DeepLearning
layer	I-DeepLearning
,	O
with	O
a	O
varying	O
number	O
of	O
hidden	B-DeepLearning
layers	I-DeepLearning
.	O
The	O
error	O
rates	O
seem	O
to	O
saturate	O
at	O
4-5	O
hidden	B-DeepLearning
layers	I-DeepLearning
,	O
and	O
the	O
curves	O
for	O
the	O
three	O
methods	O
run	O
parallel	O
and	O
have	O
only	O
slightly	O
different	O
values	O
.	O
The	O
lowest	O
error	O
rate	O
is	O
attained	O
with	O
the	O
five-layer	O
rectifier	B-DeepLearning
network	I-DeepLearning
,	O
both	O
on	O
the	O
development	O
and	O
the	O
test	B-DeepLearning
sets	I-DeepLearning
.	O
The	O
iteration	B-DeepLearning
count	O
we	O
applied	O
here	O
50	O
for	O
Gaussian	O
RBMs	B-DeepLearning
and	O
30	O
for	O
binary	O
RBMs	B-DeepLearning
is	O
an	O
average	O
value	O
,	O
and	O
follows	O
the	O
work	O
of	O
Seide	O
et	O
al	O
.	O
Discriminative	O
pre-training	B-DeepLearning
is	O
also	O
much	O
faster	O
than	O
the	O
DBN-based	O
method	O
,	O
but	O
is	O
still	O
slower	O
than	O
rectifier	B-DeepLearning
nets	I-DeepLearning
.	O
Tuning	B-DeepLearning
the	I-DeepLearning
parameters	I-DeepLearning
so	O
that	O
the	O
two	O
systems	O
had	O
a	O
similar	O
real-time	O
factor	O
was	O
also	O
out	O
of	O
the	O
question	O
,	O
as	O
the	O
hybrid	O
model	O
was	O
implemented	O
on	O
a	O
GPU	O
,	O
while	O
the	O
HMM	O
used	O
a	O
normal	O
CPU	O
.	O
Here	O
,	O
we	O
compared	O
two	O
training	O
methods	O
and	O
a	O
new	O
type	O
of	O
activation	B-DeepLearning
function	I-DeepLearning
for	O
deep	B-DeepLearning
neural	I-DeepLearning
nets	I-DeepLearning
,	O
and	O
evaluated	O
them	O
on	O
a	O
large	O
vocabulary	O
recognition	O
task	O
.	O
The	O
three	O
algorithms	O
yielded	O
quite	O
similar	O
recognition	O
performances	O
,	O
but	O
based	O
on	O
the	O
training	O
times	O
deep	B-DeepLearning
rectifier	I-DeepLearning
networks	I-DeepLearning
seem	O
to	O
be	O
the	O
preferred	O
choice	O
.	O
While	O
the	O
resulting	O
speech	O
sound	O
likelihood	O
estimates	O
are	O
demonstrated	O
to	O
be	O
better	O
that	O
the	O
earlier	O
used	O
likelihoods	O
derived	O
by	O
generative	O
Gaussian	O
Mixture	O
Models	O
,	O
unexpected	O
signal	O
distortions	O
that	O
were	O
not	O
seen	O
in	O
the	O
training	B-DeepLearning
data	I-DeepLearning
can	O
still	O
make	O
the	O
acoustic	O
likelihoods	O
unacceptably	O
low	O
.	O
Here	O
,	O
we	O
compare	O
three	O
methods	O
namely	O
,	O
the	O
unsupervised	O
pre-training	B-DeepLearning
algorithm	O
of	O
Hinton	O
et	O
al.	O
,	O
a	O
supervised	O
pre-training	B-DeepLearning
method	O
that	O
constructs	O
the	O
network	O
layer-by-layer	O
,	O
and	O
deep	B-DeepLearning
rectifier	I-DeepLearning
networks	I-DeepLearning
,	O
which	O
differ	O
from	O
standard	O
nets	O
in	O
their	O
activation	B-DeepLearning
function	I-DeepLearning
.	O
Overall	O
,	O
for	O
the	O
large	O
vocabulary	O
speech	O
recognition	O
task	O
we	O
study	O
here	O
,	O
deep	B-DeepLearning
rectifier	I-DeepLearning
networks	I-DeepLearning
offer	O
the	O
best	O
tradeoff	O
between	O
accuracy	O
and	O
training	B-DeepLearning
time	I-DeepLearning
.	O
In	O
this	O
new	O
proposed	O
approach	O
,	O
a	O
multi-layer	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
multi-layer	B-DeepLearning
perceptron	I-DeepLearning
is	O
used	O
to	O
learn	O
the	O
decision	B-DeepLearning
function	I-DeepLearning
that	O
will	O
then	O
be	O
used	O
to	O
select	O
the	O
words	O
.	O
For	O
training	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
parameters	I-DeepLearning
we	O
have	O
to	O
associate	O
a	O
target	O
value	O
to	O
each	O
word	O
.	O
The	O
scores	O
produced	O
by	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
module	O
will	O
then	O
be	O
used	O
to	O
sort	O
the	O
list	O
of	O
candidate	O
words	O
,	O
and	O
the	O
top	O
of	O
the	O
list	O
will	O
be	O
selected	O
to	O
define	O
the	O
recognition	O
vocabulary	O
.	O
Feature	B-DeepLearning
vectors	I-DeepLearning
and	O
associated	O
target	O
values	O
are	O
used	O
for	O
training	O
the	O
neural	B-DeepLearning
network:	I-DeepLearning
the	O
input-feature	B-DeepLearning
vector	I-DeepLearning
is	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
36	B-DeepLearning
input	I-DeepLearning
neurons	I-DeepLearning
the	O
target	O
value	O
is	O
the	O
output	O
of	O
the	O
unique	O
output	B-DeepLearning
layer	I-DeepLearning
neuron	I-DeepLearning
.	O
There	O
is	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
containing	O
18	B-DeepLearning
neurons	I-DeepLearning
.	O
In	O
recent	O
years	O
feed	B-DeepLearning
forward	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
FFNN	B-DeepLearning
attracted	O
attention	O
due	O
their	O
ability	O
to	O
overcome	O
biggest	O
disadvantage	O
of	O
n-gram	O
models:	O
even	O
when	O
the	O
ngram	O
is	O
not	O
observed	O
in	O
training	O
,	O
FFNN	O
estimates	O
probabilities	O
of	O
the	O
word	O
based	O
on	O
the	O
full	O
history	O
.	O
The	O
RNN	B-DeepLearning
is	O
going	O
further	O
in	O
model	B-DeepLearning
generalization:	B-DeepLearning
instead	O
of	O
considering	O
only	O
the	O
several	O
previous	O
words	O
parameter	B-DeepLearning
n	O
the	O
recursive	B-DeepLearning
weights	I-DeepLearning
are	O
assumed	O
to	O
represent	O
short	O
term	O
memory	O
.	O
Long	B-DeepLearning
Short-Term	I-DeepLearning
Memory	I-DeepLearning
LSTM	B-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
is	O
different	O
type	O
of	O
RNN	B-DeepLearning
structure	O
.	O
LSTM	B-DeepLearning
approved	O
themselves	O
in	O
various	O
applications	O
and	O
it	O
seems	O
to	O
be	O
very	O
promising	O
course	O
also	O
for	O
the	O
field	O
of	O
language	O
modelling	O
.	O
Typical	O
NN	B-DeepLearning
unit	O
consists	O
of	O
the	O
input	B-DeepLearning
activation	I-DeepLearning
which	O
is	O
transformed	O
to	O
output	B-DeepLearning
activation	I-DeepLearning
with	O
activation	B-DeepLearning
function	I-DeepLearning
usually	O
sigmoidal	B-DeepLearning
.	O
Firstly	O
,	O
the	O
activation	B-DeepLearning
function	I-DeepLearning
is	O
applied	O
to	O
all	O
gates	O
.	O
There	O
is	O
a	O
softmax	B-DeepLearning
function	I-DeepLearning
used	O
in	O
output	B-DeepLearning
layer	I-DeepLearning
to	O
produce	O
normalized	O
probabilities	O
.	O
Normalization	O
of	O
input	B-DeepLearning
vector	I-DeepLearning
which	O
is	O
generally	O
advised	O
for	O
neural	B-DeepLearning
networks	I-DeepLearning
is	O
not	O
needed	O
due	O
the	O
1-of-N	O
input	O
coding	O
.	O
All	O
models	B-DeepLearning
were	O
trained	O
with	O
20	O
cells	O
in	O
hidden	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
recurrent	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
language	O
model	O
RNNLM	O
,	O
originally	O
proposed	O
by	O
2	O
and	O
3	O
,	O
incorporates	O
the	O
time	O
dimension	O
by	O
expanding	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
,	O
which	O
represents	O
the	O
current	O
input	O
word	O
,	O
with	O
the	O
previous	O
hidden	B-DeepLearning
layer	I-DeepLearning
.	O
Theoretically	O
,	O
recurrent	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
can	O
store	O
relevant	O
information	O
from	O
previous	O
time	O
steps	O
for	O
an	O
arbitrarily	O
long	O
period	O
of	O
time	O
,	O
making	O
it	O
possible	O
to	O
learn	O
long-term	O
dependencies	O
.	O
To	O
explain	O
how	O
our	O
approach	O
works	O
,	O
let	O
us	O
examine	O
the	O
operation	O
of	O
a	O
simple	O
perceptron	B-DeepLearning
model	B-DeepLearning
.	O
Hence	O
,	O
if	O
the	O
weights	B-DeepLearning
of	O
the	O
feature	B-DeepLearning
extraction	I-DeepLearning
layer	I-DeepLearning
were	O
initialized	O
with	O
2D	O
DCT	O
or	O
Gabor	O
filter	O
coefficients	O
,	O
and	O
only	O
the	O
weights	O
of	O
the	O
hidden	O
and	O
output	B-DeepLearning
layers	I-DeepLearning
were	O
tuned	O
during	O
training	O
,	O
then	O
the	O
model	B-DeepLearning
would	O
be	O
equivalent	O
to	O
a	O
more	O
traditional	O
system	O
,	O
and	O
incorporating	O
the	O
feature	O
extraction	O
step	O
into	O
the	O
system	O
would	O
be	O
just	O
an	O
implementational	O
detail	O
.	O
Usually	O
,	O
as	O
the	O
backpropagation	B-DeepLearning
algorithm	O
guarantees	O
only	O
a	O
locally	O
optimal	O
solution	O
,	O
initializing	O
the	O
model	B-DeepLearning
with	O
weights	O
that	O
already	O
provide	O
a	O
good	O
solution	O
may	O
help	O
the	O
backpropagation	B-DeepLearning
algorithm	O
find	O
a	O
better	O
local	O
optimum	O
than	O
the	O
one	O
found	O
using	O
random	O
initial	O
values	O
.	O
We	O
should	O
add	O
that	O
the	O
same	O
weights	B-DeepLearning
are	O
applied	O
on	O
each	O
input	O
block	O
,	O
so	O
the	O
number	O
of	O
weights	B-DeepLearning
will	O
not	O
change	O
in	O
this	O
layer	O
.	O
It	O
consisted	O
of	O
a	O
hidden	O
feature	B-DeepLearning
extraction	I-DeepLearning
layer	I-DeepLearning
with	O
a	O
linear	B-DeepLearning
activation	I-DeepLearning
function	I-DeepLearning
,	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
with	O
1000	B-DeepLearning
neurons	I-DeepLearning
with	O
the	O
sigmoid	B-DeepLearning
activation	B-DeepLearning
function	I-DeepLearning
,	O
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
containing	O
softmax	B-DeepLearning
units	I-DeepLearning
.	O
The	O
number	O
of	O
output	B-DeepLearning
neurons	I-DeepLearning
was	O
set	O
to	O
the	O
number	O
of	O
classes	O
39	O
,	O
while	O
the	O
number	O
of	O
neurons	O
in	O
the	O
input	O
and	O
feature	B-DeepLearning
extraction	I-DeepLearning
layers	I-DeepLearning
varied	O
,	O
depending	O
on	O
how	O
many	O
neighbouring	O
patches	O
were	O
actually	O
used	O
.	O
The	O
neural	B-DeepLearning
net	I-DeepLearning
was	O
trained	O
with	O
random	O
initial	B-DeepLearning
weights	I-DeepLearning
in	O
the	O
hidden	O
and	O
output	B-DeepLearning
layers	I-DeepLearning
,	O
using	O
standard	O
backpropagation	B-DeepLearning
on	O
90%	O
of	O
the	O
training	B-DeepLearning
data	I-DeepLearning
in	O
semi-batch	B-DeepLearning
mode	O
,	O
while	O
crossvalidation	B-DeepLearning
on	O
the	O
remaining	O
,	O
randomly	O
selected	O
10%	O
of	O
the	O
training	B-DeepLearning
set	I-DeepLearning
was	O
used	O
as	O
the	O
stopping	O
criterion	O
.	O
The	O
‘extreme	O
learning	O
machine’	O
of	O
Huang	O
et	O
al.	O
also	O
exploits	O
this	O
suprising	O
fact:	O
this	O
learning	O
model	B-DeepLearning
is	O
practically	O
a	O
twolayer	B-DeepLearning
network	I-DeepLearning
,	O
where	O
both	O
layers	O
are	O
initialized	O
randomly	O
,	O
and	O
the	O
lowest	O
layer	O
is	O
not	O
trained	O
at	O
all	O
.	O
In	O
order	O
to	O
verify	O
how	O
the	O
whisper	O
can	O
be	O
recognized	O
by	O
the	O
ANN	B-DeepLearning
,	O
the	O
two	O
speakerdependent	O
ASRs	O
were	O
developed	O
with	O
MATLAB	O
Neural	B-DeepLearning
Network	I-DeepLearning
Toolbox	O
.	O
The	O
structures	O
of	O
these	O
ANNs	B-DeepLearning
were:	O
396	O
input	B-DeepLearning
nodes	I-DeepLearning
,	O
140	O
hidden	B-DeepLearning
neurons	I-DeepLearning
and	O
50	B-DeepLearning
output	I-DeepLearning
neurons	I-DeepLearning
.	O
The	O
training	O
,	O
development	B-DeepLearning
and	I-DeepLearning
test	I-DeepLearning
sets	I-DeepLearning
for	O
the	O
classification	O
task	O
were	O
not	O
selected	O
from	O
the	O
corpus	O
in	O
a	O
completely	O
random	O
manner	O
,	O
but	O
instead	O
the	O
division	O
respected	O
the	O
websites	O
,	O
i.e.	O
one	O
set	O
of	O
websites	O
was	O
denoted	O
the	O
training	O
set	O
,	O
another	O
–	O
development	B-DeepLearning
set	I-DeepLearning
and	O
the	O
third	O
–	O
test	B-DeepLearning
set	I-DeepLearning
.	O
The	O
ASR	O
system	O
uses	O
a	O
hybrid	O
Hidden	O
Markov	O
Model	O
HMM	O
and	O
Deep	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
DNN	O
architecture	O
and	O
a	O
general	O
550k	O
lexicon	O
.	O
The	O
DNN	B-DeepLearning
utilizes	O
five	O
hidden	B-DeepLearning
layers	I-DeepLearning
,	O
with	O
1024	B-DeepLearning
neurons	I-DeepLearning
per	O
layer	O
,	O
and	O
a	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
008	O
.	O
The	O
ReLU	B-DeepLearning
function	O
is	O
used	O
as	O
the	O
activation	B-DeepLearning
function	I-DeepLearning
of	O
neurons	O
.	O
This	O
DNN	B-DeepLearning
is	O
trained	O
for	O
35	O
epochs	B-DeepLearning
using	O
300	O
h	O
of	O
speech	O
recordings	O
.	O
Recently	O
,	O
neural-network	B-DeepLearning
based	O
approaches	O
,	O
in	O
which	O
words	O
are	O
embedded	O
into	O
a	O
low-dimensional	O
space	O
,	O
appeared	O
and	O
became	O
to	O
be	O
used	O
in	O
lexical	O
semantic	O
tasks	O
.	O
Following	O
recent	O
advances	O
in	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
research	O
,	O
the	O
recognizer	O
employs	O
parametric	B-DeepLearning
rectified	I-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
PReLU	B-DeepLearning
,	O
word	O
embeddings	O
and	O
character-level	O
embeddings	O
based	O
on	O
gated	B-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
GRU	B-DeepLearning
.	O
LSTMs	B-DeepLearning
are	O
specially	O
shaped	O
units	O
of	O
artificial	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
designed	O
to	O
process	O
whole	O
sequences	O
.	O
Recently	O
,	O
a	O
gated	B-DeepLearning
linear	I-DeepLearning
unit	I-DeepLearning
GRU	B-DeepLearning
was	O
proposed	O
by	O
Sam	O
as	O
an	O
alternative	O
to	O
LSTM	B-DeepLearning
,	O
and	O
was	O
shown	O
to	O
have	O
similar	O
performance	O
,	O
while	O
being	O
less	O
computationally	O
demanding	O
.	O
Instead	O
regularized	B-DeepLearning
averaged	I-DeepLearning
perceptron	I-DeepLearning
,	O
we	O
use	O
parametric	B-DeepLearning
rectified	I-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
,	O
character-level	O
embeddings	O
and	O
dropout	B-DeepLearning
.	O
The	O
input	B-DeepLearning
layer	I-DeepLearning
is	O
connected	O
to	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
parametric	B-DeepLearning
rectified	I-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
and	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
is	O
connected	O
to	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
which	O
is	O
a	O
softmax	B-DeepLearning
layer	I-DeepLearning
producing	O
probability	O
distribution	O
for	O
all	O
possible	O
named	O
entity	O
classes	O
in	O
BILOU	O
encoding	O
.	O
The	O
network	O
is	O
trained	O
with	O
AdaGrad	B-DeepLearning
and	O
we	O
use	O
dropout	B-DeepLearning
on	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
.	O
We	O
implemented	O
our	O
neural	B-DeepLearning
network	I-DeepLearning
in	O
Torch7	O
,	O
a	O
scientific	O
computing	O
framework	O
with	O
wide	O
support	O
for	O
machine	O
learning	O
algorithms	O
.	O
We	O
tuned	O
most	O
of	O
the	O
hyperparameters	B-DeepLearning
on	O
development	O
portion	O
of	O
CNEC	O
1.0	O
and	O
used	O
them	O
for	O
all	O
other	O
corpora	O
.	O
Notably	O
,	O
we	O
utilize	O
window	O
size	O
W	O
=	O
2	O
,	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	I-DeepLearning
200	I-DeepLearning
nodes	I-DeepLearning
,	O
dropout	B-DeepLearning
0.5	I-DeepLearning
,	O
minibatches	B-DeepLearning
of	I-DeepLearning
size	I-DeepLearning
100	I-DeepLearning
and	O
learning	B-DeepLearning
rate	I-DeepLearning
0.02	O
with	O
decay	O
.	O
All	O
reported	O
experiments	B-DeepLearning
use	O
an	O
ensemble	O
of	O
5	O
networks	O
,	O
each	O
using	O
different	O
random	B-DeepLearning
seed	I-DeepLearning
,	O
with	O
the	O
resulting	O
distributions	O
being	O
an	O
average	O
of	O
individual	O
networks	O
distributions	O
.	O
A	O
work	O
most	O
similar	O
to	O
ours	O
,	O
also	O
proposed	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
with	O
word	O
embeddings	O
and	O
character-level	O
embeddings	O
.	O
The	O
best	O
published	O
WER	O
on	O
CMUDict	O
at	O
present	O
is	O
demonstrated	O
in	O
using	O
Long	B-DeepLearning
Short-term	I-DeepLearning
Memory	I-DeepLearning
Recurrent	I-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
LSTM	B-DeepLearning
combined	O
with	O
a	O
5-gram	O
graphone	O
language	O
model	O
.	O
We	O
also	O
use	O
the	O
exact	O
same	O
split	O
of	O
90	O
%	O
training	B-DeepLearning
data	I-DeepLearning
and	O
10	O
%	O
test	B-DeepLearning
data	I-DeepLearning
as	O
in	O
9	O
,	O
and	O
thus	O
our	O
results	O
are	O
directly	O
comparable	O
to	O
theirs	O
.	O
Hierarchical	O
softmax	B-DeepLearning
and	O
related	O
procedures	O
that	O
involve	O
decomposing	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
into	O
classes	O
can	O
help	O
with	O
this	O
normalization	O
.	O
We	O
found	O
that	O
15	O
classes	O
optimized	O
perplexity	O
values	O
for	O
RNNLMs	B-DeepLearning
with	O
50	O
and	O
145	O
hidden	B-DeepLearning
nodes	I-DeepLearning
,	O
and	O
18	O
classes	O
optimized	O
perplexity	O
values	O
for	O
RNNLMs	B-DeepLearning
with	O
500	O
nodes	O
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
η	O
adaptation	O
scheme	O
is	O
managed	O
by	O
the	O
adaptive	B-DeepLearning
gradient	I-DeepLearning
methods	O
.	O
After	O
optimizing	O
on	O
the	O
development	B-DeepLearning
set	I-DeepLearning
,	O
η	O
was	O
fixed	O
to	O
0.1	O
and	O
the	O
dimensionality	O
of	O
the	O
latent	O
space	O
C	O
was	O
fixed	O
at	O
45	O
.	O
An	O
RNNLM	B-DeepLearning
with	O
145	O
hidden	B-DeepLearning
nodes	I-DeepLearning
has	O
about	O
the	O
same	O
number	O
of	O
parameters	B-DeepLearning
as	O
CDLM	O
and	O
performs	O
0.1	O
perplexity	O
points	O
worse	O
than	O
CDLM	O
.	O
Increasing	O
the	O
hidden	B-DeepLearning
units	I-DeepLearning
for	O
RNNLM	B-DeepLearning
to	O
500	O
,	O
we	O
obtain	O
the	O
best	O
performing	O
RNNLM	B-DeepLearning
.	O
To	O
produce	O
better	O
performing	O
LMs	O
with	O
fewer	O
parameters	B-DeepLearning
we	O
constructed	O
an	O
RNNLM	B-DeepLearning
with	O
50	O
hidden	O
units	O
,	O
which	O
when	O
linearly	O
combined	O
with	O
CDLM	O
CDLM+RNNLM	O
outperforms	O
the	O
best	O
RNNLM	B-DeepLearning
using	O
less	O
than	O
half	O
as	O
many	O
parameters	B-DeepLearning
.	O
The	O
architecture	O
for	O
this	O
kind	O
of	O
feature	O
extraction	O
consists	O
of	O
two	O
NNs	B-DeepLearning
trained	O
towards	O
phonetic	O
targets	O
.	O
The	O
first-stage	O
NN	B-DeepLearning
has	O
four	O
hidden	B-DeepLearning
layers	I-DeepLearning
with	O
1500	O
units	O
each	O
except	O
the	O
BN	O
layer	O
.	O
BN	O
layer’s	O
size	O
is	O
80	B-DeepLearning
neurons	I-DeepLearning
and	O
it	O
is	O
the	O
third	O
hidden	B-DeepLearning
layer	I-DeepLearning
.	O
Linear	B-DeepLearning
regression	I-DeepLearning
is	O
used	O
on	O
all	O
single	O
systems	O
for	O
arousal	O
and	O
all	O
single	O
systems	O
for	O
valence	O
except	O
for	O
processing	O
video	O
geometric	O
features	O
,	O
where	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
is	O
used	O
topology:	O
948–474–3	O
.	O
We	O
trained	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
with	O
topology	O
945–474–3	O
in	O
this	O
case	O
.	O
Our	O
approach	O
uses	O
a	O
bidirectional	B-DeepLearning
recurrent	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
BRNN	B-DeepLearning
since	O
speech	O
has	O
the	O
form	O
of	O
a	O
sequential	O
signal	O
with	O
complex	O
dependencies	O
between	O
the	O
different	O
time	O
steps	O
in	O
both	O
directions	O
.	O
Additionally	O
we	O
propose	O
an	O
alternative	O
layout	O
for	O
the	O
BRNN	B-DeepLearning
and	O
show	O
that	O
it	O
performs	O
better	O
on	O
that	O
specific	O
task	O
.	O
Usage	O
of	O
deep	B-DeepLearning
bidirectional	I-DeepLearning
GRU	I-DeepLearning
layers	O
can	O
be	O
found	O
in	O
the	O
work	O
of	O
Amodei	O
et	O
al.	O
where	O
,	O
for	O
example	O
,	O
up	O
to	O
seven	O
bidirectional	B-DeepLearning
GRU	I-DeepLearning
layers	O
are	O
used	O
and	O
even	O
combined	O
with	O
up	O
to	O
three	O
convolutional	B-DeepLearning
layers	I-DeepLearning
which	O
altogether	O
improved	O
the	O
performance	O
of	O
the	O
network	O
.	O
An	O
approach	O
for	O
speech	O
classification	O
solely	O
using	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
CNN	B-DeepLearning
instead	O
of	O
recurrent	O
ones	O
can	O
be	O
seen	O
in	O
the	O
work	O
of	O
Milde	O
and	O
Biemann	O
.	O
GRU	B-DeepLearning
Networks	I-DeepLearning
are	O
a	O
variation	O
of	O
the	O
long	B-DeepLearning
short	I-DeepLearning
term	I-DeepLearning
memory	I-DeepLearning
LSTM	B-DeepLearning
networks	O
.	O
Conventional	O
RNN	B-DeepLearning
structures	O
propagate	O
information	O
only	O
forwards	O
in	O
time	O
.	O
In	O
this	O
context	O
,	O
bidirectional	B-DeepLearning
RNNs	I-DeepLearning
can	O
be	O
helpful	O
by	O
having	O
separate	O
layers	O
processing	O
the	O
two	O
different	O
directions	O
and	O
feeding	O
each	O
others	O
output	O
into	O
the	O
same	O
output	O
layer	O
as	O
it	O
is	O
depicted	O
in	O
Fig	O
.	O
Here	O
,	O
the	O
output	O
of	O
the	O
forward	B-DeepLearning
layer	I-DeepLearning
is	O
not	O
directly	O
propagated	O
towards	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
,	O
instead	O
it	O
serves	O
as	O
the	O
input	O
of	O
the	O
backward	B-DeepLearning
layer	I-DeepLearning
.	O
Deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
usually	O
come	O
with	O
a	O
large	O
amount	O
of	O
parameters	B-DeepLearning
,	O
leaving	O
them	O
prone	O
to	O
overfitting	B-DeepLearning
.	O
One	O
straightforward	O
way	O
of	O
avoiding	O
that	O
is	O
using	O
Dropout	B-DeepLearning
.	O
Those	O
are	O
applied	O
by	O
multiplying	O
the	O
output	O
of	O
each	O
node	O
that	O
propagates	O
towards	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
by	O
some	O
random	O
noise	B-DeepLearning
with	O
each	O
batch	B-DeepLearning
of	I-DeepLearning
training	I-DeepLearning
data	I-DeepLearning
.	O
The	O
amount	O
of	O
deactivated	O
nodes	O
depends	O
on	O
the	O
dropout	B-DeepLearning
rate	O
p	O
which	O
can	O
be	O
seen	O
as	O
the	O
distribution’s	O
parameter	B-DeepLearning
in	O
the	O
binary	O
case	O
.	O
The	O
positive	O
effect	O
of	O
better	O
generalization	B-DeepLearning
capabilities	O
can	O
be	O
explained	O
in	O
two	O
ways	O
.	O
First	O
,	O
it	O
essentially	O
produces	O
an	O
ensemble	B-DeepLearning
of	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
and	O
therefore	O
producing	O
an	O
equally	O
averaged	O
result	O
over	O
those	O
.	O
The	O
RNNs	B-DeepLearning
are	O
built	O
in	O
Python	B-DeepLearning
using	O
the	O
Keras	B-DeepLearning
framework	O
.	O
We	O
use	O
two	O
GRU	B-DeepLearning
layers	O
with	O
128	O
hidden	B-DeepLearning
states	I-DeepLearning
each	O
.	O
For	O
the	O
merged	O
BRNN	B-DeepLearning
those	O
are	O
both	O
connected	O
to	O
the	O
input	O
and	O
combine	O
their	O
results	O
to	O
a	O
dropout	B-DeepLearning
layer	O
with	O
p	O
=	O
04	O
.	O
For	O
the	O
sequential	B-DeepLearning
BRNN	I-DeepLearning
the	O
first	O
GRU	B-DeepLearning
layer	I-DeepLearning
produces	O
another	O
time-dependent	O
sequence	O
which	O
is	O
then	O
fed	O
into	O
the	O
backward	B-DeepLearning
GRU	I-DeepLearning
layer	I-DeepLearning
.	O
After	O
each	O
of	O
those	O
follows	O
one	O
layer	O
of	O
dropout	B-DeepLearning
with	O
p	O
=	O
04	O
.	O
For	O
both	O
types	O
of	O
networks	O
we	O
finally	O
use	O
one	O
to	O
three	O
fully	B-DeepLearning
connected	I-DeepLearning
dense	I-DeepLearning
layers	O
with	O
,	O
again	O
,	O
128	O
hidden	B-DeepLearning
states	I-DeepLearning
each	O
,	O
eventually	O
followed	O
by	O
another	O
single-state	O
layer	O
which	O
produces	O
the	O
output	O
by	O
applying	O
the	O
sigmoid	B-DeepLearning
function	O
which	O
is	O
also	O
used	O
as	O
the	O
inner	O
activation	O
of	O
the	O
GRU	B-DeepLearning
nodes	I-DeepLearning
.	O
The	O
remaining	O
activation	B-DeepLearning
functions	I-DeepLearning
between	O
each	O
two	O
nodes	O
are	O
defined	O
as	O
the	O
rectifier	O
or	O
relu	B-DeepLearning
function	O
.	O
As	O
optimizer	B-DeepLearning
we	O
use	O
Adam	B-DeepLearning
.	O
In	O
earlier	O
stages	O
,	O
RMSprop	O
has	O
also	O
been	O
tried	O
,	O
but	O
it	O
clearly	O
proved	O
to	O
perform	O
worse	O
on	O
that	O
learning	O
task	O
and	O
also	O
oscillated	O
a	O
lot	O
,	O
making	O
it	O
useless	O
for	O
the	O
early	B-DeepLearning
stopping	I-DeepLearning
described	O
in	O
Sect.	O
44	O
.	O
The	O
loss	O
which	O
is	O
minimized	O
in	O
the	O
training	O
stage	O
is	O
the	O
binary	B-DeepLearning
cross-entropy	I-DeepLearning
.	O
The	O
model	B-DeepLearning
is	O
then	O
trained	O
for	O
a	O
maximum	O
of	O
50	O
epochs	B-DeepLearning
and	O
evaluated	O
on	O
the	O
remaining	O
900	O
samples	O
of	O
the	O
development-test	B-DeepLearning
set	I-DeepLearning
after	O
each	O
iteration	B-DeepLearning
.	O
Training	O
is	O
stopped	O
when	O
the	O
UAR	O
of	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
does	O
not	O
increase	O
for	O
10	O
epochs	B-DeepLearning
.	O
A	O
model	B-DeepLearning
checkpoint	O
is	O
used	O
to	O
keep	O
track	O
of	O
the	O
weights	B-DeepLearning
which	O
have	O
been	O
used	O
to	O
reach	O
the	O
highest	O
UAR	B-DeepLearning
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
up	O
to	O
the	O
last	O
epoch	B-DeepLearning
.	O
Finally	O
we	O
obtain	O
a	O
measure	O
of	O
general	O
performance	O
by	O
using	O
the	O
highest	O
scoring	O
model	B-DeepLearning
to	O
predict	O
the	O
labels	O
on	O
the	O
testing	B-DeepLearning
set	I-DeepLearning
.	O
Also	O
,	O
the	O
Gaussian	B-DeepLearning
dropout	I-DeepLearning
leads	O
to	O
slightly	O
higher	O
performance	O
than	O
the	O
binary	O
one	O
.	O
In	O
most	O
cases	O
the	O
network	O
with	O
two	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
after	O
the	O
recurrent	O
ones	O
achieves	O
the	O
highest	O
measures	O
.	O
Eventually	O
we	O
used	O
the	O
sequential	B-DeepLearning
BRNN	I-DeepLearning
with	O
two	O
dense	B-DeepLearning
layers	I-DeepLearning
and	O
Gaussian	B-DeepLearning
dropout	I-DeepLearning
after	O
being	O
trained	O
on	O
the	O
sets	O
specified	O
in	O
Sect.	O
4.4	O
to	O
predict	O
the	O
labels	O
of	O
the	O
testing	B-DeepLearning
set	I-DeepLearning
and	O
reached	O
a	O
UAR	B-DeepLearning
of	O
71.03	O
and	O
an	O
accuracy	O
of	O
71.30	O
percent	O
.	O
Also	O
it	O
became	O
clear	O
,	O
that	O
there	O
exists	O
a	O
variant	O
of	O
BRNNs	B-DeepLearning
which	O
has	O
,	O
to	O
our	O
knowledge	O
,	O
not	O
been	O
researched	O
thoroughly	O
yet	O
.	O
The	O
topic	O
of	O
the	O
paper	O
is	O
the	O
training	O
of	O
deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
which	O
use	O
tunable	O
piecewise-linear	B-DeepLearning
activation	I-DeepLearning
functions	I-DeepLearning
called	O
maxout	B-DeepLearning
for	O
speech	O
recognition	O
tasks	O
.	O
Maxout	B-DeepLearning
networks	I-DeepLearning
are	O
compared	O
to	O
the	O
conventional	O
fully-connected	B-DeepLearning
DNNs	I-DeepLearning
in	O
case	O
of	O
training	O
with	O
both	O
crossentropy	B-DeepLearning
and	O
sequence	B-DeepLearning
discriminative	I-DeepLearning
sMBR	O
criteria	O
.	O
The	O
clear	O
advantage	O
of	O
maxout	B-DeepLearning
networks	I-DeepLearning
over	O
DNNs	B-DeepLearning
is	O
demonstrated	O
when	O
using	O
the	O
cross-entropy	B-DeepLearning
criterion	O
on	O
both	O
corpora	O
.	O
It	O
is	O
also	O
argued	O
that	O
maxout	B-DeepLearning
networks	I-DeepLearning
are	O
prone	O
to	O
overfitting	B-DeepLearning
during	O
sequence	O
training	O
but	O
in	O
some	O
cases	O
it	O
can	O
be	O
successfully	O
overcome	O
with	O
the	O
use	O
of	O
the	O
KL-divergence	O
based	O
regularization	B-DeepLearning
.	O
The	O
greedy	B-DeepLearning
layerwise	I-DeepLearning
pretraining	I-DeepLearning
method	O
became	O
an	O
impulse	O
for	O
tempestuous	O
development	O
of	O
DNN	B-DeepLearning
training	O
.	O
The	O
pretraining	O
results	O
in	O
DNN	B-DeepLearning
weights	B-DeepLearning
initialization	O
that	O
facilitates	O
the	O
subsequent	O
finetuning	O
and	O
improves	O
its	O
quality	O
.	O
Nevertheless	O
,	O
the	O
fully	B-DeepLearning
connected	I-DeepLearning
feedforward	B-DeepLearning
deep	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
hereinafter	O
just	O
DNNs	B-DeepLearning
for	O
brevity	O
still	O
remain	O
the	O
workhorses	O
of	O
the	O
large	O
majority	O
of	O
ASR	O
systems	O
.	O
The	O
introduction	O
of	O
piecewise-linear	B-DeepLearning
ReLU	I-DeepLearning
rectified	B-DeepLearning
linear	I-DeepLearning
units	I-DeepLearning
activation	B-DeepLearning
functions	I-DeepLearning
made	O
it	O
possible	O
to	O
simplify	O
and	O
improve	O
the	O
optimization	O
process	O
during	O
DNN	B-DeepLearning
training	O
significantly	O
.	O
It	O
was	O
shown	O
that	O
DNNs	B-DeepLearning
with	O
ReLU	B-DeepLearning
activations	O
may	O
be	O
successfully	O
learned	O
without	O
layerwise	B-DeepLearning
pretraining	I-DeepLearning
.	O
However	O
,	O
the	O
fact	O
that	O
ReLU	B-DeepLearning
and	O
its	O
analogues	O
are	O
linear	O
almost	O
everywhere	O
may	O
also	O
result	O
in	O
overfitting	B-DeepLearning
and	O
instability	O
of	O
the	O
training	O
process	O
.	O
This	O
implies	O
the	O
necessity	O
of	O
using	O
effective	O
regularization	B-DeepLearning
techniques	O
.	O
Dropout	B-DeepLearning
can	O
be	O
treated	O
as	O
an	O
approximate	O
way	O
to	O
learn	O
the	O
exponentially	O
large	O
ensemble	O
of	O
different	O
neural	B-DeepLearning
nets	I-DeepLearning
with	O
subsequent	O
averaging	O
.	O
To	O
improve	O
efficiency	O
of	O
the	O
dropout	B-DeepLearning
regularization	I-DeepLearning
a	O
new	O
sort	O
of	O
tunable	O
piecewise-linear	B-DeepLearning
activation	I-DeepLearning
function	I-DeepLearning
called	O
maxout	B-DeepLearning
was	O
proposed	O
in	O
9	O
.	O
In	O
a	O
short	O
time	O
the	O
deep	B-DeepLearning
maxout	I-DeepLearning
networks	I-DeepLearning
DMN	B-DeepLearning
were	O
applied	O
to	O
speech	O
recognition	O
tasks	O
.	O
It	O
was	O
also	O
shown	O
that	O
dropout	B-DeepLearning
for	O
DMNs	B-DeepLearning
can	O
be	O
very	O
effective	O
in	O
an	O
annealed	O
mode	O
,	O
i.e.	O
if	O
the	O
dropout	B-DeepLearning
rate	O
gradually	O
decreases	O
epoch-by-epoch	B-DeepLearning
.	O
We	O
apply	O
DMNs	B-DeepLearning
to	O
two	O
significantly	O
different	O
tasks	O
and	O
demonstrate	O
their	O
clear	O
superiority	O
over	O
conventional	O
DNNs	B-DeepLearning
under	O
cross-entropy	B-DeepLearning
CE	B-DeepLearning
training	O
.	O
Dropout	B-DeepLearning
proposed	O
in	O
is	O
a	O
regularization	B-DeepLearning
technique	O
for	O
deep	B-DeepLearning
feedforward	I-DeepLearning
network	I-DeepLearning
training	O
which	O
is	O
effective	O
in	O
particular	O
for	O
training	O
network	O
with	O
a	O
large	O
amount	O
of	O
parameters	B-DeepLearning
on	O
a	O
limited	O
size	O
dataset	B-DeepLearning
.	O
In	O
the	O
original	O
form	O
of	O
dropout	B-DeepLearning
it	O
was	O
proposed	O
to	O
randomly	O
turn	O
off	O
half	O
of	O
neurons	O
per	O
every	O
training	O
example	O
.	O
When	O
the	O
neurons	O
with	O
piecewise-linear	B-DeepLearning
activation	I-DeepLearning
functions	O
like	O
ReLUx	B-DeepLearning
=	O
max0	O
,	O
x	O
are	O
used	O
the	O
input	O
space	O
is	O
divided	O
into	O
multiple	O
regions	O
where	O
data	O
is	O
linearly	O
transformed	O
by	O
the	O
network	O
.	O
From	O
this	O
point	O
of	O
view	O
using	O
of	O
dropout	B-DeepLearning
with	O
piecewise-linear	B-DeepLearning
activation	I-DeepLearning
functions	O
performs	O
the	O
more	O
exact	O
ensemble	O
averaging	O
than	O
with	O
activation	B-DeepLearning
functions	I-DeepLearning
of	O
nonzero	O
curvature	O
such	O
as	O
sigmoid	B-DeepLearning
.	O
The	O
effectiveness	O
of	O
DNNs	B-DeepLearning
with	O
ReLU	B-DeepLearning
neurons	O
trained	O
with	O
dropout	B-DeepLearning
was	O
demonstrated	O
on	O
many	O
tasks	O
from	O
different	O
domains	O
.	O
Maxout	B-DeepLearning
is	O
a	O
piecewise-linear	B-DeepLearning
activation	I-DeepLearning
function	O
which	O
was	O
proposed	O
to	O
improve	O
neural	B-DeepLearning
network	I-DeepLearning
training	O
with	O
dropout	B-DeepLearning
.	O
The	O
advantage	O
of	O
the	O
maxout	O
activation	B-DeepLearning
function	I-DeepLearning
over	O
ReLU	B-DeepLearning
is	O
the	O
possibility	O
to	O
adjust	O
its	O
form	O
by	O
means	O
of	O
parameters	B-DeepLearning
tuning	I-DeepLearning
although	O
it	O
comes	O
at	O
the	O
cost	O
of	O
k-fold	B-DeepLearning
increasing	O
of	O
the	O
parameters	B-DeepLearning
number	O
.	O
Maxout	B-DeepLearning
networks	I-DeepLearning
both	O
fully-connected	O
and	O
convolutional	B-DeepLearning
demonstrated	O
impressive	O
results	O
on	O
several	O
benchmark	O
tasks	O
from	O
the	O
computer	O
vision	O
domain	O
.	O
The	O
first	O
application	O
of	O
Deep	B-DeepLearning
Maxout	I-DeepLearning
Networks	I-DeepLearning
DMN	B-DeepLearning
to	O
speech	O
recognition	O
task	O
seems	O
to	O
be	O
done	O
in	O
February	O
and	O
in	O
March	O
,	O
where	O
it	O
was	O
shown	O
that	O
training	O
of	O
DMNs	B-DeepLearning
can	O
be	O
effective	O
even	O
without	O
using	O
dropout	B-DeepLearning
.	O
Nevertheless	O
,	O
the	O
training	O
of	O
DMNs	B-DeepLearning
can	O
be	O
successfully	O
combined	O
with	O
dropout	B-DeepLearning
regularization	I-DeepLearning
as	O
it	O
was	O
shown	O
here	O
.	O
There	O
it	O
was	O
proposed	O
to	O
use	O
not	O
the	O
conventional	O
dropout	B-DeepLearning
where	O
the	O
dropout	B-DeepLearning
rate	O
is	O
constant	O
during	O
the	O
entire	O
training	O
but	O
the	O
annealed	B-DeepLearning
dropout	I-DeepLearning
AD	O
which	O
consists	O
of	O
the	O
gradually	O
decreasing	O
dropout	B-DeepLearning
rate	O
according	O
to	O
the	O
linear	O
schedule	O
.	O
We	O
do	O
not	O
compare	O
Maxout	B-DeepLearning
+	O
AD	O
to	O
ReLU	B-DeepLearning
+	O
AD	O
because	O
,	O
in	O
our	O
experience	O
,	O
AD	O
training	O
of	O
ReLU	B-DeepLearning
networks	O
does	O
not	O
provide	O
significant	O
WER	O
reduction	O
it	O
is	O
also	O
observed	O
in	O
the	O
previous	O
paper	O
whilist	O
carefully	O
tuned	O
sigmoidal	B-DeepLearning
DNNs	I-DeepLearning
with	O
L2	B-DeepLearning
weight	I-DeepLearning
decay	I-DeepLearning
often	O
outperform	O
ReLU	B-DeepLearning
DNNs	B-DeepLearning
with	O
dropout	B-DeepLearning
and	O
other	O
types	O
of	O
regularization	B-DeepLearning
.	O
When	O
the	O
model	B-DeepLearning
was	O
trained	O
the	O
low-rank	O
factorization	O
based	O
on	O
SVD	O
of	O
the	O
last	O
hidden	B-DeepLearning
layer	I-DeepLearning
was	O
performed	O
and	O
the	O
model	B-DeepLearning
was	O
fine-tuned	O
to	O
provide	O
bottleneck	O
features	O
,	O
hereinafter	O
SDBNs	O
Speaker-Dependent	O
BottleNeck	O
.	O
In	O
our	O
first	O
attempts	O
we	O
did	O
not	O
use	O
regularization	B-DeepLearning
.	O
Since	O
we	O
observed	O
that	O
the	O
cross-validation	B-DeepLearning
value	O
of	O
the	O
sMBR	O
criterion	O
either	O
increases	O
or	O
remains	O
constant	O
epoch-by-epoch	B-DeepLearning
we	O
concluded	O
that	O
the	O
model	B-DeepLearning
is	O
overfit	B-DeepLearning
.	O
So	O
to	O
improve	O
ST	O
performance	O
an	O
effective	O
regularization	B-DeepLearning
is	O
required	O
.	O
We	O
tried	O
to	O
use	O
the	O
L1	B-DeepLearning
and	O
L2	B-DeepLearning
penalty	O
as	O
well	O
as	O
dropout	B-DeepLearning
and	O
F-smoothing	O
to	O
make	O
ST	O
work	O
on	O
Switchboard	O
,	O
however	O
none	O
of	O
these	O
approaches	O
succeeded	O
in	O
overfitting	B-DeepLearning
reduction	O
.	O
We	O
considered	O
the	O
use	O
of	O
maxout	B-DeepLearning
activation	I-DeepLearning
functions	I-DeepLearning
for	O
training	O
deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
as	O
acoustic	O
models	O
for	O
ASR	O
.	O
Using	O
two	O
English	O
speech	O
corpora	O
namely	O
CHiME	O
Challenge	O
2015	O
dataset	B-DeepLearning
and	O
Switchboard	O
we	O
demonstrated	O
that	O
in	O
case	O
of	O
training	O
with	O
the	O
cross-entropy	B-DeepLearning
criterion	O
Deep	B-DeepLearning
Maxout	I-DeepLearning
Networks	I-DeepLearning
DMN	B-DeepLearning
are	O
superior	O
to	O
conventional	O
fully-connected	O
feedforward	B-DeepLearning
sigmoidal	I-DeepLearning
DNNs	I-DeepLearning
.	O
For	O
the	O
same	O
layer	B-DeepLearning
sizes	I-DeepLearning
the	O
number	O
of	O
DMN	B-DeepLearning
parameters	B-DeepLearning
is	O
larger	O
than	O
that	O
of	O
DNN	B-DeepLearning
but	O
the	O
increase	O
in	O
DNN	B-DeepLearning
layer	I-DeepLearning
sizes	I-DeepLearning
is	O
unable	O
to	O
provide	O
the	O
comparable	O
accuracy	O
gain	O
.	O
We	O
also	O
found	O
that	O
sequence	B-DeepLearning
discriminative	I-DeepLearning
training	O
of	O
maxout	B-DeepLearning
networks	I-DeepLearning
is	O
prone	O
to	O
overfitting	B-DeepLearning
which	O
can	O
be	O
reduced	O
with	O
the	O
use	O
of	O
KLD-regularization	B-DeepLearning
.	O
The	O
performance	O
of	O
the	O
developed	O
models	B-DeepLearning
was	O
examined	O
in	O
terms	O
of	O
the	O
Area	O
Under	O
the	O
Curve	O
AUC	B-DeepLearning
derived	O
from	O
the	O
ROC	B-DeepLearning
curves	I-DeepLearning
as	O
well	O
as	O
Pearson	O
coefficient	O
R	O
between	O
the	O
predicted	O
and	O
the	O
actual	O
efficacy	O
.	O
Associative	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
ASNN	B-DeepLearning
approach	O
and	O
fragment	O
descriptors	O
were	O
used	O
to	O
build	O
the	O
models	B-DeepLearning
.	O
In	O
three	O
layers	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
each	O
neuron	O
in	O
the	O
initial	B-DeepLearning
layer	I-DeepLearning
corresponded	O
to	O
one	O
molecular	O
descriptor	O
.	O
Hidden	B-DeepLearning
layer	I-DeepLearning
contained	O
from	O
three	O
to	O
six	B-DeepLearning
neurons	I-DeepLearning
,	O
whereas	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
contained	O
one	O
for	O
STL	O
and	O
FN	O
or	O
11	O
MTL	O
neurons	O
,	O
corresponding	O
to	O
the	O
number	O
of	O
simultaneously	O
treated	O
properties	O
.	O
Each	O
model	B-DeepLearning
was	O
validated	O
using	O
external	O
fivefold	B-DeepLearning
cross-validation	I-DeepLearning
procedure	O
.	O
Supervised	O
learning	O
is	O
usually	O
achieved	O
in	O
what	O
are	O
called	O
feed-forward	B-DeepLearning
NNs	I-DeepLearning
that	O
process	O
data	O
in	O
several	O
layers	O
consisting	O
of	O
varying	O
numbers	O
of	O
nodes	O
.	O
These	O
nodes	O
are	O
organized	O
as	O
input	B-DeepLearning
nodes	I-DeepLearning
,	O
several	O
layers	O
of	O
hidden	B-DeepLearning
nodes	I-DeepLearning
,	O
and	O
output	B-DeepLearning
nodes	I-DeepLearning
.	O
A	O
sliding	O
window	O
covers	O
60	O
nucleotides	O
,	O
which	O
is	O
calculated	O
to	O
240	O
input	B-DeepLearning
units	I-DeepLearning
to	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
structure	O
is	O
a	O
standard	O
three	O
layer	O
feedforward	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
.	O
This	O
kind	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
has	O
several	O
names	O
,	O
such	O
as	O
multilayer	B-DeepLearning
perceptrons	I-DeepLearning
MLP	B-DeepLearning
,	O
feed	B-DeepLearning
forward	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
,	O
and	O
backpropagation	B-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
.	O
There	O
are	O
two	O
output	B-DeepLearning
units	I-DeepLearning
corresponding	O
to	O
the	O
donor	O
and	O
acceptor	O
splice	O
sites	O
,	O
128	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	O
and	O
240	O
input	B-DeepLearning
units	I-DeepLearning
.	O
The	O
240	O
input	B-DeepLearning
units	I-DeepLearning
were	O
used	O
since	O
the	O
orthogonal	O
input	O
scheme	O
uses	O
four	O
inputs	O
each	O
nucleotide	O
in	O
the	O
window	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
program	O
code	O
was	O
reused	O
from	O
a	O
previous	O
study	O
,	O
and	O
in	O
this	O
code	O
the	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
was	O
hard	O
coded	O
and	O
optimized	O
for	O
128	O
hidden	B-DeepLearning
units	I-DeepLearning
.	O
There	O
is	O
also	O
a	O
bias	B-DeepLearning
signal	O
added	O
to	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
and	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
activation	B-DeepLearning
function	I-DeepLearning
is	O
a	O
standard	O
sigmoid	B-DeepLearning
function	O
,	O
shown	O
in	O
Eq.	O
1	O
.	O
The	O
β	O
values	O
for	O
the	O
sigmoid	B-DeepLearning
functions	O
are	O
0.1	O
for	O
both	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
activation	O
and	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
activation	O
.	O
When	O
doing	O
forward	O
calculations	O
and	O
backpropagation	B-DeepLearning
,	O
the	O
sigmoid	B-DeepLearning
function	O
is	O
called	O
repetitively	O
.	O
A	O
fast	O
and	O
effective	O
evaluation	O
of	O
the	O
sigmoid	B-DeepLearning
function	O
can	O
improve	O
the	O
overall	O
performance	O
considerably	O
.	O
To	O
improve	O
the	O
performance	O
of	O
the	O
sigmoid	B-DeepLearning
function	O
,	O
a	O
precalculated	O
table	O
for	O
the	O
exponential	O
function	O
is	O
used	O
.	O
There	O
is	O
no	O
momentum	B-DeepLearning
used	O
in	O
the	O
training	O
.	O
We	O
have	O
not	O
implemented	O
any	O
second	O
order	O
methods	O
to	O
help	O
the	O
convergence	B-DeepLearning
of	O
the	O
weights	B-DeepLearning
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
training	O
is	O
done	O
using	O
standard	O
backpropagation	B-DeepLearning
.	O
The	O
training	O
was	O
done	O
in	O
three	O
sessions	O
,	O
and	O
for	O
each	O
session	O
we	O
chose	O
separate	O
,	O
but	O
constant	O
,	O
learning	B-DeepLearning
rates	I-DeepLearning
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
,	O
η	O
,	O
was	O
chosen	O
to	O
be	O
0.2	O
,	O
0.1	O
,	O
and	O
0.02	O
,	O
respectively	O
.	O
The	O
shown	O
indicators	O
are	O
computed	O
using	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
which	O
has	O
been	O
trained	O
for	O
about	O
80	O
epochs	B-DeepLearning
,	O
with	O
a	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
02	O
.	O
The	O
best	O
performing	O
neural	B-DeepLearning
network	I-DeepLearning
,	O
achieved	O
a	O
correlation	O
coefficient	O
of	O
0552	O
.	O
The	O
MGN	O
works	O
in	O
this	O
case	O
as	O
a	O
discrete	O
time	O
cellular	O
neural	B-DeepLearning
network	I-DeepLearning
and	O
the	O
training	O
is	O
based	O
on	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
as	O
described	O
in	O
the	O
paper	O
.	O
The	O
training	O
of	O
a	O
TPNN	O
is	O
based	O
on	O
a	O
combination	O
of	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descend	I-DeepLearning
and	O
back	B-DeepLearning
propagation	I-DeepLearning
with	O
several	O
improvements	O
that	O
make	O
the	O
training	O
of	O
the	O
shared	O
weights	O
feasible	O
and	O
that	O
were	O
reported	O
in	O
detail	O
in	O
the	O
context	O
of	O
training	O
CNNs	B-DeepLearning
for	O
pattern	O
recognition	O
.	O
In	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
,	O
the	O
true	O
gradient	O
is	O
approximated	O
by	O
the	O
gradient	O
of	O
the	O
loss	B-DeepLearning
function	I-DeepLearning
which	O
is	O
evaluated	O
on	O
a	O
single	O
training	O
sample	O
.	O
Weight	B-DeepLearning
decay	I-DeepLearning
is	O
a	O
regularization	B-DeepLearning
method	O
that	O
penalizes	O
large	O
weights	B-DeepLearning
in	O
the	O
network	O
.	O
The	O
weight	B-DeepLearning
decay	I-DeepLearning
penalty	O
term	O
causes	O
the	O
insignificant	O
weights	B-DeepLearning
to	O
converge	O
to	O
zero	O
.	O
The	O
parameter	B-DeepLearning
µ	O
is	O
controlling	O
the	O
stepsize	B-DeepLearning
of	O
the	O
gradient	B-DeepLearning
descent	I-DeepLearning
.	O
The	O
initial	O
step	B-DeepLearning
size	I-DeepLearning
is	O
already	O
small	O
around	O
µ	O
=	O
0.01	O
and	O
it	O
is	O
decreased	O
after	O
each	O
training	O
epoch	B-DeepLearning
with	O
a	O
constant	O
factor	O
.	O
This	O
is	O
necessary	O
to	O
achieve	O
a	O
slow	O
convergence	B-DeepLearning
of	O
the	O
weights	B-DeepLearning
.	O
A	O
sweep	O
through	O
the	O
whole	O
data	B-DeepLearning
set	I-DeepLearning
is	O
called	O
one	O
epoch	B-DeepLearning
.	O
Up	O
to	O
1000	O
epochs	B-DeepLearning
are	O
necessary	O
to	O
train	O
the	O
TPNN	O
in	O
a	O
typical	O
experimental	O
setup	O
20	O
amino	O
acids	O
in	O
the	O
alphabet	O
,	O
peptides	O
of	O
length	O
5	O
to	O
10	O
,	O
30-50	O
training	O
samples	O
from	O
measurements	O
as	O
a	O
starting	O
set	O
.	O
The	O
slow	O
convergence	B-DeepLearning
of	O
the	O
error	O
is	O
a	O
consequence	O
of	O
the	O
relative	O
small	O
stepsize	B-DeepLearning
in	O
the	O
gradient	B-DeepLearning
descent	I-DeepLearning
,	O
but	O
it	O
is	O
necessary	O
in	O
order	O
to	O
get	O
overall	O
convergence	B-DeepLearning
of	O
the	O
weights	B-DeepLearning
.	O
It	O
is	O
well	O
known	O
,	O
that	O
neural	B-DeepLearning
network	I-DeepLearning
ensembles	I-DeepLearning
perform	O
better	O
in	O
terms	O
of	O
generalisation	B-DeepLearning
than	O
single	O
models	B-DeepLearning
would	O
do	O
.	O
An	O
ensemble	O
of	O
TPNNs	O
consists	O
of	O
several	O
single	O
TPNN	O
models	O
that	O
are	O
trained	O
on	O
randomly	O
chosen	O
subsets	O
of	O
the	O
training	B-DeepLearning
data	I-DeepLearning
and	O
the	O
training	O
starts	O
with	O
random	O
weight	B-DeepLearning
initializations	I-DeepLearning
.	O
Two	O
analytical	O
models	O
have	O
been	O
used	O
to	O
fit	O
the	O
data	B-DeepLearning
set	I-DeepLearning
to	O
a	O
prognostic	O
index:	O
a	O
piecewise	O
linear	O
model	O
Cox	O
regression	O
,	O
also	O
known	O
as	O
proportional	O
hazards	O
,	O
and	O
a	O
flexible	O
model	O
consisting	O
of	O
Partial	O
Logistic	O
Artificial	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
regularised	O
with	O
Automatic	O
Relevance	O
Determination	O
PLANN-ARD	O
.	O
If	O
these	O
features	O
are	O
stored	O
in	O
a	O
vector	B-DeepLearning
f	O
=	O
f1...fh	O
,	O
and	O
if	O
we	O
represent	O
the	O
i-th	O
residue	O
in	O
the	O
sequence	O
as	O
ri	O
,	O
then	O
f	O
is	O
obtained	O
as:	O
where	O
N	O
h	O
is	O
a	O
non-linear	O
function	O
,	O
which	O
we	O
implement	O
by	O
a	O
two-layered	O
feedforward	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
with	O
h	O
non-linear	O
output	B-DeepLearning
units	I-DeepLearning
.	O
The	O
number	O
of	O
free	O
parameters	B-DeepLearning
in	O
the	O
overall	O
N1-NN	B-DeepLearning
can	O
be	O
controlled	O
by:	O
the	O
number	O
of	O
units	O
in	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
the	O
sequence-to-feature	O
network	O
N	O
h	O
N	O
H	O
f	O
the	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
in	O
the	O
feature-to-output	O
network	O
N	O
o	O
,	O
N	O
H	O
o	O
the	O
number	O
of	O
hidden	B-DeepLearning
states	I-DeepLearning
in	O
the	O
feature	B-DeepLearning
vector	I-DeepLearning
f	O
,	O
which	O
is	O
also	O
the	O
number	O
of	O
output	B-DeepLearning
units	I-DeepLearning
in	O
the	O
sequence-to-feature	O
network	O
,	O
Nf	O
.	O
Each	O
training	O
is	O
conducted	O
by	O
10	O
fold-cross	B-DeepLearning
validation	I-DeepLearning
,	O
i.e.	O
10	O
different	O
sets	O
of	O
training	O
runs	O
are	O
performed	O
in	O
which	O
a	O
different	O
tenth	O
of	O
the	O
overall	O
set	O
is	O
reserved	O
for	O
testing	O
.	O
The	O
training	B-DeepLearning
set	I-DeepLearning
is	O
used	O
to	O
learn	O
the	O
free	O
parameters	B-DeepLearning
of	O
the	O
network	O
by	O
gradient	B-DeepLearning
descent	I-DeepLearning
,	O
while	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
is	O
used	O
to	O
choose	O
model	B-DeepLearning
and	O
hyperparameters	B-DeepLearning
network	B-DeepLearning
size	I-DeepLearning
and	O
architecture	O
,	O
i.e.	O
N	O
H	O
f	O
Nf	O
and	O
N	O
H	O
o	O
.	O
For	O
each	O
fold	O
the	O
three	O
networks	O
for	O
the	O
best	O
architecture	O
are	O
ensemble	O
averaged	O
and	O
evaluated	O
on	O
the	O
corresponding	O
test	B-DeepLearning
set	I-DeepLearning
.	O
Training	O
is	O
performed	O
by	O
gradient	B-DeepLearning
descent	I-DeepLearning
on	O
the	O
error	O
,	O
which	O
we	O
model	O
as	O
the	O
relative	O
entropy	O
between	O
the	O
target	O
class	O
and	O
the	O
output	O
of	O
the	O
network	O
.	O
The	O
overall	O
output	O
of	O
the	O
network	O
output	O
layer	O
of	O
N	O
o	O
is	O
implemented	O
as	O
a	O
softmax	B-DeepLearning
function	O
,	O
while	O
all	O
internal	O
squashing	O
functions	O
are	O
implemented	O
as	O
hyperbolic	B-DeepLearning
tangents	I-DeepLearning
.	O
Training	O
terminates	O
when	O
either	O
the	O
walltime	O
on	O
the	O
server	O
is	O
reached	O
6	O
days	O
for	O
fungi	O
and	O
plants	O
,	O
10	O
days	O
for	O
animals	O
or	O
the	O
epoch	B-DeepLearning
limit	O
is	O
reached	O
40k	O
for	O
fungi	O
and	O
plants	O
and	O
20k	O
for	O
animals	O
.	O
The	O
gradient	O
is	O
updated	O
360	O
times	O
for	O
each	O
epoch	B-DeepLearning
or	O
once	O
every	O
2-6	O
examples	O
,	O
depending	O
on	O
the	O
set	O
,	O
and	O
the	O
examples	O
are	O
shuffled	O
between	O
epochs	B-DeepLearning
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
is	O
halved	O
every	O
time	O
a	O
reduction	O
of	O
the	O
error	O
is	O
not	O
observed	O
for	O
more	O
than	O
50	O
epochs	B-DeepLearning
.	O
Both	O
predictors	O
are	O
assessed	O
by	O
ten-fold	B-DeepLearning
cross-validation	I-DeepLearning
.	O
The	O
type	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
which	O
we	O
performed	O
the	O
experiments	B-DeepLearning
was	O
Multilayer	B-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
,	O
because	O
the	O
results	O
obtained	O
with	O
other	O
types	O
of	O
networks	O
were	O
not	O
satisfactory	O
and	O
they	O
tended	O
to	O
be	O
slower	O
than	O
MLP	B-DeepLearning
.	O
The	O
preparatory	O
steps	O
for	O
conducting	O
the	O
experiments	B-DeepLearning
consisted	O
in	O
determining:	O
a	O
the	O
size	O
of	O
the	O
training	O
and	O
testing	B-DeepLearning
set	I-DeepLearning
b	O
the	O
error	B-DeepLearning
function	I-DeepLearning
c	O
the	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
d	O
the	O
activation	B-DeepLearning
functions	I-DeepLearning
for	O
the	O
hidden	O
and	O
the	O
output	B-DeepLearning
neurons	I-DeepLearning
e	O
the	O
minimum	O
and	O
maximum	O
values	O
for	O
weight	B-DeepLearning
decay	I-DeepLearning
.	O
For	O
the	O
hidden	B-DeepLearning
neurons	I-DeepLearning
were	O
used	O
as	O
activation	B-DeepLearning
functions	I-DeepLearning
identity	O
,	O
logistic	O
,	O
tanh	B-DeepLearning
and	O
exponential	O
and	O
the	O
same	O
ones	O
were	O
chosen	O
for	O
the	O
output	B-DeepLearning
neurons	I-DeepLearning
.	O
The	O
error	B-DeepLearning
functions	I-DeepLearning
used	O
were	O
sum	B-DeepLearning
of	I-DeepLearning
squares	I-DeepLearning
and	O
cross	B-DeepLearning
entropy	I-DeepLearning
.	O
The	O
minimum	O
and	O
the	O
maximum	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
were	O
chosen	O
differently	O
for	O
each	O
experiment	B-DeepLearning
because	O
we	O
conducted	O
several	O
experiments	B-DeepLearning
which	O
had	O
different	O
number	O
of	O
inputs	O
.	O
The	O
larger	O
the	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
in	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
model	B-DeepLearning
the	O
stronger	O
the	O
model	B-DeepLearning
is	O
,	O
the	O
more	O
capable	O
the	O
network	O
is	O
to	O
model	O
complex	O
relationships	O
between	O
the	O
inputs	O
and	O
the	O
target	O
variables	O
.	O
The	O
optimal	O
number	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
is	O
minimum	O
1/10	O
of	O
the	O
number	O
of	O
training	O
cases	O
and	O
maximum	O
1/5	O
,	O
but	O
we	O
have	O
varied	O
this	O
interval	O
.	O
The	O
use	O
of	O
decay	B-DeepLearning
weights	I-DeepLearning
for	O
hidden	B-DeepLearning
layer	I-DeepLearning
and	O
output	B-DeepLearning
layer	I-DeepLearning
was	O
preferred	O
in	O
order	O
to	O
prevent	O
overfitting	B-DeepLearning
,	O
thereby	O
potentially	O
improving	O
generalization	B-DeepLearning
performance	O
of	O
the	O
network	O
.	O
Weight	B-DeepLearning
decay	I-DeepLearning
or	O
weight	B-DeepLearning
elimination	O
are	O
often	O
used	O
in	O
MLP	B-DeepLearning
training	O
and	O
aim	O
to	O
minimize	O
a	O
cost	B-DeepLearning
function	I-DeepLearning
which	O
penalizes	O
large	O
weights	B-DeepLearning
.	O
These	O
techniques	O
tend	O
to	O
result	O
in	O
networks	O
with	O
smaller	O
weights	B-DeepLearning
.	O
The	O
minimum	O
chosen	O
weight	B-DeepLearning
decay	I-DeepLearning
was	O
0.0001	O
and	O
the	O
maximum	O
0001	O
.	O
To	O
perform	O
the	O
experiments	B-DeepLearning
the	O
data	O
were	O
split	O
in	O
a	O
training	O
67	O
%	O
and	O
a	O
testing	B-DeepLearning
dataset	I-DeepLearning
33	O
%	O
.	O
As	O
error	B-DeepLearning
functions	I-DeepLearning
,	O
we	O
tested	O
the	O
cross	B-DeepLearning
entropy	I-DeepLearning
and	O
the	O
sum	B-DeepLearning
of	I-DeepLearning
squares	I-DeepLearning
.	O
The	O
weight	B-DeepLearning
decays	I-DeepLearning
in	O
both	O
the	O
hidden	O
and	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
varied	O
between	O
0.0001	O
and	O
0001	O
.	O
The	O
values	O
of	O
the	O
CV	O
criterion	O
Table	O
1	O
show	O
a	O
better	O
performance	O
in	O
correspondence	O
to	O
a	O
choice	O
of	O
6	O
hidden	B-DeepLearning
units	I-DeepLearning
and	O
decay	B-DeepLearning
parameter	I-DeepLearning
equal	O
to	O
0.01	O
,	O
that	O
we	O
therefore	O
used	O
.	O
Ten-fold	B-DeepLearning
cross-validation	I-DeepLearning
obtained	O
by	O
averaging	O
over	O
five	O
fits	O
for	O
various	O
values	O
of	O
the	O
number	O
of	O
hidden	O
units	O
H	O
and	O
of	O
the	O
decay	B-DeepLearning
parameter	I-DeepLearning
.	O
Scheme	O
of	O
a	O
multilayer	B-DeepLearning
perceptron	I-DeepLearning
with	O
the	O
layer	O
of	O
inputs	O
x1...xp	O
,	O
t	O
,	O
the	O
layer	O
of	O
hidden	B-DeepLearning
units	I-DeepLearning
z1...zH	O
and	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
,	O
here	O
represented	O
by	O
the	O
unique	O
response	O
h	O
.	O
Additionally	O
,	O
one	O
of	O
the	O
authors	O
recently	O
took	O
advantage	O
of	O
a	O
deep	B-DeepLearning
learning	I-DeepLearning
algorithm	O
,	O
built	O
on	O
a	O
multilayer	O
autoencoder	B-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
,	O
that	O
lead	O
to	O
interesting	O
prediction	O
results	O
in	O
reference	O
.	O
We	O
use	O
the	O
July	O
2009	O
version	O
of	O
the	O
datasets	B-DeepLearning
for	O
analyzing	O
and	O
selecting	O
hyper-parameters	B-DeepLearning
.	O
Autoencoder	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
were	O
trained	O
using	O
the	O
free	O
GPU-accelerated	O
software	B-DeepLearning
package	I-DeepLearning
Torch7	I-DeepLearning
using	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
with	O
a	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
0.01	O
for	O
25	O
iterations	B-DeepLearning
.	O
L2	B-DeepLearning
regularization	I-DeepLearning
was	O
used	O
on	O
all	O
weights	B-DeepLearning
,	O
which	O
were	O
initialized	O
randomly	O
from	O
the	O
uniform	O
distribution	O
.	O
The	O
hidden	B-DeepLearning
unit	I-DeepLearning
function	O
is	O
a	O
Sigmoid	B-DeepLearning
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
is	O
a	O
modified	O
version	O
of	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
GRNN	B-DeepLearning
used	O
as	O
a	O
classification	O
tool	O
.	O
In	O
order	O
to	O
perform	O
the	O
barcode	O
sequences	O
classification	O
,	O
we	O
introduce	O
a	O
modified	O
version	O
of	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
GRNN	B-DeepLearning
that	O
use	O
,	O
alternatively	O
,	O
a	O
function	O
derived	O
from	O
Jaccard	O
distance	O
and	O
fractional	O
distance	O
instead	O
of	O
the	O
euclidean	O
one	O
to	O
compare	O
learned	O
prototypes	O
against	O
test	O
sequences	O
.	O
The	O
proposed	O
method	O
is	O
based	O
on	O
two	O
modified	O
versions	O
of	O
the	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
.	O
The	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
is	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
created	O
for	O
regression	B-DeepLearning
i.e.	O
the	O
approximation	O
of	O
a	O
dependent	O
variable	O
y	O
given	O
a	O
set	O
of	O
sample	O
x	O
,	O
y	O
,	O
where	O
x	O
is	O
the	O
independent	O
variable	O
.	O
Classification	O
results	O
,	O
in	O
terms	O
of	O
accuracy	B-DeepLearning
,	O
precision	B-DeepLearning
and	O
recall	B-DeepLearning
scores	O
,	O
have	O
been	O
compared	O
with	O
both	O
the	O
GRNN	B-DeepLearning
algorithm	O
using	O
J-function	O
and	O
the	O
SVM	O
classifier	O
.	O
Simple	B-DeepLearning
spiking	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
models	I-DeepLearning
such	O
as	O
Integrate	O
and	O
fire	O
models	O
without	O
bio-realistic	O
features	O
were	O
simulated	O
in	O
the	O
older	O
generation	O
GPUs	O
.	O
All	O
above	O
described	O
methods	O
require	O
to	O
set	O
two	O
hyper-parameters:	B-DeepLearning
λ	O
and	O
α	O
controlling	O
the	O
sparsity	O
and	O
the	O
network	O
influence	O
,	O
respectively	O
.	O
Deep	B-DeepLearning
learning	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
are	O
capable	O
to	O
extract	O
significant	O
features	O
from	O
raw	O
data	O
,	O
and	O
to	O
use	O
these	O
features	O
for	O
classification	O
tasks	O
.	O
In	O
this	O
work	O
we	O
present	O
a	O
deep	B-DeepLearning
learning	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
for	O
DNA	O
sequence	O
classification	O
based	O
on	O
spectral	O
sequence	O
representation	O
.	O
The	O
framework	O
is	O
tested	O
on	O
a	O
dataset	B-DeepLearning
of	O
16S	O
genes	O
and	O
its	O
performances	O
,	O
in	O
terms	O
of	O
accuracy	B-DeepLearning
and	O
F1	O
score	O
,	O
are	O
compared	O
to	O
the	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
,	O
already	O
tested	O
on	O
a	O
similar	O
problem	O
,	O
as	O
well	O
as	O
naive	O
Bayes	O
,	O
random	O
forest	O
and	O
support	O
vector	O
machine	O
classifiers	O
.	O
The	O
obtained	O
results	O
demonstrate	O
that	O
the	O
deep	B-DeepLearning
learning	I-DeepLearning
approach	I-DeepLearning
outperformed	O
all	O
the	O
other	O
classifiers	O
when	O
considering	O
classification	O
of	O
small	O
sequence	O
fragment	O
500	O
bp	O
long	O
.	O
Among	O
the	O
deep	B-DeepLearning
learning	I-DeepLearning
architecture	O
,	O
it	O
is	O
usually	O
comprised	O
the	O
LeNet-5	B-DeepLearning
network	O
,	O
or	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
CNN	B-DeepLearning
,	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
that	O
is	O
inspired	O
by	O
the	O
visual	O
system’s	O
structure	O
.	O
In	O
this	O
work	O
we	O
want	O
to	O
understand	O
if	O
the	O
convolutional	B-DeepLearning
network	I-DeepLearning
is	O
capable	O
to	O
identify	O
and	O
to	O
use	O
these	O
features	O
for	O
sequence	O
classification	O
,	O
outperforming	O
the	O
classifiers	O
proposed	O
in	O
the	O
past	O
.	O
The	O
one	O
used	O
in	O
this	O
work	O
is	O
a	O
modified	O
version	O
of	O
the	O
LeNet-5	B-DeepLearning
network	O
introduced	O
by	O
LeCun	O
et	O
al.	O
in	O
and	O
it	O
is	O
implemented	O
using	O
the	O
python	O
Theano	O
package	O
for	O
deep	B-DeepLearning
learning	I-DeepLearning
.	O
The	O
modified	O
LeNet-5	B-DeepLearning
proposed	O
network	O
is	O
made	O
of	O
two	O
lower	O
layers	O
of	O
convolutional	B-DeepLearning
and	O
max-pooling	B-DeepLearning
processing	O
elements	O
,	O
followed	O
by	O
two	O
traditional	O
fully	B-DeepLearning
connected	I-DeepLearning
Multi	B-DeepLearning
Layer	I-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
processing	O
layers	O
,	O
so	O
that	O
there	O
are	O
6	O
processing	O
layers	O
.	O
The	O
max-pooling	B-DeepLearning
is	O
a	O
non-linear	O
down-sampling	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
first	O
layer	O
of	O
convolutional	B-DeepLearning
kernels	I-DeepLearning
,	O
named	O
kernel	B-DeepLearning
0	O
,	O
is	O
made	O
of	O
L	O
=	O
10	O
kernels	B-DeepLearning
of	O
dimension	O
5	O
so	O
that	O
n	O
=	O
2	O
.	O
From	O
a	O
spectral	O
representation	O
vector	B-DeepLearning
made	O
of	O
1024	O
components	O
this	O
layer	O
produces	O
10	O
vectors	B-DeepLearning
of	O
1024	O
dimensions	O
that	O
the	O
pooling	B-DeepLearning
layer	I-DeepLearning
reduces	O
to	O
the	O
10	O
feature	O
maps	O
of	O
510	O
dimensions	O
.	O
These	O
vectors	B-DeepLearning
are	O
the	O
input	O
for	O
the	O
second	O
convolutional	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
second	O
layer	B-DeepLearning
of	I-DeepLearning
kernel	I-DeepLearning
kernel	B-DeepLearning
1	O
is	O
made	O
of	O
L	O
=	O
20	O
kernels	B-DeepLearning
of	O
dimension	O
5	O
.	O
In	O
both	O
cases	O
the	O
max-pooling	B-DeepLearning
layer	O
has	O
dimension	O
2	O
.	O
Convolution	B-DeepLearning
and	O
maxpooling	B-DeepLearning
are	O
usually	O
considered	O
together	O
and	O
they	O
are	O
represented	O
in	O
the	O
lower	O
part	O
of	O
Fig.	O
2	O
as	O
two	O
highly	O
connected	O
blocks	O
.	O
The	O
two	O
upper	O
level	O
layers	O
corresponds	O
to	O
a	O
traditional	O
fully-connected	O
MLP:	B-DeepLearning
the	O
first	O
layer	O
of	O
the	O
MLP	B-DeepLearning
operates	O
on	O
the	O
total	O
number	O
of	O
output	O
from	O
the	O
lower	O
level	O
the	O
output	O
is	O
flattened	O
to	O
a	O
1-D	B-DeepLearning
vector	I-DeepLearning
and	O
the	O
total	O
number	O
of	O
units	O
in	O
the	O
Hidden	B-DeepLearning
Layer	I-DeepLearning
is	O
500	O
.	O
In	O
the	O
second	O
case	O
,	O
the	O
ten-fold	O
cross	B-DeepLearning
validation	I-DeepLearning
scheme	O
was	O
repeated	O
considering	O
as	O
test	O
set	O
the	O
sequence	O
fragments	O
of	O
shorter	O
size	O
,	O
500	O
bp	O
long	O
,	O
obtained	O
randomly	O
extracting	O
500	O
consecutive	O
nucleotides	O
from	O
the	O
original	O
full	O
length	O
sequences	O
.	O
The	O
CNN	B-DeepLearning
134	O
has	O
been	O
run	O
considering	O
two	O
different	O
kernels	B-DeepLearning
sizes:	I-DeepLearning
kernel	B-DeepLearning
0	O
=	O
kernel	B-DeepLearning
1	O
=	O
5	O
in	O
the	O
first	O
run	O
kernel	B-DeepLearning
0	O
=	O
25	O
,	O
kernel	B-DeepLearning
1	O
=	O
15	O
in	O
the	O
second	O
run	O
.	O
In	O
both	O
configurations	O
the	O
training	B-DeepLearning
phase	I-DeepLearning
has	O
been	O
run	O
for	O
200	O
epochs	B-DeepLearning
.	O
The	O
spectral	O
representation	O
is	O
obtained	O
as	O
k-mers	O
frequencies	O
along	O
the	O
sequences	O
the	O
CNN	B-DeepLearning
belongs	O
to	O
the	O
so	O
called	O
deep	B-DeepLearning
learning	I-DeepLearning
algorithms	O
.	O
We	O
designed	O
and	O
tested	O
some	O
topic	O
modeling	O
techniques	O
,	O
and	O
we	O
took	O
advantage	O
of	O
a	O
deep	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
approach	O
.	O
A	O
convolutional	B-DeepLearning
neural	I-DeepLearning
net	I-DeepLearning
learns	O
to	O
classify	O
patches	O
as	O
salient	O
long	O
looks	O
or	O
not	O
.	O
Two	O
output	B-DeepLearning
neurons	I-DeepLearning
were	O
connected	O
to	O
the	O
reinitialized	O
layer	O
,	O
then	O
training	O
followed	O
on	O
augmented	O
800	B-DeepLearning
×	I-DeepLearning
800	I-DeepLearning
px	I-DeepLearning
patches	I-DeepLearning
for	O
10000	O
iterations	B-DeepLearning
in	O
Caffe	O
.	O
Arguments	O
of	O
rkhs$new.	O
define	O
initial	O
values	O
of	O
the	O
functions	O
and	O
the	O
initial	O
value	O
of	O
the	O
l	B-DeepLearning
2	I-DeepLearning
norm	I-DeepLearning
weighting	I-DeepLearning
parameter	I-DeepLearning
.	O
When	O
new	O
data	O
is	O
added	O
,	O
the	O
ANN	B-DeepLearning
needs	O
to	O
be	O
retrained	B-DeepLearning
in	O
order	O
to	O
achieve	O
good	O
performance	O
.	O
The	O
set	O
of	O
selected	O
features	O
are	O
processed	O
to	O
accomplish	O
the	O
voxel	O
classification	O
by	O
means	O
of	O
a	O
Multilevel	B-DeepLearning
Artificial	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
MANN	B-DeepLearning
,	O
which	O
assures	O
various	O
computational	O
advantages	O
.	O
Then	O
we	O
discuss	O
the	O
issues	O
concerning	O
the	O
minimum	O
requirements	O
that	O
an	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
should	O
fulfill	O
in	O
order	O
that	O
it	O
would	O
be	O
capable	O
of	O
expressing	O
the	O
categories	O
and	O
mappings	O
between	O
them	O
,	O
underlying	O
the	O
MES	O
.	O
First	O
generation	O
networks	O
studied	O
the	O
McCulloch-Pitts	O
neuron	O
,	O
or	O
perceptrons	B-DeepLearning
,	O
using	O
digital	O
inputs	O
and	O
outputs	O
,	O
usually	O
binary	O
.	O
Multi-layer	B-DeepLearning
perceptrons	I-DeepLearning
with	O
a	O
single	O
hidden	B-DeepLearning
layer	I-DeepLearning
were	O
able	O
to	O
compute	O
any	O
binary	O
function	O
.	O
Second	O
generation	O
networks	O
had	O
neurons	O
with	O
activation	B-DeepLearning
functions	I-DeepLearning
,	O
usually	O
some	O
logistic	B-DeepLearning
function	I-DeepLearning
like	O
the	O
sigmoid	B-DeepLearning
,	O
with	O
continuous	O
inputs	O
and	O
outputs	O
,	O
having	O
probabilistic	O
weights	B-DeepLearning
in	O
the	O
synapses	O
,	O
and	O
learning	O
algorithms	O
based	O
on	O
gradient	B-DeepLearning
descent	I-DeepLearning
.	O
Third	O
generation	O
networks	O
are	O
based	O
on	O
spiking	B-DeepLearning
or	I-DeepLearning
pulsating	I-DeepLearning
neurons	I-DeepLearning
,	O
that	O
incorporate	O
recent	O
neurobiology	O
discoveries	O
,	O
modeling	O
the	O
neuron	O
as	O
a	O
dynamic	O
electrical	O
system	O
with	O
bifurcation	O
in	O
its	O
equilibrium	O
,	O
usually	O
following	O
some	O
variant	O
of	O
the	O
Hodgkin-Huxley	O
model	O
.	O
The	O
network	B-DeepLearning
was	I-DeepLearning
trained	I-DeepLearning
with	O
a	O
100-sentence	B-DeepLearning
database	I-DeepLearning
and	O
was	O
generated	O
by	O
a	O
Kohonen	O
network	O
with	O
60	B-DeepLearning
x	I-DeepLearning
60	I-DeepLearning
neurons	I-DeepLearning
and	O
a	O
decreasing	O
neighborhood	O
.	O
It	O
is	O
needed	O
for	O
instance	O
,	O
when	O
channels	O
used	O
in	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
have	O
different	O
lengths	O
or	O
if	O
the	O
duration	O
of	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
simulation	O
is	O
different	O
for	O
some	O
reason	O
.	O
This	O
gives	O
an	O
accuracy	B-DeepLearning
rate	I-DeepLearning
of	O
0627	O
.	O
The	O
accuracy	B-DeepLearning
of	O
the	O
system	O
is	O
calculated	O
as	O
the	O
total	O
number	O
of	O
correctly	O
classified	O
objects	O
1094	O
divided	O
by	O
the	O
total	O
number	O
of	O
objects	O
1627	O
.	O
Neural	B-DeepLearning
networks	I-DeepLearning
are	O
generally	O
better	O
at	O
discriminating	B-DeepLearning
between	I-DeepLearning
classes	I-DeepLearning
,	O
as	O
is	O
shown	O
here	O
.	O
The	O
whole	O
data	B-DeepLearning
set	I-DeepLearning
was	O
divided	O
into	O
different	O
parts	O
in	O
order	O
to	O
get	O
different	O
validation	O
data	B-DeepLearning
sets	I-DeepLearning
.	O
EXP-1	O
,	O
which	O
contains	B-DeepLearning
all	I-DeepLearning
data	I-DeepLearning
from	O
the	O
first	O
clinical	B-DeepLearning
experiment	I-DeepLearning
with	O
2500	O
measurements	O
.	O
Accuracy	B-DeepLearning
is	O
computed	O
as	O
the	O
percentage	O
of	O
cases	O
correctly	O
classified	O
.	O
If	O
yes	O
,	O
with	O
probability	O
0.76	O
this	O
is	O
classified	B-DeepLearning
as	O
a	O
cancerous	O
regions	O
,	O
otherwise	O
,	O
the	O
region	O
is	O
certainly	O
cancerous	O
.	O
Results	O
show	O
that	O
on	O
both	O
data	B-DeepLearning
sets	I-DeepLearning
the	O
certainty	O
for	O
the	O
correctly	O
classified	O
lesions	O
is	O
higher	O
lower	O
entropy	O
than	O
for	O
the	O
incorrectly	O
classified	O
lesions	O
.	O
According	O
to	O
[	O
25	O
]	O
,	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
research	O
evolved	O
into	O
three	O
generations	O
.	O
Under	O
normal	O
circumstances	O
,	O
however	O
,	O
accuracy	B-DeepLearning
is	O
determined	O
by	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
of	O
the	O
inferred	O
rule	O
ie	O
,	O
the	O
percentage	O
of	O
actions	O
incorrectly	O
classified	O
as	O
either	O
legitimate	O
or	O
illegitimate	O
calculated	O
over	O
the	O
entire	O
uniformly-sampled	B-DeepLearning
set	I-DeepLearning
of	O
action	O
possibilities	O
within	O
the	O
a	O
priori	O
motor	O
space	O
.	O
As	O
a	O
result	O
,	O
the	O
data	B-DeepLearning
set	I-DeepLearning
is	O
optimally	O
classified	B-DeepLearning
into	I-DeepLearning
two	I-DeepLearning
classes	I-DeepLearning
,	O
targets	O
and	O
non	O
targets	O
following	O
a	O
validation	O
process	O
,	O
and	O
an	O
average	O
accuracy	B-DeepLearning
metric	I-DeepLearning
is	O
then	O
calculated	O
.	O
This	O
set	B-DeepLearning
was	I-DeepLearning
divided	I-DeepLearning
into	O
2	O
groups:	O
training	O
320	O
thermographies	O
in	O
each	O
class	O
and	O
testing	O
80	O
thermographies	O
in	O
reach	O
class	O
.	O
Therefore	O
,	O
a	O
total	O
of	O
2.411	O
images	B-DeepLearning
of	O
healthy	B-DeepLearning
patients	B-DeepLearning
and	O
534	O
images	B-DeepLearning
of	O
diseased	B-DeepLearning
patients	B-DeepLearning
were	O
used	O
.	O
A	O
balanced	O
randomized	O
selection	O
was	O
performed	O
on	O
500	O
healthy	B-DeepLearning
patients	B-DeepLearning
and	O
500	O
sick	B-DeepLearning
patients	B-DeepLearning
.	O
80%	O
400	O
Healthy	B-DeepLearning
and	O
400	O
Sick	B-DeepLearning
were	O
allocated	O
to	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
,	O
that	O
is	O
,	O
the	O
processes	O
of	O
extracting	O
the	O
attributes	O
and	O
optimizing	O
the	O
weights	B-DeepLearning
of	O
the	O
filters	B-DeepLearning
.	O
20%	O
100	O
thermographies	O
in	O
each	O
class	O
were	O
reserved	B-DeepLearning
for	I-DeepLearning
blind	I-DeepLearning
validation	I-DeepLearning
and	O
establishing	O
the	O
final	O
predictive	O
accuracy	B-DeepLearning
of	O
these	O
architectures	O
using	O
a	O
database	O
of	O
images	B-DeepLearning
that	O
was	O
not	O
used	O
for	O
learning	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
.	O
Data	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
comes	O
from	O
a	O
simulation	O
experiment	B-DeepLearning
.	O
Before	O
giving	O
more	O
in	O
depth	O
description	O
of	O
these	O
data	O
,	O
it	O
is	O
good	O
to	O
acknowledge	O
that	O
the	O
input	B-DeepLearning
of	I-DeepLearning
a	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consists	O
of	O
several	O
cell	O
positions	O
and	O
output	O
is	O
the	O
velocity	O
of	O
that	O
cell	O
at	O
a	O
particular	O
time	O
.	O
Data	B-DeepLearning
for	I-DeepLearning
this	I-DeepLearning
study	I-DeepLearning
were	O
recorded	B-DeepLearning
every	I-DeepLearning
0.001	I-DeepLearning
s	I-DeepLearning
once	O
in	O
1000	O
simulation	B-DeepLearning
steps	I-DeepLearning
,	O
which	O
results	O
in	O
almost	O
9	O
200	O
records	B-DeepLearning
for	O
each	O
cell	O
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
sets	O
used	O
for	O
neural	B-DeepLearning
network	I-DeepLearning
are	O
extracted	O
from	O
simulations	O
which	O
differ	O
only	O
in	O
initial	B-DeepLearning
seeding	I-DeepLearning
of	O
the	O
cells	O
.	O
As	O
was	O
already	O
mentioned	O
,	O
the	O
input	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
are	O
the	O
positions	O
of	O
the	O
cell	O
center	O
and	O
the	O
output	O
is	O
the	O
velocity	O
of	O
this	O
cell	O
at	O
given	O
position	O
.	O
The	O
learning	B-DeepLearning
set	I-DeepLearning
for	I-DeepLearning
our	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consist	O
of	O
the	O
simulation	O
output	O
which	O
also	O
includes	O
redundant	B-DeepLearning
data	I-DeepLearning
.	O
We	O
processed	B-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
in	O
the	O
following	O
steps	O
.	O
We	O
loaded	O
the	O
data	O
from	O
the	O
experiments	B-DeepLearning
and	O
checked	O
their	O
correctness	O
.	O
Next	O
step	O
was	O
the	O
normalization	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
.	O
Normalization	B-DeepLearning
of	I-DeepLearning
data	I-DeepLearning
is	O
common	O
in	O
neural	O
networks.	O
.	O
We	O
made	O
the	O
desired	O
pairs	O
of	O
the	O
inputs	O
and	O
the	O
outputs	O
from	O
the	O
normalized	B-DeepLearning
data	I-DeepLearning
.	O
We	O
work	O
with	O
two	O
types	O
of	O
the	O
input	O
tensor	B-DeepLearning
in	I-DeepLearning
the	I-DeepLearning
form	I-DeepLearning
of	I-DeepLearning
a	I-DeepLearning
3−dimensional	I-DeepLearning
array	I-DeepLearning
.	O
The	O
horizontal	O
layers	O
of	O
the	O
tensor	B-DeepLearning
represent	O
different	O
cells	O
,	O
where	O
the	O
cell	O
we	O
are	O
focused	O
on	O
is	O
on	O
the	O
top	O
of	O
the	O
tensor	B-DeepLearning
.	O
For	O
our	O
tensor	B-DeepLearning
we	O
used	O
zero	O
padding	O
of	O
width	O
p	O
.	O
Thus	O
the	O
dimension	B-DeepLearning
of	I-DeepLearning
this	I-DeepLearning
tensor	I-DeepLearning
is:	O
.	O
This	O
input	O
type	O
for	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
set	O
by	O
parameter	B-DeepLearning
called	O
spatial	B-DeepLearning
tensor	I-DeepLearning
is	O
inspired	O
by	O
the	O
image	B-DeepLearning
processing	O
.	O
We	O
obtain	O
a	O
small	O
tensor	B-DeepLearning
A	O
by	O
determining	O
if	O
the	O
cell	O
is	O
present	O
or	O
not	O
,	O
in	O
each	O
of	O
the	O
disc	O
x	O
×	O
disc	O
y	O
×	O
disc	O
z	O
channel	O
areas	O
.	O
Since	O
this	O
input	B-DeepLearning
is	I-DeepLearning
orthogonal	I-DeepLearning
the	O
neuron	B-DeepLearning
networks	I-DeepLearning
gives	O
us	O
more	O
accurate	O
output	O
values	O
.	O
Each	O
tensor	B-DeepLearning
records	O
the	O
position	O
of	O
the	O
centre	O
of	O
one	O
cell	O
.	O
Then	O
the	O
input	B-DeepLearning
tensor	I-DeepLearning
consists	O
of	O
small	O
tensors	B-DeepLearning
An...A1	O
,	O
Bn...B1	O
in	O
this	O
order	O
.	O
The	O
small	O
tensor	B-DeepLearning
An	O
is	O
at	O
the	O
top	O
horizontal	O
layer	O
of	O
the	O
input	O
tensor	B-DeepLearning
and	O
B1	O
is	O
at	O
the	O
bottom	O
layer	O
of	O
the	O
input	O
tensor	B-DeepLearning
.	O
Therefore	O
the	O
dimension	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
input	I-DeepLearning
tensor	I-DeepLearning
is:	O
.	O
We	O
also	O
used	O
the	O
gaussian	B-DeepLearning
kernel	I-DeepLearning
.	O
Hence	O
,	O
the	O
dimension	O
of	O
the	O
input	B-DeepLearning
tensor	I-DeepLearning
for	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
was	O
16	O
×	O
16	O
×	O
24	O
.	O
Data	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
input	I-DeepLearning
tensors	I-DeepLearning
are	O
selected	O
as	O
follows	O
.	O
Besides	O
using	O
convolution	B-DeepLearning
or	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
,	O
we	O
also	O
used	O
a	O
relatively	O
new	O
type	O
of	O
layers	O
,	O
the	O
dense	B-DeepLearning
convolution	I-DeepLearning
layers	I-DeepLearning
,	O
introduced	O
in	O
[	O
7	O
]	O
.	O
There	O
were	O
6	O
different	O
architectures	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
see	O
the	O
Table	O
2	O
.	O
First	O
four	O
experiments	B-DeepLearning
,	O
named	O
Experiment	B-DeepLearning
0	O
to	O
Experiment	B-DeepLearning
3	O
have	O
no	O
spatial	O
tensor	B-DeepLearning
input	O
with	O
the	O
same	O
parameters	B-DeepLearning
,	O
but	O
mutually	O
different	O
networks	B-DeepLearning
architectures	I-DeepLearning
.	O
The	O
network	B-DeepLearning
architectures	I-DeepLearning
for	O
experiment	B-DeepLearning
pairs	O
2	O
and	O
4	O
and	O
for	O
3	O
and	O
5	O
are	O
the	O
same	O
.	O
Comparing	O
the	O
difference	O
between	O
the	O
target	O
and	O
resulted	O
values	O
given	O
from	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
we	O
observed	O
that	O
networks	O
with	O
spatial	O
tensor	B-DeepLearning
parameter	B-DeepLearning
Experiments	B-DeepLearning
4	O
to	O
7	O
give	O
better	O
results	O
than	O
for	O
no	O
spatial	O
tensor	B-DeepLearning
parameter	B-DeepLearning
.	O
The	O
Experiment	B-DeepLearning
7	O
,	O
where	B-DeepLearning
the	I-DeepLearning
network	I-DeepLearning
has	I-DeepLearning
the	I-DeepLearning
most	I-DeepLearning
layers	I-DeepLearning
gave	I-DeepLearning
us	I-DeepLearning
the	I-DeepLearning
best	I-DeepLearning
result	I-DeepLearning
.	O
In	O
this	O
section	O
we	O
report	O
on	O
a	O
set	O
of	O
experiments	B-DeepLearning
performed	O
on	O
a	O
large	O
data	B-DeepLearning
set	I-DeepLearning
of	O
nanopore	O
FASTQ	O
files	O
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
is	O
comprised	O
of	O
336	O
different	O
files	O
,	O
with	O
sizes	O
ranging	O
from	O
7.2	O
KB	O
to	O
3.5	O
GB	O
,	O
including	O
reads	O
that	O
are	O
up	O
to	O
hundreds	O
of	O
thousands	O
base-pair	O
long	O
.	O
The	O
total	O
size	O
of	O
the	O
data	B-DeepLearning
set	I-DeepLearning
amounts	O
to	O
114.2	O
GB	O
and	O
the	O
dynamic	O
range	O
of	O
quality	O
scores	O
is	O
7	O
bits	O
.	O
Among	O
these	O
methods	O
,	O
the	O
credibility	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
back	I-DeepLearning
propagation	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
is	I-DeepLearning
up	I-DeepLearning
to	I-DeepLearning
98%.	I-DeepLearning
.	O
To	O
address	O
the	O
two	O
issues	O
,	O
in	O
this	O
paper	O
we	O
propose	O
to	O
use	O
a	O
deep	O
ResNet	B-DeepLearning
structure	O
with	O
Convolutional	B-DeepLearning
Block	I-DeepLearning
Attention	I-DeepLearning
Module	I-DeepLearning
CBAM	B-DeepLearning
,	O
in	O
order	O
to	O
extract	O
richer	O
and	O
finer	O
features	O
from	O
pathological	O
images	B-DeepLearning
.	O
Existing	O
deep	B-DeepLearning
learning	I-DeepLearning
approaches	O
for	O
Breast	O
Cancer	O
BC	O
histology	O
images	B-DeepLearning
classification	O
task	O
include	O
cell	O
nuclei	O
segmentation	O
[	O
1415	O
]	O
and	O
the	O
patch-wise	O
classification	O
[	O
1216	O
]	O
.	O
In	O
order	O
to	O
classify	O
the	O
BC	O
pathological	O
images	B-DeepLearning
steadily	O
and	O
accurately	O
,	O
in	O
this	O
paper	O
,	O
we	O
adopt	O
an	O
improved	O
ResNet	B-DeepLearning
architecture	O
to	O
extract	O
local	O
and	O
global	O
features	O
from	O
pathological	O
images	B-DeepLearning
and	O
perform	O
end-to-end	O
training	O
.	O
ResNet	B-DeepLearning
[	O
17	O
]	O
is	O
a	O
recently	O
popular	O
CNN	B-DeepLearning
architecture	O
,	O
it	O
prevents	O
vanishing	O
gradient	O
by	O
using	O
the	O
residual	O
connection	O
,	O
which	O
allows	O
the	O
network	B-DeepLearning
architecture	I-DeepLearning
to	O
be	O
deeper	O
to	O
obtain	O
richer	O
and	O
more	O
abstract	O
features	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
Convolutional	B-DeepLearning
Block	I-DeepLearning
Attention	I-DeepLearning
Module	I-DeepLearning
CBAM	B-DeepLearning
[	O
18	O
]	O
to	O
enhance	O
the	O
performance	O
of	O
the	O
ResNet	B-DeepLearning
.	O
To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
method	O
,	O
we	O
conduct	O
a	O
series	O
of	O
experiments	B-DeepLearning
on	O
the	O
publicly	O
available	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
.	O
The	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
,	O
a	O
benchmark	O
proposed	O
by	O
[	O
11	O
]	O
for	O
the	O
BC	O
histological	O
images	B-DeepLearning
classification	O
,	O
consists	O
of	O
7909	O
breast	O
histopathological	O
images	B-DeepLearning
from	O
82	O
patients	B-DeepLearning
.	O
We	O
use	O
the	O
deeper	O
ResNet	B-DeepLearning
CNN	B-DeepLearning
architecture	O
to	O
extract	O
richer	O
and	O
more	O
abstract	O
features	O
of	O
patients’	B-DeepLearning
BC	O
tissue	O
.	O
We	O
use	O
CABM	B-DeepLearning
to	O
refine	O
the	O
tissue	O
features	O
extracted	O
from	O
each	O
layer	O
of	O
ResNet	B-DeepLearning
,	O
which	O
can	O
improve	O
the	O
classification	O
performance	O
.	O
We	O
have	O
significantly	O
improved	O
the	O
accuracy	O
of	O
classification	O
on	O
the	O
publicly	O
available	O
BreakHis	B-DeepLearning
Dataset	I-DeepLearning
.	O
Fortunately	O
,	O
Breast	B-DeepLearning
cancer	I-DeepLearning
histopathology	I-DeepLearning
database	I-DeepLearning
provided	I-DeepLearning
7909	I-DeepLearning
2480	I-DeepLearning
benign	I-DeepLearning
and	I-DeepLearning
5429	I-DeepLearning
malignant	I-DeepLearning
samples	I-DeepLearning
microscopic	O
images	B-DeepLearning
of	O
breast	O
tumor	O
which	O
collected	O
from	O
82	O
patients	B-DeepLearning
by	O
surgery	O
[	O
11	O
]	O
.	O
AlexNet	B-DeepLearning
is	O
a	O
typical	O
deep	B-DeepLearning
learning	I-DeepLearning
model	I-DeepLearning
,	O
which	O
[	O
22	O
]	O
achieves	O
a	O
winning	O
top-5	O
test	B-DeepLearning
error	I-DeepLearning
rate	I-DeepLearning
of	O
15.3%	O
,	O
which	O
is	O
10.9%	O
lower	O
than	O
the	O
second	O
one	O
who	O
uses	O
SIFT	O
[	O
23	O
]	O
and	O
FVS	O
[	O
24	O
]	O
.	O
The	O
Breast	O
Cancer	O
Histopathological	O
Image	B-DeepLearning
Classification	O
BreakHis	O
dataset	B-DeepLearning
composes	O
of	O
7909	O
microscopic	O
images	B-DeepLearning
of	O
breast	O
tumor	O
tissue	O
.	O
They	O
are	O
collected	O
from	O
82	O
patients	B-DeepLearning
in	O
different	O
magnifying	O
factors	O
include	O
40×	O
,	O
100×	O
,	O
200×	O
,	O
and	O
400×	O
.	O
The	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
contains	O
2480	B-DeepLearning
benign	I-DeepLearning
and	I-DeepLearning
5429	I-DeepLearning
malignant	I-DeepLearning
samples	I-DeepLearning
700	O
×	O
460	O
pixels	O
,	O
3-channel	O
RGB	O
,	O
8-bit	O
depth	O
in	O
each	O
channel	O
,	O
PNG	O
format	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
the	O
ResNet	B-DeepLearning
with	I-DeepLearning
50	I-DeepLearning
layers	I-DeepLearning
,	O
which	O
can	O
be	O
represented	O
by	O
ResNet-50	B-DeepLearning
.	O
Then	O
we	O
add	O
CBAM	B-DeepLearning
to	O
each	O
block	O
of	O
the	O
ResNet-50	B-DeepLearning
.	O
The	O
original	O
top	O
layer	O
of	O
ResNet-50	B-DeepLearning
is	O
replaced	O
by	O
a	O
global	O
average-pooling	B-DeepLearning
layer	I-DeepLearning
.	O
A	O
dropout	B-DeepLearning
of	I-DeepLearning
0.3	I-DeepLearning
is	O
also	O
used	O
after	O
the	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
for	O
helping	O
reduce	O
overfitting	B-DeepLearning
.	O
The	O
loss	B-DeepLearning
is	I-DeepLearning
categorical	I-DeepLearning
cross-entropy	I-DeepLearning
and	O
the	O
optimizer	B-DeepLearning
is	O
Stochastic	B-DeepLearning
Gradient	I-DeepLearning
Descent	I-DeepLearning
SGD	B-DeepLearning
.	O
We	O
set	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
at	O
0001	O
.	O
We	O
pre-train	B-DeepLearning
our	O
model	B-DeepLearning
on	O
the	O
ImageNet	B-DeepLearning
dataset	I-DeepLearning
,	O
and	O
then	O
fine-tune	O
our	O
model	B-DeepLearning
on	O
the	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
.	O
We	O
retrospectively	O
evaluate	O
our	O
model	B-DeepLearning
on	O
the	O
data	O
of	O
142	O
patients	B-DeepLearning
with	O
305	O
lesions	O
and	O
70	O
no-lesion	O
volumes	O
,	O
and	O
it	O
significantly	O
outperforms	O
the	O
comparison	O
methods	O
with	O
the	O
sensitivity	B-DeepLearning
of	O
92.1%	O
with	O
1.92	O
false	O
positives	O
per	O
volume	O
.	O
The	O
model	B-DeepLearning
is	O
based	O
on	O
the	O
standard	O
UNet	B-DeepLearning
segmentation	I-DeepLearning
architecture	I-DeepLearning
with	O
a	O
down-sampling	O
and	O
upsampling	O
path	O
,	O
where	O
all	O
the	O
simply	O
stacked	O
convolutional	B-DeepLearning
layers	I-DeepLearning
are	O
replaced	O
with	O
residual	B-DeepLearning
blocks	I-DeepLearning
to	O
prevent	O
gradient	B-DeepLearning
vanishing	I-DeepLearning
.	O
We	O
combine	O
spatial	O
information	O
into	O
different	O
layers	O
of	O
UNet	B-DeepLearning
to	O
reduce	O
the	O
false	O
positive	O
rate	O
.	O
Finally	O
,	O
considering	O
the	O
blurred	O
boundaries	O
between	O
lesion	O
and	O
no-lesions	O
,	O
we	O
adopt	O
the	O
sub-hard	O
mining	O
strategy	O
in	O
loss	B-DeepLearning
function	I-DeepLearning
,	O
which	O
only	O
computes	O
the	O
loss	B-DeepLearning
of	O
the	O
specific	O
samples	O
to	O
update	O
the	O
parameter	B-DeepLearning
of	O
the	O
network	O
,	O
in	O
order	O
to	O
ensure	O
the	O
network	O
learn	O
correct	O
information	O
.	O
To	O
validate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
framework	O
,	O
we	O
conduct	O
extensive	O
experiments	B-DeepLearning
on	O
an	O
ABUS	B-DeepLearning
dataset	I-DeepLearning
of	O
142	O
patients’	B-DeepLearning
images	B-DeepLearning
with	O
375	O
volumes	O
.	O
We	O
introduce	O
the	O
spatial	B-DeepLearning
information	I-DeepLearning
into	I-DeepLearning
different	I-DeepLearning
layers	I-DeepLearning
of	O
the	O
segmentation	O
network	O
,	O
which	O
significantly	O
reduce	B-DeepLearning
the	I-DeepLearning
false	I-DeepLearning
positive	I-DeepLearning
rate	I-DeepLearning
.	O
Our	O
method	O
is	O
based	O
on	O
UNet	B-DeepLearning
.	O
The	O
image	B-DeepLearning
resolution	O
reduces	O
by	O
half	O
after	O
max	B-DeepLearning
pooling	I-DeepLearning
along	O
the	O
downsampling	B-DeepLearning
path	I-DeepLearning
,	O
while	O
the	O
resolution	O
doubles	O
via	O
deconvolution	B-DeepLearning
operation	I-DeepLearning
along	O
the	O
upsampling	B-DeepLearning
path	I-DeepLearning
.	O
Finally	O
,	O
a	O
softmax	B-DeepLearning
layer	I-DeepLearning
is	O
used	O
to	O
transform	O
the	O
result	O
into	O
a	O
two-class	O
problem	O
.	O
We	O
improve	O
UNet	B-DeepLearning
by	O
replacing	O
the	O
simply	O
stacked	O
convolutional	B-DeepLearning
layers	I-DeepLearning
with	O
residual	B-DeepLearning
blocks	I-DeepLearning
to	O
prevent	O
vanishing	B-DeepLearning
gradient	I-DeepLearning
.	O
Furthermore	O
,	O
the	O
attention	B-DeepLearning
skip	I-DeepLearning
connection	I-DeepLearning
is	O
included	O
to	O
make	O
the	O
model	B-DeepLearning
pay	O
more	O
attention	O
to	O
useful	O
spatial	O
areas	O
and	O
improve	O
the	O
ability	O
of	O
localization	O
.	O
As	O
is	O
shown	O
in	O
Fig.	O
3	O
,	O
it	O
contains	O
two	O
3	B-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
with	O
stride	B-DeepLearning
1	O
,	O
and	O
the	O
same	B-DeepLearning
padding	I-DeepLearning
is	O
used	O
to	O
ensure	O
the	O
output	O
has	O
the	O
same	O
size	O
with	O
the	O
input	O
.	O
The	O
reception	O
field	O
of	O
two	O
successive	O
3	B-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
is	O
the	O
same	O
with	O
that	O
of	O
a	O
5×5×5	B-DeepLearning
convolutional	I-DeepLearning
layer	I-DeepLearning
but	O
with	O
fewer	O
parameters	B-DeepLearning
to	O
be	O
computed	O
.	O
Since	O
the	O
extreme	O
imbalance	O
between	O
normal	O
regions	O
and	O
lesion	O
regions	O
,	O
the	O
model	B-DeepLearning
tends	I-DeepLearning
to	I-DeepLearning
predict	I-DeepLearning
most	O
areas	O
as	O
normal	O
regions	O
.	O
To	O
normally	O
represent	O
the	O
actual	O
shape	O
and	O
size	O
of	O
lesions	O
,	O
we	O
firstly	O
normalize	B-DeepLearning
the	O
pixel	O
spacing	O
of	O
the	O
ABUS	O
volume	O
in	O
each	O
direction	O
.	O
Our	O
proposed	O
framework	O
is	O
implemented	O
with	O
PyTorch	B-DeepLearning
and	O
trained	O
on	O
NVIDIA	O
Tesla	O
M40	O
GPU	O
.	O
During	O
training	O
,	O
in	O
order	O
to	O
alleviate	O
the	O
computational	O
complexity	O
in	O
each	O
batch	O
and	O
increase	O
the	O
augmentation	O
,	O
we	O
randomly	O
crop	O
the	O
ABUS	O
images	B-DeepLearning
into	O
small	O
patches	B-DeepLearning
with	I-DeepLearning
the	I-DeepLearning
size	I-DeepLearning
of	I-DeepLearning
160	I-DeepLearning
×	I-DeepLearning
80	I-DeepLearning
×	I-DeepLearning
160	I-DeepLearning
,	O
70%	O
of	O
which	O
are	O
lesion-centered	O
.	O
In	O
addition	O
,	O
we	O
adopt	O
various	O
data	B-DeepLearning
augmentations	I-DeepLearning
,	O
e.g.	O
,	O
contrast	O
adjustment	O
,	O
rotation	O
,	O
cropping	O
,	O
flipping	O
,	O
especially	O
the	O
elastic	O
transform	O
[	O
26	O
]	O
,	O
which	O
are	O
widely	O
used	O
in	O
small	O
medical	O
image	B-DeepLearning
dataset	I-DeepLearning
to	O
increase	O
the	O
data	O
diversity	O
.	O
Adam	B-DeepLearning
optimizer	I-DeepLearning
is	O
used	O
to	O
train	O
the	O
whole	O
network	O
,	O
and	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
is	O
set	O
as	O
1e-4	O
,	O
and	O
training	O
is	O
stopped	O
when	O
the	O
model	B-DeepLearning
has	O
similar	O
performance	O
on	O
training	O
and	O
validation	B-DeepLearning
dataset	I-DeepLearning
.	O
Then	O
the	O
skip	O
attention	O
is	O
used	O
to	O
identify	O
the	O
image	B-DeepLearning
regions	O
and	O
prune	O
the	O
irrelevant	O
feature	O
responses	O
so	O
as	O
to	O
preserve	O
only	O
the	O
activation	O
relevant	O
to	O
this	O
segmentation	O
task	O
.	O
The	O
architecture	O
of	O
AssCNV23’s	O
network	O
contains	O
four	O
convolutional	B-DeepLearning
layers	I-DeepLearning
intersected	O
with	O
three	O
max	B-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
,	O
three	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
.	O
The	O
detected	O
results	O
were	O
compared	O
to	O
the	O
benchmark	O
data	O
,	O
and	O
the	O
performance	O
of	O
each	O
tool	O
was	O
evaluated	O
by	O
Precision	B-DeepLearning
Pre	O
,	O
Sensitivity	B-DeepLearning
Sen	O
,	O
F1-score	B-DeepLearning
F1	O
and	O
Matthews	B-DeepLearning
correlation	I-DeepLearning
coefficient	I-DeepLearning
MCC	B-DeepLearning
.	O
They	O
used	O
Gold-standard	B-DeepLearning
dataset	I-DeepLearning
,	O
SCOP	O
version	O
1.75	O
and	O
6SSE	O
,	O
5SSE	O
,	O
4SSE	O
and	O
3SSE	O
[	O
10	O
]	O
.	O
Indeed	O
,	O
this	O
work	O
will	O
focus	O
on	O
the	O
development	O
of	O
a	O
diagnosis	B-DeepLearning
support	I-DeepLearning
system	I-DeepLearning
,	O
in	O
terms	O
of	O
its	O
knowledge	O
representation	O
and	O
reasoning	O
procedures	O
,	O
under	O
a	O
formal	O
framework	O
based	O
on	O
Logic	O
Programming	O
,	O
complemented	O
with	O
an	O
approach	O
to	O
computing	O
centered	O
on	O
Artificial	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
,	O
to	O
evaluate	O
ACS	O
predisposing	O
and	O
the	O
respective	O
Degree-of-Confidence	O
that	O
one	O
has	O
on	O
such	O
a	O
happening	O
.	O
In	O
this	O
study	O
627	O
patients	B-DeepLearning
admitted	O
on	O
the	O
emergency	O
department	O
were	O
considered	O
,	O
during	O
the	O
period	O
of	O
three	O
months	O
from	O
February	O
to	O
May	O
of	O
2013	O
,	O
with	O
suspect	O
of	O
ACS	O
.	O
The	O
gender	B-DeepLearning
distribution	I-DeepLearning
was	O
48%	B-DeepLearning
and	I-DeepLearning
52%	I-DeepLearning
for	I-DeepLearning
female	I-DeepLearning
and	I-DeepLearning
male	I-DeepLearning
,	O
respectively	O
.	O
Seventeen	O
variables	O
were	O
selected	O
allowing	O
one	O
to	O
have	O
a	O
multivariable	O
dataset	B-DeepLearning
with	O
627	O
records	O
.	O
To	O
ensure	O
statistical	O
significance	O
of	O
the	O
attained	O
results	O
,	O
20	B-DeepLearning
twenty	I-DeepLearning
experiments	I-DeepLearning
were	O
applied	O
in	O
all	O
tests	O
.	O
In	O
each	O
simulation	O
,	O
the	O
available	O
data	O
was	O
randomly	O
divided	O
into	O
two	O
mutually	O
exclusive	O
partitions	O
,	O
i.e.	O
,	O
the	O
training	B-DeepLearning
set	I-DeepLearning
with	O
67%	O
of	O
the	O
available	O
data	O
and	O
,	O
the	O
test	B-DeepLearning
set	I-DeepLearning
with	O
the	O
remaining	O
33%	O
of	O
the	O
cases	O
.	O
In	O
the	O
other	O
layers	O
we	O
used	O
the	O
sigmoid	B-DeepLearning
function	O
.	O
As	O
the	O
output	B-DeepLearning
function	I-DeepLearning
in	O
the	O
preprocessing	O
layer	O
it	O
was	O
used	O
the	O
identity	B-DeepLearning
one	O
.	O
Table	O
2	O
shows	O
that	O
the	O
model	B-DeepLearning
accuracy	B-DeepLearning
was	O
96.9%	O
for	O
the	O
training	B-DeepLearning
set	I-DeepLearning
410	O
correctly	O
classified	O
in	O
423	O
and	O
94.6%	O
for	O
test	O
set	O
193	O
correctly	O
classified	O
in	O
204	O
.	O
Thus	O
,	O
the	O
predictions	O
made	O
by	O
the	O
ANN	B-DeepLearning
model	I-DeepLearning
are	O
satisfactory	O
,	O
attaining	O
accuracies	B-DeepLearning
close	I-DeepLearning
to	I-DeepLearning
95%	I-DeepLearning
.	O
Huang	O
and	O
Liao	O
[	O
45	O
]	O
introduced	O
a	O
comprehensive	O
study	O
to	O
investigate	O
the	O
capability	O
of	O
the	O
probabilistic	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
PNN	B-DeepLearning
associated	O
with	O
a	O
feature	O
selection	O
method	O
,	O
the	O
so-called	O
signal-to-noise	O
statistic	O
,	O
in	O
cancer	B-DeepLearning
classification	I-DeepLearning
.	O
As	O
an	O
illustrated	O
in	O
Figure	O
1.6a	O
,	O
the	O
entire	O
data-set	O
of	O
all	O
88	O
experiments	B-DeepLearning
was	O
first	O
quality	O
filtered	O
1	O
and	O
then	O
the	O
dimensionality	B-DeepLearning
was	I-DeepLearning
further	I-DeepLearning
reduced	I-DeepLearning
by	O
principal	B-DeepLearning
component	I-DeepLearning
analysis	I-DeepLearning
PCA	B-DeepLearning
to	O
10	B-DeepLearning
PC	I-DeepLearning
projections	I-DeepLearning
2	O
,	O
from	O
the	O
original	O
6567	O
expression	O
values	O
.	O
Next	O
,	O
the	O
25	O
test	B-DeepLearning
experiments	I-DeepLearning
were	O
set	O
aside	O
and	O
the	O
63	O
training	B-DeepLearning
experiments	I-DeepLearning
were	O
randomly	O
partitioned	O
into	O
3	O
groups	O
3	O
.	O
ANN	B-DeepLearning
models	I-DeepLearning
were	O
then	O
calibrated	O
using	O
for	O
each	O
sample	O
the	O
10	B-DeepLearning
PC	I-DeepLearning
values	I-DeepLearning
as	I-DeepLearning
input	I-DeepLearning
and	O
the	O
cancer	B-DeepLearning
category	I-DeepLearning
as	I-DeepLearning
output	I-DeepLearning
5	O
.	O
For	O
each	O
model	B-DeepLearning
,	O
the	O
calibration	O
was	O
optimized	O
with	O
100	O
iterative	O
cycles	O
epochs	B-DeepLearning
.	O
In	O
addition	O
,	O
there	O
was	O
no	O
sign	O
of	O
over-training:	B-DeepLearning
if	O
the	O
models	B-DeepLearning
begin	O
to	O
learn	O
features	O
in	O
the	O
training	B-DeepLearning
set	I-DeepLearning
,	O
which	O
are	O
not	O
present	O
in	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
this	O
would	O
result	O
in	O
an	O
increase	O
in	O
the	O
error	O
for	O
the	O
validation	O
at	O
that	O
point	O
and	O
the	O
curves	O
would	O
no	O
longer	O
remain	O
parallel	O
.	O
A	O
standard	O
approach	O
to	O
neural	B-DeepLearning
network	I-DeepLearning
training	O
is	O
the	O
use	O
of	O
backpropagation	B-DeepLearning
to	O
optimize	O
the	O
weight	B-DeepLearning
assignments	I-DeepLearning
for	O
a	O
fixed	O
neural	O
network	O
topology	O
.	O
Gene	O
selection	O
using	O
Feed	B-DeepLearning
Forward	I-DeepLearning
Back	I-DeepLearning
Propagation	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
as	O
a	O
classifier	O
is	O
illustrated	O
in	O
Figure	O
17	O
.	O
The	O
new	O
technique	O
is	O
based	O
on	O
Switching	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
SNN	B-DeepLearning
,	O
learning	O
machines	O
that	O
assign	O
a	O
relevance	O
value	O
to	O
each	O
input	O
variable	O
,	O
and	O
adopts	O
Recursive	O
Feature	O
Addition	O
RFA	O
for	O
performing	O
gene	O
selection	O
.	O
After	O
that	O
,	O
authors	O
classify	O
the	O
microarray	O
data	B-DeepLearning
sets	I-DeepLearning
with	O
a	O
Fuzzy	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
FNN	B-DeepLearning
.	O
Recently	O
,	O
Marylyn	O
et	O
al.	O
[	O
74	O
]	O
took	O
a	O
serious	O
attempt	O
to	O
introduce	O
a	O
genetic	B-DeepLearning
programming	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
GPNN	B-DeepLearning
as	O
a	O
method	O
for	O
optimizing	O
the	O
architecture	O
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
to	O
improve	O
the	O
identification	O
of	O
genetic	O
and	O
gene-environment	O
combinations	O
associated	O
with	O
a	O
disease	O
risk	O
.	O
This	O
empirical	O
studies	O
suggest	O
GPNN	B-DeepLearning
has	O
excellent	O
power	O
for	O
identifying	O
gene-gene	O
and	O
gene-environment	O
interactions	O
.	O
Using	O
simulated	O
dataauthors	O
show	O
that	O
GPNN	B-DeepLearning
has	O
higher	O
power	O
to	O
identify	O
gene-gene	O
and	O
gene-environment	O
interactions	O
than	O
SLR	O
and	O
CART	O
.	O
These	O
include	O
an	O
independent	O
variable	O
input	O
set	O
,	O
a	O
list	O
of	O
mathematical	O
functions	O
,	O
a	O
fitness	B-DeepLearning
function	I-DeepLearning
,	O
and	O
finally	O
the	O
operating	O
parameters	B-DeepLearning
of	O
the	O
GP	O
.	O
These	O
operating	O
parameters	B-DeepLearning
include	O
number	B-DeepLearning
of	I-DeepLearning
demes	I-DeepLearning
or	I-DeepLearning
populations	I-DeepLearning
,	O
population	B-DeepLearning
size	I-DeepLearning
,	O
number	B-DeepLearning
of	I-DeepLearning
generations	I-DeepLearning
,	O
reproduction	B-DeepLearning
rate	I-DeepLearning
,	O
crossover	B-DeepLearning
rate	I-DeepLearning
,	O
mutation	B-DeepLearning
rate	I-DeepLearning
,	O
and	O
migration	B-DeepLearning
[	O
90	O
]	O
.	O
Here	O
,	O
we	O
will	O
train	B-DeepLearning
the	I-DeepLearning
GPNN	I-DeepLearning
on	I-DeepLearning
9/10	I-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
to	O
develop	O
an	O
NN	B-DeepLearning
model	I-DeepLearning
.	O
Another	O
work	O
introduced	O
by	O
Alison	O
et	O
al	O
[	O
74	O
]	O
which	O
developed	O
a	O
grammatical	B-DeepLearning
evolution	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
GENN	B-DeepLearning
approach	O
that	O
accounts	O
for	O
the	O
drawbacks	O
of	O
GPNN	B-DeepLearning
.	O
They	O
also	O
,	O
compare	O
the	O
performance	O
of	O
GENN	B-DeepLearning
to	O
GPNN	B-DeepLearning
,	O
a	O
traditional	O
Back-Propagation	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
BPNN	B-DeepLearning
and	O
a	O
random	B-DeepLearning
search	I-DeepLearning
algorithm	O
.	O
GENN	B-DeepLearning
outperforms	O
both	O
BPNN	B-DeepLearning
and	O
the	O
random	B-DeepLearning
search	I-DeepLearning
,	O
and	O
performs	O
at	O
least	O
as	O
well	O
as	O
GPNN	B-DeepLearning
.	O
Liang	O
and	O
Kelemen	O
[	O
60	O
]	O
proposed	O
a	O
Time	B-DeepLearning
Lagged	I-DeepLearning
Recurrent	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
with	O
trajectory	O
learning	O
for	O
identifying	O
and	O
classifying	O
the	O
gene	O
functional	O
patterns	O
from	O
the	O
heterogeneous	O
nonlinear	O
time	O
series	O
microarray	O
experiments	B-DeepLearning
.	O
We	O
used	O
a	O
fuzzy	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
FNN	B-DeepLearning
proposed	O
earlier	O
for	O
cancer	B-DeepLearning
classification	I-DeepLearning
.	O
In	O
this	O
work	O
we	O
used	O
three	O
well-known	O
microarray	B-DeepLearning
databases	I-DeepLearning
,	O
i.e.	O
,	O
the	O
lymphoma	B-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
,	O
the	O
small	B-DeepLearning
round	I-DeepLearning
blue	I-DeepLearning
cell	I-DeepLearning
tumor	I-DeepLearning
SRBCT	I-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
,	O
and	O
the	O
ovarian	B-DeepLearning
cancer	I-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
.	O
Our	O
result	O
shows	O
the	O
FNN	B-DeepLearning
classifier	O
not	O
only	O
improves	O
the	O
accuracy	O
of	O
cancer	O
classification	O
problem	O
but	O
also	O
helps	O
biologists	O
to	O
find	O
a	O
better	O
relationship	O
between	O
important	O
genes	O
and	O
development	O
of	O
cancers	O
.	O
An	O
adaptive	B-DeepLearning
learning	I-DeepLearning
rate	I-DeepLearning
provides	O
the	O
network	O
with	O
higher	O
convergence	B-DeepLearning
speed	O
and	O
learning	O
performance	O
,	O
i.e.	O
,	O
accuracy	O
.	O
Here	O
we	O
use	O
a	O
heuristic	O
for	O
tuning	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
η	I-DeepLearning
.	O
If	O
the	O
error	O
undergoes	O
five	O
consecutive	O
reductions	O
,	O
increase	B-DeepLearning
η	I-DeepLearning
by	I-DeepLearning
5%	I-DeepLearning
.	O
If	O
the	O
error	O
undergoes	O
three	O
consecutive	O
combinations	O
of	O
one	O
increase	O
and	O
one	O
reduction	O
,	O
decrease	B-DeepLearning
η	I-DeepLearning
by	I-DeepLearning
5%	I-DeepLearning
.	O
Since	O
we	O
dynamically	O
update	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
,	O
initial	O
value	O
for	O
η	B-DeepLearning
becomes	O
insignificant	O
as	O
long	O
as	O
it	O
is	O
not	O
too	O
large	O
.	O
For	O
the	O
SRBCT	O
data	O
,	O
the	O
FNN	B-DeepLearning
needs	O
only	O
8	O
genes	O
to	O
obtain	O
the	O
same	O
accuracy	O
,	O
i.e.	O
,	O
100%	O
,	O
while	O
the	O
well-known	O
evolutionary	O
algorithm	O
reported	O
by	O
Deutsch	O
[	O
8	O
]	O
used	O
12	O
genes	O
.	O
The	O
network	B-DeepLearning
configuration	I-DeepLearning
,	O
as	O
shown	O
in	O
Figure	O
4	O
,	O
is	O
composed	B-DeepLearning
of	I-DeepLearning
two	I-DeepLearning
layers	I-DeepLearning
one	B-DeepLearning
hidden	I-DeepLearning
and	I-DeepLearning
one	I-DeepLearning
output	I-DeepLearning
.	O
The	O
hidden	B-DeepLearning
layer	I-DeepLearning
consistes	O
on	O
five	B-DeepLearning
neurons	I-DeepLearning
,	O
while	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
has	O
only	O
one	B-DeepLearning
neuron	I-DeepLearning
.	O
Sigmoid	B-DeepLearning
type	O
of	O
neurons	O
have	O
been	O
employed	O
in	O
both	O
layers	O
,	O
the	O
hidden	O
and	O
the	O
output	O
layers	O
.	O
The	O
algorithm	O
used	O
for	O
training	O
process	O
was	O
the	O
Scaled	B-DeepLearning
Conjugate	I-DeepLearning
Gradient	I-DeepLearning
and	O
performance	O
is	O
optimized	O
by	O
the	O
function	O
Cross-Entropy	B-DeepLearning
.	O
In	O
this	O
paper	O
we	O
describe	O
use	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
for	I-DeepLearning
ECG	I-DeepLearning
signal	I-DeepLearning
prediction	I-DeepLearning
.	O
A	O
training	O
signal	O
figure	O
4	O
,	O
red	O
part	O
consists	O
of	O
620	B-DeepLearning
samples	I-DeepLearning
.	O
The	O
test	O
signal	O
is	O
also	O
10	O
second	O
long	O
1000	B-DeepLearning
samples	I-DeepLearning
.	O
We	O
used	O
the	O
log-sigmoid	B-DeepLearning
transfer	O
function	O
logsig	B-DeepLearning
in	O
each	O
layer	O
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
was	O
set	O
to	O
value	O
0.05	O
and	O
the	O
tolerable	O
error	O
to	O
510-5	O
.	O
The	O
said	O
algorithm	O
is	O
based	O
on	O
Levenberg-Marquardt	B-DeepLearning
optimization	O
.	O
In	O
this	O
study	O
a	O
novel	O
neural	B-DeepLearning
network-based	I-DeepLearning
method	O
for	O
peptide	B-DeepLearning
identification	I-DeepLearning
is	O
proposed	O
.	O
The	O
presented	O
here	O
Qual	O
method	O
uses	O
for	O
classification	O
purposes	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
,	O
feed-forward	B-DeepLearning
multilayer	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
[	O
6	O
]	O
with	O
6	B-DeepLearning
input	I-DeepLearning
nodes	I-DeepLearning
corresponding	O
to	O
the	O
spectral	O
features	O
,	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
,	O
and	O
one	B-DeepLearning
output	I-DeepLearning
node	I-DeepLearning
.	O
The	O
number	O
of	O
nodes	O
in	O
the	O
hidden	O
layer	O
was	O
determined	O
by	O
cross-validation	O
during	O
training	O
with	O
the	O
back-propagation	B-DeepLearning
Leveberg-Marquadt	I-DeepLearning
algorithm	O
.	O
The	O
final	O
neural	B-DeepLearning
network	I-DeepLearning
model	I-DeepLearning
included	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
with	I-DeepLearning
8	I-DeepLearning
nodes	I-DeepLearning
.	O
The	O
training	B-DeepLearning
set	I-DeepLearning
consisted	O
of	O
184	O
653	O
tandem	O
spectra	O
acquired	O
with	O
an	O
high	O
resolution	O
LTQ–	O
FTICR	O
mass	O
spectrometer	O
and	O
searched	O
against	O
a	O
target/decoy	O
database	O
with	O
the	O
X!Tandem	O
engine	O
[	O
3	O
]	O
.	O
The	O
expected	O
neural	B-DeepLearning
network	I-DeepLearning
output	I-DeepLearning
values	I-DeepLearning
for	O
positive	O
and	O
negative	O
examples	O
were	O
equal	O
to	O
1	O
and	O
0	O
,	O
respectively	O
.	O
To	O
recognize	O
the	O
performed	O
activity	O
the	O
backpropagation	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
with	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
was	O
used	O
.	O
We	O
conduct	O
experiments	B-DeepLearning
on	O
the	O
newly	O
proposed	O
COSMIC	B-DeepLearning
CNA	I-DeepLearning
dataset	I-DeepLearning
,	O
which	O
contains	O
25	O
types	O
of	O
cancer	O
.	O
We	O
obtained	O
4731	O
corn	O
seed	O
RGB	B-DeepLearning
images	I-DeepLearning
consisting	O
of	O
952	O
haploid	O
and	O
3779	O
diploid	O
seeds	O
from	O
several	O
different	O
proprietary	O
maize	O
inbred	O
lines	O
.	O
We	O
train	O
our	O
network	O
using	O
the	O
image	B-DeepLearning
dataset	I-DeepLearning
that	O
was	O
randomly	O
split	O
into	O
4021	O
training	O
809	O
haploid	O
and	O
3212	O
diploid	O
seeds	O
and	O
710	O
test	O
143	O
haploids	O
and	O
567	O
diploids	O
images	B-DeepLearning
with	O
20%	O
haploids	O
in	O
both	O
sets	O
.	O
Input	O
images	B-DeepLearning
of	O
the	O
corn	O
seeds	O
are	O
convolved	O
with	O
16	O
filter	B-DeepLearning
kernels	I-DeepLearning
in	O
each	O
convolutional	B-DeepLearning
layer	I-DeepLearning
,	O
followed	O
by	O
two	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
and	O
an	O
output	O
layer	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
Convolutional	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
CNN	B-DeepLearning
as	O
a	O
tool	O
to	O
improve	O
efficiency	O
and	O
accuracy	B-DeepLearning
of	O
Osteosarcoma	O
tumor	O
classification	O
into	O
tumor	O
classes	O
viable	O
tumor	O
,	O
necrosis	O
vs	O
non-tumor	O
.	O
The	O
proposed	O
CNN	B-DeepLearning
architecture	O
contains	O
five	B-DeepLearning
learned	I-DeepLearning
layers:	I-DeepLearning
three	B-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
interspersed	O
with	O
max	B-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
for	O
feature	O
extraction	O
and	O
two	B-DeepLearning
fully-connected	I-DeepLearning
layers	I-DeepLearning
with	O
data	B-DeepLearning
augmentation	I-DeepLearning
strategies	O
to	O
boost	O
performance	O
.	O
The	O
convolution	B-DeepLearning
filters	I-DeepLearning
are	O
applied	O
to	O
small	O
patches	O
of	O
the	O
input	O
image	B-DeepLearning
to	O
detect	O
and	O
extract	O
image	B-DeepLearning
features	O
.	O
Our	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
combines	O
features	O
of	O
AlexNet	B-DeepLearning
[	O
9	O
]	O
and	O
LeNet	B-DeepLearning
[	O
10	O
]	O
to	O
develop	O
a	O
fast	O
and	O
accurate	O
slide	O
classification	O
system	O
.	O
The	O
goal	O
of	O
this	O
paper	O
is	O
to	O
utilize	O
CNN	B-DeepLearning
to	O
identify	O
the	O
four	O
regions	O
of	O
interest	O
Fig.	O
2	O
,	O
namely	O
,	O
1	O
Viable	O
tumor	O
,	O
2	O
Coagulative	O
necrosis	O
,	O
3	O
fibrosis	O
or	O
osteoid	O
,	O
and	O
4	O
Non	O
tumor	O
Bone	O
,	O
cartilage	O
.	O
Spanhol	O
et	O
al.	O
[	O
15	O
]	O
developed	O
on	O
existing	O
AlexNet	B-DeepLearning
for	O
different	O
segmentation	O
and	O
classification	O
tasks	O
in	O
breast	O
cancer	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
deep	B-DeepLearning
learning	I-DeepLearning
approach	O
capable	O
of	O
assigning	O
tumor	O
classes	O
viable	O
tumor	O
,	O
necrosis	O
vs	O
non-tumor	O
directly	O
to	O
input	O
slides	O
in	O
osteosarcoma	O
,	O
a	O
type	O
of	O
cancer	O
with	O
significantly	O
more	O
variability	O
in	O
tumor	O
description	O
.	O
We	O
extend	O
the	O
successful	O
Alexnet	B-DeepLearning
proposed	O
by	O
Krizhevsky	O
see	O
[	O
9	O
]	O
and	O
LeNet	B-DeepLearning
network	B-DeepLearning
architectures	I-DeepLearning
introduced	O
by	O
LeCun	O
see	O
[	O
10	O
]	O
which	O
uses	O
gradient	B-DeepLearning
based	I-DeepLearning
learning	I-DeepLearning
with	O
back	B-DeepLearning
propogation	I-DeepLearning
algorithm	O
.	O
The	O
typical	O
CNN	B-DeepLearning
architecture	O
for	O
image	B-DeepLearning
classification	O
consists	O
of	O
a	O
series	O
of	O
convolution	B-DeepLearning
filters	I-DeepLearning
paired	O
with	O
pooling	B-DeepLearning
layers	I-DeepLearning
.	O
We	O
develop	O
on	O
existing	O
proven	O
networks	O
LeNet	B-DeepLearning
and	O
AlexNet	B-DeepLearning
because	O
finding	O
a	O
successful	O
network	O
configuration	O
for	O
a	O
given	O
problem	O
can	O
be	O
a	O
difficult	O
challenge	O
given	O
the	O
total	O
number	O
of	O
possible	O
configurations	O
that	O
can	O
be	O
defined	O
.	O
The	O
data	B-DeepLearning
augmentation	I-DeepLearning
methods	O
to	O
reduce	O
over-fitting	B-DeepLearning
on	O
image	B-DeepLearning
data	O
as	O
described	O
by	O
Krizhevsky	O
[	O
9	O
]	O
has	O
been	O
proclaimed	O
for	O
its	O
success	O
rate	O
in	O
various	O
object	O
recognition	O
applications	O
.	O
We	O
start	O
with	O
a	O
simple	O
3	B-DeepLearning
layer	I-DeepLearning
network	I-DeepLearning
INPUT	B-DeepLearning
-	I-DeepLearning
CONVOLUTION	I-DeepLearning
-	I-DeepLearning
MAX	I-DeepLearning
POOL	I-DeepLearning
-	I-DeepLearning
MLP	I-DeepLearning
.	O
INPUT	B-DeepLearning
128	I-DeepLearning
×	I-DeepLearning
128	I-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
will	O
hold	O
the	O
raw	O
pixel	O
values	O
of	O
the	O
image	B-DeepLearning
,	O
i.e.	O
an	O
image	B-DeepLearning
of	O
width	O
128	O
,	O
height	O
128	O
,	O
and	O
with	O
three	O
color	O
channels	O
RGB	O
.	O
This	O
may	O
result	O
in	O
volume	O
such	O
as	O
124	B-DeepLearning
×	I-DeepLearning
124	I-DeepLearning
×	I-DeepLearning
4	I-DeepLearning
for	O
4	O
filters	B-DeepLearning
.	O
MAX	B-DeepLearning
POOL	I-DeepLearning
layer	I-DeepLearning
will	O
down-sample	O
along	O
the	O
spatial	O
dimensions	O
width	O
,	O
height	O
,	O
resulting	O
in	O
volume	O
62	B-DeepLearning
×	I-DeepLearning
62	I-DeepLearning
×	I-DeepLearning
4	I-DeepLearning
.	O
MLP	B-DeepLearning
layer	I-DeepLearning
will	O
compute	O
the	O
class	O
scores	O
,	O
resulting	O
in	O
volume	O
of	O
size	O
1	B-DeepLearning
×	I-DeepLearning
1	I-DeepLearning
×	I-DeepLearning
4	I-DeepLearning
,	O
where	O
each	O
of	O
the	O
4	O
numbers	O
correspond	O
to	O
a	O
class	O
score	O
for	O
the	O
4	O
tumor	O
regions	O
.	O
The	O
different	O
layers	O
in	O
the	O
network	O
are	O
3	O
Convolution	B-DeepLearning
layer	I-DeepLearning
C	O
,	O
3	O
Sub-Sampling	B-DeepLearning
layer	I-DeepLearning
P	O
,	O
and	O
2	O
fully	B-DeepLearning
connected	I-DeepLearning
multi-level	B-DeepLearning
perceptrons	I-DeepLearning
M	O
.	O
We	O
worked	O
with	O
different	O
number	O
of	O
hidden	B-DeepLearning
layers	I-DeepLearning
to	O
define	O
the	O
best	O
output	O
in	O
terms	O
of	O
tumor	O
identification	O
and	O
computational	O
resources	O
needed	O
see	O
Table	O
1	O
.	O
The	O
detailed	O
architecture	O
of	O
the	O
five	O
level	O
CNN	B-DeepLearning
for	O
tumor	O
classification	O
is	O
shown	O
in	O
Fig.	O
3	O
.	O
Our	O
architecture	O
combines	O
the	O
simplicity	O
of	O
Lenet	B-DeepLearning
architecture	O
with	O
the	O
data	B-DeepLearning
augmentation	I-DeepLearning
methods	O
used	O
by	O
AlexNet	B-DeepLearning
architecture	O
.	O
The	O
lower	O
3	O
layers	O
are	O
comprised	O
of	O
alternating	O
convolution	B-DeepLearning
and	I-DeepLearning
max-pooling	I-DeepLearning
layers	I-DeepLearning
.	O
The	O
first	O
convolution	B-DeepLearning
layer	I-DeepLearning
has	O
filter	B-DeepLearning
size	I-DeepLearning
5	I-DeepLearning
×	I-DeepLearning
5	I-DeepLearning
used	O
to	O
detect	O
low	O
level	O
features	O
like	O
edges	O
which	O
is	O
followed	O
by	O
a	O
max	B-DeepLearning
pooling	I-DeepLearning
layer	I-DeepLearning
of	I-DeepLearning
scale	I-DeepLearning
2	I-DeepLearning
to	O
down-sample	O
the	O
data.	O
.	O
This	O
data	O
is	O
then	O
sent	O
to	O
second	O
layer	O
of	O
5	B-DeepLearning
×	I-DeepLearning
5	I-DeepLearning
filters	I-DeepLearning
to	O
detect	O
higher	O
order	O
features	O
like	O
texture	O
and	O
spatial	O
connectivity	O
followed	O
by	O
a	O
max-pooling	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
last	O
convolution	B-DeepLearning
layer	I-DeepLearning
uses	O
a	O
filter	B-DeepLearning
of	I-DeepLearning
size	I-DeepLearning
3	I-DeepLearning
×	I-DeepLearning
3	I-DeepLearning
and	O
max-pooling	B-DeepLearning
size	I-DeepLearning
2	I-DeepLearning
for	O
down-	O
sampling	O
to	O
generate	O
more	O
higher	O
order	O
features	O
.	O
The	O
upper	O
2	O
layers	O
are	O
fully-connected	B-DeepLearning
multi-level	I-DeepLearning
perceptron	I-DeepLearning
MLP	B-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
hidden	B-DeepLearning
layer	I-DeepLearning
+	O
logistic	B-DeepLearning
regression	I-DeepLearning
.	O
The	O
second	O
layer	O
of	O
the	O
MLP	B-DeepLearning
is	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
consisting	O
of	O
four	B-DeepLearning
neurons	I-DeepLearning
see	O
Table	O
2	O
.	O
The	O
sum	O
of	O
the	O
output	O
probabilities	O
from	O
the	O
MLP	B-DeepLearning
is	O
1	O
,	O
ensured	O
by	O
the	O
use	O
of	O
Softmax	B-DeepLearning
algorithm	O
as	O
the	O
activation	B-DeepLearning
function	I-DeepLearning
in	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
of	O
the	O
MLP	O
.	O
The	O
convolution	B-DeepLearning
and	O
max	B-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
are	O
feature	O
extractors	O
and	O
the	O
MLP	B-DeepLearning
is	O
the	O
classifier	O
.	O
The	O
easiest	O
and	O
most	O
common	O
method	O
to	O
reduce	O
overfitting	B-DeepLearning
of	O
data	O
is	O
to	O
artificially	O
augment	O
the	O
dataset	B-DeepLearning
using	O
label-preserving	O
transformations	O
.	O
We	O
use	O
two	O
distinct	O
data	B-DeepLearning
augmentation	I-DeepLearning
techniques	O
both	O
of	O
which	O
allow	O
transformed	O
images	B-DeepLearning
to	O
be	O
produced	O
from	O
the	O
original	O
images	B-DeepLearning
with	O
very	O
little	O
computation	O
,	O
so	O
the	O
transformed	O
images	B-DeepLearning
do	O
not	O
need	O
to	O
be	O
stored	O
on	O
disk	O
.	O
The	O
second	O
technique	O
for	O
data	B-DeepLearning
augmentation	I-DeepLearning
alters	O
the	O
intensities	O
of	O
the	O
RGB	O
channels	O
in	O
training	B-DeepLearning
images	I-DeepLearning
[	O
9	O
]	O
.	O
We	O
perform	O
Principal	B-DeepLearning
component	I-DeepLearning
analysis	I-DeepLearning
PCA	B-DeepLearning
on	O
the	O
set	O
of	O
RGB	O
pixel	O
values	O
throughout	O
the	O
training	B-DeepLearning
set	I-DeepLearning
and	O
then	O
,	O
for	O
each	O
training	B-DeepLearning
image	I-DeepLearning
,	O
we	O
add	O
the	O
following	O
quantity	O
to	O
each	O
RGB	B-DeepLearning
image	I-DeepLearning
pixel	O
i.e.	O
,	O
Ixy	O
=	O
[	O
IR	O
xy	O
,	O
IG	O
xy	O
,	O
IB	O
xy	O
]	O
T	O
:	O
[	O
p1	O
,	O
p2	O
,	O
p3}{α1λ1	O
,	O
α2λ2	O
,	O
α3λ3	O
]	O
T	O
where	O
pi	O
and	O
λi	O
are	O
the	O
i-th	O
eigenvector	O
and	O
eigenvalue	O
of	O
the	O
3	O
×	O
3	O
covariance	O
matrix	O
of	O
RGB	O
pixel	O
values	O
,	O
respectively	O
,	O
and	O
αi	O
is	O
a	O
random	O
variable	O
drawn	O
from	O
a	O
Gaussian	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
01	O
.	O
Data	B-DeepLearning
augmentation	I-DeepLearning
helps	O
alleviate	O
over-fitting	B-DeepLearning
by	O
considerably	O
increasing	O
the	O
amount	O
of	O
training	B-DeepLearning
data	I-DeepLearning
,	O
removing	O
rotation	O
dependency	O
and	O
making	O
the	O
training	B-DeepLearning
images	I-DeepLearning
invariant	O
to	O
changes	O
in	O
the	O
color	O
brightness	O
and	O
intensity	O
through	O
PCA	O
.	O
The	O
network	O
is	O
trained	O
with	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
.	O
We	O
initialized	O
all	O
weights	B-DeepLearning
with	O
0	O
mean	O
by	O
assigning	O
them	O
small	O
,	O
random	O
and	O
unique	O
numbers	O
from	O
10−2	O
standard	O
deviation	O
Gaussian	O
random	O
numbers	O
,	O
so	O
that	O
each	O
layer	O
calculates	O
unique	O
updates	O
and	O
integrate	O
themselves	O
as	O
different	O
units	O
of	O
the	O
full	O
network	O
.	O
Each	O
case	O
consists	O
of	O
an	O
average	O
of	O
25	O
individual	O
svs	O
images	B-DeepLearning
representing	O
different	O
sections	O
of	O
the	O
microscopic	O
slide	O
.	O
The	O
256	B-DeepLearning
×	I-DeepLearning
256	I-DeepLearning
patches	I-DeepLearning
limited	O
the	O
CNN	B-DeepLearning
due	O
to	O
memory	O
issues	O
and	O
the	O
64	B-DeepLearning
×	I-DeepLearning
64	I-DeepLearning
patch	I-DeepLearning
size	I-DeepLearning
had	O
very	O
low	O
accuracy	B-DeepLearning
.	O
Hence	O
we	O
decided	O
on	O
a	O
128	B-DeepLearning
×	I-DeepLearning
128	I-DeepLearning
patch	I-DeepLearning
size	I-DeepLearning
.	O
This	O
resulted	O
in	O
about	O
5000	O
image	B-DeepLearning
patches	O
in	O
the	O
dataset	B-DeepLearning
.	O
Only	O
60%	O
patches	O
were	O
used	O
for	O
training	O
,	O
and	O
20%	O
data	O
was	O
used	O
as	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
the	O
remaining	O
20%	O
data	O
was	O
use	O
for	O
test	B-DeepLearning
set	I-DeepLearning
.	O
The	O
architecture	O
was	O
developed	O
in	O
JAVA	O
using	O
dl4j	B-DeepLearning
deep	B-DeepLearning
learning	I-DeepLearning
for	I-DeepLearning
java	I-DeepLearning
libraries	O
[	O
1	O
]	O
.	O
The	O
training	B-DeepLearning
data	I-DeepLearning
was	O
fed	O
to	O
the	O
network	O
in	O
batch	B-DeepLearning
sizes	I-DeepLearning
of	O
100	O
to	O
utilize	O
parallelism	O
and	O
improve	O
the	O
network	O
efficiency	O
.	O
The	O
objective	O
of	O
the	O
network	O
was	O
to	O
classify	O
the	O
input	B-DeepLearning
images	I-DeepLearning
tiles	O
into	O
one	O
of	O
the	O
four	O
regions	O
viable	O
tumor	O
,	O
coagulative	O
necrosis	O
,	O
osteoid	O
or	O
fibrosis	O
,	O
non-tumor	O
mentioned	O
before	O
.	O
The	O
performance	O
of	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
was	O
monitored	O
by	O
assessing	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
once	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
saturated	O
after	O
10	O
epochs	B-DeepLearning
,	O
training	O
was	O
stopped	O
.	O
Our	O
implementation	O
gives	O
F1-score	B-DeepLearning
of	O
0.86	O
and	O
an	O
accuracy	B-DeepLearning
of	O
084	O
.	O
In	O
this	O
section	O
we	O
present	O
and	O
compare	O
the	O
qualitative	O
output	O
of	O
three	O
architectures:	O
AlexNet	B-DeepLearning
,	O
Lenet	B-DeepLearning
,	O
and	O
our	O
proposed	O
architecture	O
.	O
We	O
find	O
that	O
the	O
running	O
time	O
of	O
Lenet	B-DeepLearning
is	O
fastest	O
but	O
the	O
accuracy	B-DeepLearning
and	O
precision	B-DeepLearning
of	O
our	O
proposed	O
architecture	O
is	O
better	O
than	O
both	O
AlexNet	B-DeepLearning
and	O
Lenet	B-DeepLearning
see	O
Table	O
3	O
.	O
We	O
can	O
continue	O
to	O
explore	O
different	O
architectures	O
and	O
strategies	O
for	O
the	O
training	O
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
by	O
changing	O
the	O
hyper-parameters	B-DeepLearning
or	O
pre-processing	O
the	O
input	B-DeepLearning
data	I-DeepLearning
like	O
using	O
the	O
LAB	O
color	O
space	O
instead	O
of	O
RGB	O
space	O
or	O
by	O
augmenting	O
the	O
results	O
of	O
initial	O
segmentation	O
otsu	O
segmentation	O
in	O
the	O
input	B-DeepLearning
data	I-DeepLearning
.	O
This	O
can	O
be	O
done	O
by	O
applying	O
the	O
full	O
convolution	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
to	O
generate	O
color	O
coded	O
likelihood	O
maps	O
for	O
the	O
pathologists	O
.	O
As	O
far	O
as	O
the	O
authors	O
are	O
aware	O
,	O
this	O
is	O
the	O
first	O
paper	O
describing	O
the	O
applicability	O
of	O
convolutional	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
for	O
diagnostic	O
analysis	B-DeepLearning
of	I-DeepLearning
osteosarcoma	I-DeepLearning
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
computational	O
method	O
for	O
predicting	O
DTIs	O
from	O
drug	O
molecular	O
structure	O
and	O
protein	O
sequence	O
by	O
using	O
the	O
stacked	O
autoencoder	B-DeepLearning
of	O
deep	O
learning	O
which	O
can	O
adequately	O
extracts	O
the	O
raw	O
data	O
information	O
.	O
The	O
experimental	O
results	O
of	O
5-fold	B-DeepLearning
cross-validation	I-DeepLearning
indicate	O
that	O
the	O
proposed	O
method	O
achieves	O
superior	O
performance	O
on	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
with	O
accuracy	B-DeepLearning
of	O
0.9414	O
,	O
0.9116	O
,	O
0.8669	O
and	O
0.8056	O
,	O
respectively	O
.	O
In	O
the	O
experiment	B-DeepLearning
,	O
we	O
make	O
the	O
predictions	O
on	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
drug-target	O
interactions	O
datasets	B-DeepLearning
involving	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
In	O
this	O
experiment	B-DeepLearning
,	O
we	O
used	O
the	O
chemical	O
structure	O
of	O
the	O
molecular	O
substructure	O
fingerprints	O
from	O
PubChem	B-DeepLearning
database	I-DeepLearning
.	O
It	O
defines	O
an	O
881	O
dimensional	O
binary	O
vector	B-DeepLearning
to	O
represent	O
the	O
molecular	O
substructure	O
.	O
Stacked	B-DeepLearning
Auto-Encoder	I-DeepLearning
SAE	B-DeepLearning
is	O
a	O
popular	O
depth	O
learning	O
model	B-DeepLearning
,	O
which	O
uses	O
autoencoders	B-DeepLearning
as	O
building	O
blocks	O
to	O
create	O
deep	O
neural	B-DeepLearning
network	I-DeepLearning
[	O
17	O
]	O
.	O
The	O
auto-encoder	B-DeepLearning
AE	O
can	O
be	O
considered	O
as	O
a	O
special	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
one	B-DeepLearning
input	I-DeepLearning
layer	I-DeepLearning
,	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
and	O
one	B-DeepLearning
output	I-DeepLearning
layer	I-DeepLearning
,	O
as	O
shown	O
in	O
Fig.	O
1	O
.	O
The	O
combination	O
of	O
multiple	O
auto-encoders	B-DeepLearning
together	O
constitutes	O
the	O
stacked	O
auto-encoders	B-DeepLearning
,	O
which	O
has	O
the	O
characteristics	O
of	O
deep	O
learning	O
.	O
Figure	O
2	O
shows	O
the	O
structure	O
of	O
the	O
stacked	O
auto-encoder	B-DeepLearning
with	O
h-level	O
auto-encoders	B-DeepLearning
which	O
are	O
trained	O
in	O
the	O
layer-wise	B-DeepLearning
and	O
bottom-up	O
manner	O
.	O
The	O
activation	B-DeepLearning
function	I-DeepLearning
is	O
usually	O
the	O
sigmoid	B-DeepLearning
function	O
or	O
tanh	B-DeepLearning
function	O
.	O
In	O
this	O
paper	O
,	O
we	O
set	O
up	O
a	O
3	B-DeepLearning
layer	I-DeepLearning
auto-encoder	I-DeepLearning
,	O
and	O
use	O
the	O
rotation	O
forest	O
as	O
the	O
final	O
classifier	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
5-fold	B-DeepLearning
cross-validation	I-DeepLearning
to	O
assess	O
the	O
predictive	O
ability	O
of	O
our	O
model	B-DeepLearning
in	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
involving	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
The	O
proposed	O
model	B-DeepLearning
performs	O
well	O
in	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets:	I-DeepLearning
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
Table	O
1	O
lists	O
the	O
experimental	O
results	O
on	O
the	O
enzyme	B-DeepLearning
dataset	I-DeepLearning
,	O
it	O
yielded	O
an	O
accuracy	B-DeepLearning
of	O
0.9414	O
,	O
sensitivity	B-DeepLearning
of	O
0.9555	O
,	O
precision	B-DeepLearning
of	O
0.9293	O
,	O
MCC	B-DeepLearning
of	O
0.8832	O
and	O
AUC	B-DeepLearning
of	O
09425	O
.	O
The	O
highest	O
accuracy	B-DeepLearning
of	O
the	O
five	O
models	B-DeepLearning
reached	O
0.9462	O
,	O
and	O
the	O
lowest	O
also	O
reached	O
09385	O
.	O
The	O
accuracy	B-DeepLearning
,	O
sensitivity	B-DeepLearning
,	O
precision	B-DeepLearning
,	O
MCC	B-DeepLearning
and	O
AUC	B-DeepLearning
of	O
cross-validation	B-DeepLearning
are	O
0.9116	O
,	O
0.9569	O
,	O
0.8778	O
,	O
0.8271	O
and	O
0.9107	O
,	O
respectively	O
.	O
The	O
average	O
accuracy	B-DeepLearning
,	O
sensitivity	B-DeepLearning
,	O
precision	B-DeepLearning
,	O
MCC	B-DeepLearning
and	O
AUC	B-DeepLearning
are	O
0.8669	O
,	O
0.8164	O
,	O
0.9102	O
,	O
0.7396	O
and	O
0.8743	O
,	O
respectively	O
.	O
The	O
highest	O
accuracy	B-DeepLearning
of	O
the	O
five	O
models	B-DeepLearning
reached	O
09331	O
.	O
We	O
achieved	O
an	O
accuracy	B-DeepLearning
of	O
0.8056	O
,	O
sensitivity	B-DeepLearning
of	O
0.7627	O
,	O
precision	B-DeepLearning
of	O
0.8410	O
,	O
MCC	B-DeepLearning
of	O
0.6188	O
and	O
AUC	B-DeepLearning
of	O
08176	O
.	O
Figures	O
4	O
,	O
5	O
,	O
6	O
and	O
7	O
show	O
the	O
ROC	B-DeepLearning
curves	I-DeepLearning
obtained	O
on	O
the	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	B-DeepLearning
receptors	I-DeepLearning
datasets	I-DeepLearning
by	O
the	O
proposed	O
method	O
.	O
The	O
results	O
of	O
the	O
comparison	O
show	O
that	O
the	O
stacked	O
auto-encoder	B-DeepLearning
combined	O
with	O
the	O
rotation	O
forest	O
classifier	O
can	O
improve	O
the	O
prediction	O
ability	O
on	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
Table	O
6	O
.	O
CAST	O
uses	O
Multi-Layer	B-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
,	O
a	O
type	O
of	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
that	O
is	O
often	O
applied	O
to	O
classification	O
problems	O
[	O
17	O
]	O
.	O
The	O
retina	B-DeepLearning
training	I-DeepLearning
set	I-DeepLearning
is	O
comprised	O
of	O
5891	B-DeepLearning
pixel	I-DeepLearning
examples	I-DeepLearning
.	O
The	O
accuracy	B-DeepLearning
of	O
the	O
training	B-DeepLearning
set	I-DeepLearning
was	O
tested	O
using	O
Weka	O
Explorer	O
,	O
which	O
calculated	O
the	O
accuracy	B-DeepLearning
of	O
correctly	O
classifying	O
pixels	O
to	O
be	O
96.4013%	O
[	O
19	O
]	O
.	O
The	O
network	O
used	O
consisted	O
of	O
97	B-DeepLearning
input	I-DeepLearning
nodes	I-DeepLearning
,	O
1	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
with	I-DeepLearning
49	I-DeepLearning
nodes	I-DeepLearning
,	O
and	O
2	B-DeepLearning
output	I-DeepLearning
nodes	I-DeepLearning
.	O
The	O
network	O
was	O
trained	O
using	O
ten-fold	B-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
.	O
The	O
learning	O
method	O
used	O
was	O
the	O
standard	O
method	O
of	O
backpropagation	B-DeepLearning
gradient	B-DeepLearning
decent	I-DeepLearning
.	O
In	O
order	O
to	O
develop	O
CAST	O
we	O
utilized	O
an	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
and	O
tested	O
its	O
ability	O
against	O
fast	O
marching	O
.	O
Classifiers	O
such	O
as	O
perceptrons	B-DeepLearning
,	O
the	O
basic	O
component	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
have	O
been	O
found	O
useful	O
for	O
identifying	O
membranes	O
in	O
EM	O
images	B-DeepLearning
[	O
6	O
]	O
.	O
One	O
kind	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
that	O
has	O
been	O
widely	O
used	O
for	O
image	B-DeepLearning
segmentation	O
is	O
the	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
CNN	B-DeepLearning
.	O
ANNs	B-DeepLearning
require	O
significant	O
computing	O
power	O
,	O
which	O
can	O
be	O
an	O
important	O
factor	O
if	O
the	O
data	B-DeepLearning
set	I-DeepLearning
is	O
large	O
.	O
Use	O
of	O
ANNs	B-DeepLearning
has	O
become	O
common	O
in	O
image	B-DeepLearning
processing	O
due	O
to	O
generally	O
high	O
performance	O
,	O
and	O
our	O
results	O
do	O
not	O
differ	O
in	O
that	O
we	O
also	O
find	O
classification	O
by	O
ANN	B-DeepLearning
to	O
provide	O
superior	O
performance	O
.	O
The	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
examines	O
each	O
pixel	O
in	O
the	O
selected	O
area	O
.	O
This	O
allows	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
to	O
make	O
an	O
accurate	O
classification	O
despite	O
low	O
contrasts	O
and	O
fragmented	O
boundaries	O
.	O
Outlines	O
made	O
using	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
are	O
subjectively	O
much	O
more	O
accurate	O
than	O
those	O
made	O
using	O
fast	O
marching	O
.	O
Expand	O
Area	O
works	O
in	O
three	O
dimensions	O
[	O
21	O
]	O
,	O
but	O
the	O
current	O
neural	B-DeepLearning
network	I-DeepLearning
only	O
works	O
in	O
two	O
dimensions	O
.	O
If	O
part	O
of	O
the	O
structure	O
of	O
interest	O
shifted	O
out	O
of	O
the	O
area	O
being	O
examined	O
by	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
somewhere	O
in	O
the	O
stack	O
,	O
the	O
results	O
produced	O
by	O
the	O
network	O
would	O
be	O
off	O
.	O
One	O
possible	O
solution	O
would	O
be	O
to	O
move	O
the	O
region	O
being	O
examined	O
by	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
on	O
a	O
slice-to-slice	O
basis	O
.	O
These	O
words	O
are	O
filtered	O
by	O
the	O
NLTK	B-DeepLearning
toolkit	I-DeepLearning
[	O
9	O
]	O
.	O
In	O
contrast	O
to	O
the	O
one-hot	O
encoding	O
,	O
the	O
NNLM	B-DeepLearning
generates	O
word	O
vectors	B-DeepLearning
with	O
low	O
and	O
dense	O
vector	B-DeepLearning
,	O
it	O
is	O
easier	O
to	O
capture	O
the	O
word’s	O
property	O
.	O
Mikolov	O
et	O
al.	O
[	O
12	O
]	O
extended	O
the	O
Bengio’	O
work	O
and	O
built	O
a	O
new	O
neural	B-DeepLearning
network	I-DeepLearning
language	O
learning	O
model	B-DeepLearning
,	O
Word2Ve	B-DeepLearning
,	O
using	O
different	O
learning	O
methods:	O
Continues	O
Bag	O
of	O
Words	O
CBOW	O
and	O
Skip-gram	O
.	O
The	O
reason	O
that	O
we	O
use	O
the	O
Word2Vec	B-DeepLearning
due	O
to	O
it	O
consume	O
less	O
time	O
than	O
Benhio’s	O
NNLM	B-DeepLearning
while	O
training	O
.	O
We	O
applied	O
the	O
Word2Vec	B-DeepLearning
to	O
generate	O
the	O
good	O
quality	O
of	O
word	O
vectors	B-DeepLearning
through	O
training	O
the	O
whole	O
data	B-DeepLearning
set	I-DeepLearning
.	O
In	O
order	O
to	O
explore	O
the	O
performances	O
of	O
different	O
combinations	O
of	O
the	O
nine	O
features	O
,	O
we	O
trained	O
5621000	B-DeepLearning
pattern	I-DeepLearning
recognition	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
[	O
11	O
]	O
.	O
In	O
this	O
article	O
,	O
we	O
study	O
the	O
impacts	O
of	O
encoding	O
schemes	O
on	O
several	O
variant	O
algorithms	O
in	O
auto-encoders	B-DeepLearning
by	O
applying	O
deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
to	O
gene	O
annotation	O
.	O
Four	O
variant	O
auto-encoder	B-DeepLearning
algorithms	O
include	O
orthodox	B-DeepLearning
auto-encoder	I-DeepLearning
,	O
denoising	B-DeepLearning
auto-encoder	I-DeepLearning
,	O
hidden-layer	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
and	O
double	B-DeepLearning
denoising	I-DeepLearning
auto-encoder.	I-DeepLearning
.	O
The	O
data	B-DeepLearning
sets	I-DeepLearning
are	O
the	O
standard	O
benchmark	O
from	O
fruitfly.org	O
for	O
predicting	B-DeepLearning
gene	I-DeepLearning
splicing	I-DeepLearning
sites	I-DeepLearning
on	O
human	O
genome	O
sequences	O
[	O
16	O
]	O
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
I	O
is	O
the	O
Acceptor	O
locations	O
containing	O
6877	B-DeepLearning
sequences	I-DeepLearning
with	O
90	B-DeepLearning
features	I-DeepLearning
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
II	O
is	O
the	O
Donor	O
locations	O
including	O
6246	B-DeepLearning
sequences	I-DeepLearning
with	O
15	B-DeepLearning
features	I-DeepLearning
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
of	O
cleaned	O
269	O
genes	O
is	O
divided	O
into	O
a	O
test	O
and	O
a	O
training	O
data	B-DeepLearning
set	I-DeepLearning
[	O
16	O
]	O
.	O
Denoising	B-DeepLearning
auto-encoder	I-DeepLearning
seems	O
not	O
fit	O
to	O
the	O
application	O
of	O
DNA	B-DeepLearning
structure	I-DeepLearning
prediction	I-DeepLearning
because	O
corrupted	O
input	B-DeepLearning
data	I-DeepLearning
DNA	O
features	O
at	O
each	O
location	O
may	O
have	O
a	O
high	O
dependency	O
with	O
others	O
such	O
that	O
denoising	O
makes	O
the	O
prediction	O
messed	O
.	O
Compared	O
with	O
the	O
performance	O
of	O
input-layer	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
in	O
Fig.	O
2c	O
,	O
d	O
,	O
in	O
hidden-layer	B-DeepLearning
denoising	B-DeepLearning
auto-encoder	I-DeepLearning
model	B-DeepLearning
,	O
corrupting	O
some	O
nodes	O
on	O
hidden	B-DeepLearning
layers	I-DeepLearning
makes	O
a	O
less	O
impact	O
than	O
corrupting	O
nodes	O
on	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
Figure	O
2g	O
,	O
h	O
shows	O
the	O
performance	O
of	O
double	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
.	O
On	O
the	O
other	O
side	O
,	O
it	O
shows	O
that	O
auto-encoder	B-DeepLearning
method	O
without	O
denoising	O
is	O
better	O
than	O
other	O
three	O
variants	O
.	O
The	O
over-fitting	B-DeepLearning
issues	O
occur	O
in	O
Denoising	B-DeepLearning
Auto-encoder	I-DeepLearning
more	O
frequently	O
than	O
others	O
.	O
The	O
over-fitting	B-DeepLearning
occurrence	O
is	O
correlated	O
to	O
the	O
auto-encoder	B-DeepLearning
algorithm	O
and	O
the	O
encoding	O
schemes	O
.	O
The	O
DAE	O
shows	O
the	O
poorest	O
performance	O
among	O
the	O
autoencoder	B-DeepLearning
algorithms	O
.	O
In	O
case	O
study	O
section	O
given	O
below	O
,	O
we	O
demonstrate	O
the	O
application	O
of	O
a	O
Neural	B-DeepLearning
Network	I-DeepLearning
for	O
identifying	O
multimarker	O
panels	O
for	O
breast	O
cancer	O
based	O
on	O
liquid	O
chromatography	O
tandem	O
mass	O
spectrometry	O
LC/MS/MS	O
proteomics	O
profiles	O
.	O
Lancashire	O
et	O
al.	O
24	O
described	O
with	O
recent	O
literature	O
that	O
artificial	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
can	O
cope	O
with	O
highly	O
dimensional	O
,	O
complex	O
datasets	B-DeepLearning
such	O
as	O
those	O
generated	O
by	O
protein	O
mass	O
spectrometry	O
and	O
DNA	O
microarray	O
experiments	B-DeepLearning
,	O
and	O
can	O
also	O
be	O
used	O
to	O
solve	O
problems	O
,	O
such	O
as	O
disease	B-DeepLearning
classification	I-DeepLearning
and	O
identification	O
of	O
biomarkers	O
.	O
We	O
used	O
a	O
data	O
analysis	O
method	O
based	O
on	O
a	O
Feed-Forward	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
FFNN	B-DeepLearning
to	O
identify	O
multiprotein	O
biomarker	O
panels	O
,	O
with	O
which	O
we	O
are	O
capable	O
of	O
separating	O
plasma	O
samples	O
regarding	O
reference	O
and	O
cancerous	O
with	O
high	O
predictive	O
performance	O
.	O
According	O
to	O
an	O
area-under-the-curve	B-DeepLearning
AUC	B-DeepLearning
criterion	O
the	O
optimal	O
combination	O
of	O
a	O
panel	O
of	O
five	O
markers	O
was	O
determined	O
,	O
using	O
both	O
a	O
twovariable	O
output	O
encoding	O
scheme	O
and	O
a	O
single-variable	O
encoding	O
scheme	O
.	O
We	O
compared	O
the	O
Receiver	B-DeepLearning
Operating	I-DeepLearning
Characteristics	I-DeepLearning
ROC	B-DeepLearning
performance	O
and	O
verified	O
that	O
the	O
best	O
five-marker	O
panel	O
performed	O
well	O
in	O
both	O
training	B-DeepLearning
dataset	I-DeepLearning
and	O
test	B-DeepLearning
dataset	I-DeepLearning
,	O
achieving	O
more	O
than	O
82.5%	O
in	O
sensitivity	B-DeepLearning
and	O
specificity	B-DeepLearning
.	O
The	O
algorithm	O
NNPP	B-DeepLearning
Neural	B-DeepLearning
Network	I-DeepLearning
Promoter	I-DeepLearning
Prediction	I-DeepLearning
,	O
which	O
is	O
discussed	O
in	O
Section	O
10.5	O
,	O
was	O
developed	O
for	O
the	O
Berkeley	O
Drosophila	O
Genome	O
Project	O
BDGP	O
,	O
and	O
uses	O
time-delay	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
architecture	O
and	O
recognition	O
of	O
specific	O
sequence	O
patterns	O
to	O
predict	O
promoters	O
see	O
Figure	O
10.14	O
for	O
details	O
.	O
Each	O
sequence	O
position	O
is	O
represented	O
by	O
four	B-DeepLearning
input	I-DeepLearning
units	I-DeepLearning
,	O
each	O
representing	O
one	O
base	O
,	O
with	O
a	O
total	O
of	O
51	O
bases	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
Each	O
TATA-box	O
hidden	B-DeepLearning
unit	I-DeepLearning
has	O
inputs	O
from	O
15	O
consecutive	O
bases	O
,	O
and	O
the	O
Inr	O
hidden	B-DeepLearning
units	I-DeepLearning
also	O
receive	O
signals	O
from	O
15	O
consecutive	O
bases	O
.	O
The	O
same	O
weighting	O
scheme	O
is	O
applied	O
to	O
all	O
the	O
TATA-box	O
hidden	B-DeepLearning
units	I-DeepLearning
and	O
similarly	O
with	O
another	O
set	B-DeepLearning
of	I-DeepLearning
weights	I-DeepLearning
for	O
the	O
Inr	O
hidden	B-DeepLearning
units	I-DeepLearning
.	O
The	O
four	O
layers	O
of	O
squares	O
underneath	O
the	O
sequence	O
represent	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
units	O
and	O
encode	O
the	O
sequence	O
in	O
a	O
very	O
simple	O
way	O
.	O
Each	O
of	O
the	O
units	O
in	O
the	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
the	O
TATA	O
and	O
Inr	O
layers	O
receives	O
input	O
from	O
a	O
consecutive	O
set	O
of	O
15	O
bases	O
.	O
A	O
variety	O
of	O
measures	O
,	O
such	O
as	O
the	O
weight	O
matrix	O
score	O
for	O
TATA-box	O
signal	O
and	O
simple	O
measurements	O
such	O
as	O
the	O
GC	O
content	O
and	O
the	O
distance	O
between	O
different	O
signals	O
,	O
are	O
used	O
to	O
assign	O
values	O
to	O
the	O
units	O
in	O
an	O
input	B-DeepLearning
layer	I-DeepLearning
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
.	O
These	O
feed	O
via	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
to	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
unit	O
,	O
which	O
predicts	O
the	O
presence	O
of	O
RNA	O
polymerase	O
II	O
promoters	O
in	O
the	O
sequence	O
window	O
.	O
The	O
donor	O
sites	O
are	O
modeled	O
with	O
a	O
set	O
of	O
10	B-DeepLearning
networks	I-DeepLearning
,	O
each	O
with	O
23	O
bases	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
represented	O
by	O
four	O
units	O
each	O
,	O
as	O
in	O
Figure	O
10.14	O
,	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
10	O
units	O
,	O
and	O
a	O
single	O
output	B-DeepLearning
layer	I-DeepLearning
unit	O
.	O
The	O
10	B-DeepLearning
networks	I-DeepLearning
were	O
separately	O
trained	O
from	O
different	O
starting	O
points	O
,	O
and	O
their	O
outputs	O
from	O
0	O
to	O
1	O
averaged	O
to	O
get	O
the	O
final	O
score	O
.	O
A	O
similar	O
set	O
of	O
10	B-DeepLearning
networks	I-DeepLearning
was	O
used	O
to	O
score	O
acceptor	O
sites	O
,	O
except	O
that	O
these	O
have	O
a	O
61-base	O
input	O
window	O
and	O
15	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	O
.	O
The	O
set	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
used	O
to	O
predict	O
coding	O
regions	O
in	O
NetPlantGene	O
has	O
as	O
output	O
a	O
score	O
for	O
each	O
base	O
of	O
its	O
likelihood	O
of	O
being	O
in	O
a	O
coding	O
region	O
.	O
A	O
neural	B-DeepLearning
network	I-DeepLearning
predicts	O
the	O
three	O
secondary	O
structure	O
types	O
a	O
,	O
b	O
,	O
and	O
coil	O
using	O
real	O
numbers	O
from	O
the	O
output	O
units	O
that	O
make	O
up	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
first	O
network	O
is	O
a	O
sequence-to-structure	B-DeepLearning
network	I-DeepLearning
whose	O
input	O
represents	O
the	O
local	O
alignment	O
,	O
residue	O
conservation	O
,	O
and	O
additionally	O
long-range	O
sequence	O
information	O
see	O
Figure	O
1122	O
.	O
The	O
output	O
of	O
the	O
first	O
network	O
forms	O
the	O
input	O
to	O
the	O
second	O
neural	B-DeepLearning
network	I-DeepLearning
,	O
a	O
structure-to-structure	B-DeepLearning
network	I-DeepLearning
.	O
NNSSP	B-DeepLearning
Neural	B-DeepLearning
Net	I-DeepLearning
Nearest	I-DeepLearning
Neighbor	I-DeepLearning
is	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
development	O
of	O
a	O
statistical	O
and	O
knowledge-based	O
program	O
,	O
SSP	O
,	O
which	O
,	O
like	O
PREDATOR	O
,	O
used	O
the	O
nearestneighbor	O
approach	O
.	O
NNSSP	B-DeepLearning
has	O
an	O
advanced	O
scoring	O
system	O
that	O
combines	O
sequence	O
similarity	O
,	O
local	O
sequence	O
information	O
,	O
and	O
knowledge-based	O
information	O
on	O
b-turns	O
and	O
the	O
amino-	O
and	O
carboxy-terminal	O
properties	O
of	O
a-helices	O
and	O
b-strands	O
.	O
Schematic	O
representation	O
of	O
the	O
double	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	I-DeepLearning
frequently	O
used	O
in	O
protein	B-DeepLearning
secondary	I-DeepLearning
structure	I-DeepLearning
prediction	I-DeepLearning
.	O
The	O
three	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	O
receive	O
the	O
same	O
signals	O
from	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
,	O
but	O
apply	O
different	O
weights	B-DeepLearning
to	O
them	O
.	O
The	O
first	O
hidden	B-DeepLearning
layer	I-DeepLearning
will	O
contain	O
39	B-DeepLearning
units	I-DeepLearning
for	O
a	O
13-residue	O
window	O
,	O
and	O
these	O
send	O
signals	O
to	O
units	O
in	O
a	O
second	O
hidden	B-DeepLearning
layer	I-DeepLearning
,	O
which	O
then	O
signals	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
see	O
Figure	O
1226	O
.	O
The	O
communication	O
from	O
the	O
first	O
hidden	B-DeepLearning
layer	I-DeepLearning
to	O
the	O
second	O
has	O
a	O
repeat	O
pattern	O
intended	O
to	O
mimic	O
the	O
helical	O
structure	O
,	O
and	O
has	O
been	O
colored	O
for	O
clarity	O
.	O
The	O
output	B-DeepLearning
layer	I-DeepLearning
Oi	O
has	O
three	B-DeepLearning
units	I-DeepLearning
,	O
corresponding	O
to	O
predicting	O
a-helix	O
,	O
b-strand	O
,	O
or	O
coil	O
for	O
residue	O
i	O
.	O
These	O
feed	O
signals	O
to	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
15	B-DeepLearning
units	I-DeepLearning
that	O
in	O
turn	O
send	O
signals	O
to	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
of	O
seven	B-DeepLearning
units	I-DeepLearning
.	O
The	O
initial	O
neural	B-DeepLearning
network	I-DeepLearning
has	O
a	O
sevenstate	B-DeepLearning
output	I-DeepLearning
,	O
namely	O
a-helix	O
H	O
,	O
b-strand	O
E	O
,	O
coil	O
C	O
,	O
and	O
the	O
single	O
residue	O
at	O
the	O
beginning	O
and	O
end	O
of	O
a	O
helix	O
or	O
strand	O
Hb	O
,	O
He	O
,	O
Eb	O
,	O
and	O
Ee	O
,	O
respectively	O
.	O
A	O
15-residue	O
sequence	O
window	O
is	O
used	O
with	O
20	B-DeepLearning
input	I-DeepLearning
layer	I-DeepLearning
units	O
per	O
residue	O
representing	O
the	O
PSSM	O
and	O
an	O
additional	O
spacer	O
unit	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
used	O
is	O
a	O
standard	O
feed-forward	B-DeepLearning
model	I-DeepLearning
as	O
described	O
in	O
Section	O
12.4	O
,	O
with	O
six	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	I-DeepLearning
and	O
two	O
output	B-DeepLearning
units	I-DeepLearning
.	O
Indeed	O
,	O
this	O
work	O
will	O
focus	O
on	O
the	O
development	O
of	O
a	O
diagnosis	O
support	O
system	O
,	O
in	O
terms	O
of	O
its	O
knowledge	O
representation	O
and	O
reasoning	O
procedures	O
,	O
under	O
a	O
formal	O
framework	O
based	O
on	O
Logic	O
Programming	O
,	O
complemented	O
with	O
an	O
approach	O
to	O
computing	O
centered	O
on	O
Artificial	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
,	O
to	O
evaluate	O
ACS	O
predisposing	O
and	O
the	O
respective	O
Degree-of-Confidence	O
that	O
one	O
has	O
on	O
such	O
a	O
happening	O
.	O
Each	O
base	O
pair	O
A	O
,	O
C	O
,	O
T	O
,	O
G	O
was	O
denoted	O
as	O
a	O
one-hot	O
vector	B-DeepLearning
[	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
]	O
,	O
[	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
]	O
,	O
[	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
]	O
and	O
[	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
]	O
respectively	O
.	O
For	O
example	O
,	O
if	O
we	O
used	O
DNase-seq	O
data	O
with	O
101	O
bp	O
,	O
the	O
vector	B-DeepLearning
was	O
of	O
size	O
1	O
x	O
101	O
for	O
a	O
sample	O
.	O
The	O
CNN	B-DeepLearning
consists	O
of	O
a	O
convolutional	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
max-pooling	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
,	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
[	O
28	O
]	O
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
DNN	B-DeepLearning
consists	O
of	O
one	O
or	O
two	O
full	B-DeepLearning
connection	I-DeepLearning
layers	I-DeepLearning
,	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
after	O
each	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
For	O
CNN	B-DeepLearning
models	I-DeepLearning
,	O
we	O
vary	O
the	O
number	B-DeepLearning
of	I-DeepLearning
kernels	I-DeepLearning
,	O
the	O
size	B-DeepLearning
of	I-DeepLearning
kernel	I-DeepLearning
window	O
,	O
and	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
.	O
For	O
DNN	B-DeepLearning
models	I-DeepLearning
,	O
we	O
vary	O
the	O
number	B-DeepLearning
of	I-DeepLearning
layers	I-DeepLearning
,	O
and	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neural	I-DeepLearning
in	O
each	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
.	O
For	O
training	O
,	O
we	O
used	O
the	O
cross-entropy	B-DeepLearning
as	O
the	O
loss	B-DeepLearning
function	I-DeepLearning
.	O
Given	O
this	O
loss	B-DeepLearning
function	I-DeepLearning
and	O
different	O
hyper-parameters	B-DeepLearning
see	O
below	O
,	O
the	O
models	B-DeepLearning
were	O
trained	O
using	O
the	O
standard	B-DeepLearning
error	I-DeepLearning
back-propagation	I-DeepLearning
algorithm	O
and	O
AdaDetla	B-DeepLearning
method	O
[	O
29	O
]	O
.	O
We	O
set	O
each	O
model	B-DeepLearning
for	O
100	B-DeepLearning
epochs	I-DeepLearning
and	O
128	O
mini-batch	B-DeepLearning
size	I-DeepLearning
and	O
validated	O
the	O
model	B-DeepLearning
after	O
each	O
epoch	O
.	O
Then	O
the	O
early-stop	B-DeepLearning
trick	O
was	O
used	O
to	O
stop	B-DeepLearning
training	I-DeepLearning
as	O
the	O
error	O
on	O
validation	B-DeepLearning
set	I-DeepLearning
is	O
higher	O
than	O
the	O
last	O
four	O
epochs	B-DeepLearning
.	O
The	O
best	O
model	B-DeepLearning
was	O
chosen	O
according	O
to	O
the	O
accuracy	B-DeepLearning
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
.	O
For	O
KNN	O
,	O
LR	O
,	O
RF	O
,	O
we	O
implemented	O
these	O
baselines	O
using	O
the	O
python	O
based	O
scikit-learn	B-DeepLearning
package	O
.	O
We	O
used	O
python	B-DeepLearning
and	O
Keras	B-DeepLearning
framework	O
to	O
train	O
neural	B-DeepLearning
networks	I-DeepLearning
.	O
We	O
used	O
python	B-DeepLearning
and	O
skcikit-learn	B-DeepLearning
to	O
train	O
conventional	O
machine	O
learning	O
methods	O
[	O
30	O
]	O
.	O
We	O
used	O
different	O
sample	B-DeepLearning
lengths	I-DeepLearning
to	O
train	O
the	O
CNN	B-DeepLearning
models	I-DeepLearning
and	O
different	O
hyper-parameters	B-DeepLearning
for	O
each	O
length	O
.	O
For	O
each	O
length	O
,	O
we	O
selected	O
the	O
results	O
of	O
best	O
hyper-parameters	B-DeepLearning
.	O
Performance	O
evaluation	O
of	O
CNN	B-DeepLearning
with	O
respect	O
to	O
sample	B-DeepLearning
length	I-DeepLearning
and	O
model	B-DeepLearning
structure	O
using	O
HMS	O
data	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
AUCs	B-DeepLearning
across	O
256	O
experiments	B-DeepLearning
.	O
The	O
effect	O
of	O
kernel	B-DeepLearning
number	I-DeepLearning
.	O
The	O
effect	O
of	O
neuron	B-DeepLearning
number	I-DeepLearning
.	O
The	O
effect	O
of	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
.	O
The	O
effect	O
of	O
sample	B-DeepLearning
length	I-DeepLearning
and	O
DNN	B-DeepLearning
model	I-DeepLearning
structure	I-DeepLearning
.	O
The	O
performance	O
comparison	O
of	O
DNN	B-DeepLearning
versus	O
CNN	B-DeepLearning
.	O
First	O
,	O
more	O
convolutional	B-DeepLearning
kernels	I-DeepLearning
could	O
also	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
2B	O
.	O
However	O
,	O
when	O
more	O
than	O
64	O
kernels	B-DeepLearning
were	O
used	O
,	O
the	O
improvement	O
seemed	O
to	O
be	O
saturated	O
for	O
the	O
256	O
experiments	B-DeepLearning
Fig.	O
2B	O
.	O
Second	O
,	O
more	O
neurons	O
in	O
the	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
of	O
CNN	B-DeepLearning
could	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
2C	O
.	O
We	O
observe	O
that	O
small	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
achieves	O
better	O
performance	O
than	O
using	O
large	O
ones	O
Fig.	O
2D	O
while	O
big	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
usually	O
used	O
in	O
sequence-based	O
CNN	B-DeepLearning
models	I-DeepLearning
.	O
We	O
find	O
that	O
deeper	O
neural	B-DeepLearning
networks	I-DeepLearning
and	O
longer	O
sample	O
length	O
work	O
better	O
too	O
for	O
DNN	B-DeepLearning
Fig.	O
2E	O
.	O
However	O
,	O
the	O
performance	O
of	O
DNN	B-DeepLearning
is	O
still	O
slightly	O
worse	O
than	O
that	O
of	O
CNN	B-DeepLearning
,	O
indicating	O
the	O
importance	O
of	O
combining	O
convolution	O
operation	O
with	O
HMS	O
data	O
Fig.	O
2F	O
.	O
We	O
conducted	O
leave-one-feature-out	O
feature	O
selection	O
experiments	B-DeepLearning
to	O
train	O
the	O
CNN	B-DeepLearning
models	I-DeepLearning
by	O
using	O
merely	O
four	O
histone	O
modifications	O
data	O
with	O
the	O
same	O
hyperparameters	B-DeepLearning
in	O
previous	O
section	O
.	O
For	O
model	B-DeepLearning
architectures	O
,	O
more	O
convolutional	B-DeepLearning
kernels	I-DeepLearning
could	O
also	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
4B	O
.	O
Thus	O
,	O
no	O
matter	O
what	O
the	O
data	O
type	O
is	O
,	O
the	O
additional	O
kernels	B-DeepLearning
are	O
beneficial	O
to	O
enhance	O
power	O
in	O
extracting	O
features	O
and	O
improve	O
model	B-DeepLearning
performance	O
.	O
By	O
changing	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
last	O
dense	B-DeepLearning
layer	I-DeepLearning
of	O
CNN	B-DeepLearning
,	O
we	O
can	O
see	O
that	O
models	B-DeepLearning
with	O
more	O
hidden	B-DeepLearning
neurons	I-DeepLearning
achieve	O
better	O
performance	O
Fig.	O
4C	O
.	O
We	O
also	O
see	O
that	O
CNN	B-DeepLearning
models	I-DeepLearning
with	O
small	O
and	O
large	O
kernel	B-DeepLearning
window	I-DeepLearning
sizes	I-DeepLearning
4	O
and	O
24	O
achieve	O
almost	O
the	O
same	O
performance	O
for	O
different	O
sample	O
lengths	O
Fig.	O
4D	O
.	O
This	O
suggests	O
that	O
kernel	B-DeepLearning
window	I-DeepLearning
sizes	I-DeepLearning
4	O
and	O
24	O
could	O
not	O
distinctly	O
influence	O
DHS	O
data	O
information	O
.	O
For	O
comparison	O
with	O
CNN	B-DeepLearning
,	O
we	O
also	O
trained	O
DNN	B-DeepLearning
using	O
different	O
sequence	O
lengths	O
and	O
hyper-parameters	B-DeepLearning
for	O
the	O
DHS	O
data	O
.	O
Moreover	O
,	O
the	O
performance	O
of	O
DNN	B-DeepLearning
is	O
slightly	O
worse	O
than	O
that	O
of	O
CNN	B-DeepLearning
,	O
indicating	O
the	O
importance	O
of	O
combining	O
convolution	O
operation	O
with	O
DHS	O
data	O
Fig.	O
4F	O
.	O
In	O
both	O
HMS	O
and	O
DHS	O
cases	O
,	O
CNN	B-DeepLearning
perform	O
significantly	O
better	O
than	O
conventional	O
classifiers	O
in	O
term	O
of	O
the	O
distribution	O
of	O
AUCs	B-DeepLearning
across	O
256	O
experiments	B-DeepLearning
Fig.	O
5	O
.	O
With	O
3D	B-DeepLearning
convolution	I-DeepLearning
kernel	I-DeepLearning
in	O
3DCNN	B-DeepLearning
could	O
learn	O
more	O
spatial	O
information	O
than	O
the	O
standard	O
2D	O
convolutional	O
neural	B-DeepLearning
networks	I-DeepLearning
with	O
the	O
2D	B-DeepLearning
kernels	I-DeepLearning
.	O
Our	O
3DCNN	B-DeepLearning
is	O
a	O
seven	B-DeepLearning
layers	I-DeepLearning
deep	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consists	O
the	O
layers	O
of	O
3D	B-DeepLearning
convolution	I-DeepLearning
,	O
batch	B-DeepLearning
normalization	I-DeepLearning
,	O
and	O
Softmax	B-DeepLearning
.	O
As	O
the	O
input	O
is	O
a	O
high-resolution	O
3D	O
MRI	O
data	O
,	O
3DCNN	B-DeepLearning
is	O
applied	O
deep	O
convolution	B-DeepLearning
layers	I-DeepLearning
with	O
small	O
kernel	B-DeepLearning
numbers	O
to	O
overcome	O
the	O
overfitting	B-DeepLearning
problem	O
.	O
Meanwhile	O
,	O
3DMaxPooling	B-DeepLearning
is	O
introduced	O
to	O
dimension	B-DeepLearning
reduction	I-DeepLearning
.	O
Except	O
for	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
,	O
all	O
the	O
layers	O
are	O
3D	B-DeepLearning
convolution	I-DeepLearning
layers	I-DeepLearning
,	O
which	O
have	O
a	O
much	O
stronger	O
nonlinear	O
mapping	O
power	O
and	O
feature	O
extraction	O
capacity	O
than	O
traditional	O
2D	B-DeepLearning
convolution	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
.	O
At	O
last	O
,	O
due	O
to	O
the	O
3D	O
high	O
dimension	O
in	O
MRI	O
data	O
,	O
all	O
the	O
parameters	B-DeepLearning
and	O
convolution	B-DeepLearning
kernels	I-DeepLearning
in	O
3DCNN	B-DeepLearning
are	O
small	O
to	O
prevent	O
the	O
overfitting	B-DeepLearning
.	O
There	O
are	O
80	B-DeepLearning
samples	I-DeepLearning
in	O
our	O
MRI	B-DeepLearning
dataset	I-DeepLearning
which	O
consists	O
40	O
BECT	O
instances	O
and	O
40	O
control	O
instances	O
.	O
Therefore	O
,	O
we	O
construct	O
a	O
traditional	O
2D	B-DeepLearning
convolution	I-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
2DCNN	B-DeepLearning
as	O
the	O
baseline	O
model	B-DeepLearning
for	O
BECT	O
prediction	O
.	O
The	O
convolution	O
kernel	B-DeepLearning
sizes	I-DeepLearning
of	O
2DCNN	B-DeepLearning
are	O
set	O
the	O
same	O
3DCNN	B-DeepLearning
.	O
And	O
the	O
stride	B-DeepLearning
of	O
2DCNN	B-DeepLearning
is	O
modified	O
to	O
get	O
a	O
full	O
convolution	O
architecture	O
which	O
reduces	O
the	O
number	O
of	O
weights	B-DeepLearning
and	O
overcomes	O
the	O
overfitting	B-DeepLearning
problem	O
.	O
However	O
,	O
3DCNN	B-DeepLearning
extends	O
one	O
more	O
dimension	O
as	O
the	O
data	O
channel	O
dimension	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
detailed	O
comparison	O
of	O
2DCNN	B-DeepLearning
and	O
3DCNN	B-DeepLearning
are	O
shown	O
in	O
Table	O
1	O
.	O
We	O
evaluate	O
3DCNN	B-DeepLearning
on	O
our	O
MRI	B-DeepLearning
dataset	I-DeepLearning
in	O
five-fold	B-DeepLearning
cross-validation	I-DeepLearning
.	O
The	O
whole	O
80-cases	O
dataset	B-DeepLearning
is	O
divided	O
into	O
training	B-DeepLearning
dataset	I-DeepLearning
54	B-DeepLearning
instances	I-DeepLearning
and	O
testing	B-DeepLearning
dataset	I-DeepLearning
16	B-DeepLearning
instances	I-DeepLearning
,	O
and	O
the	O
evaluation	O
criterion	O
is	O
the	O
prediction	O
accuracy	B-DeepLearning
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
is	O
set	O
to	O
1e	O
−	O
4	O
under	O
Adam	B-DeepLearning
optimizer	I-DeepLearning
.	O
Table	O
2	O
shows	O
the	O
performance	O
of	O
3DCNN	B-DeepLearning
and	O
2DCNN	B-DeepLearning
on	O
the	O
test	B-DeepLearning
dataset	I-DeepLearning
.	O
The	O
result	O
shows	O
3DCNN	B-DeepLearning
achieving	O
a	O
prediction	O
accuracy	B-DeepLearning
of	O
89.80%	O
on	O
the	O
five-fold	B-DeepLearning
cross-validation	I-DeepLearning
evaluation	O
,	O
which	O
outperforms	O
the	O
2DCNN	B-DeepLearning
baseline	O
model	B-DeepLearning
8250%	O
.	O
Finally	O
,	O
the	O
dataset	B-DeepLearning
for	O
model	B-DeepLearning
contains	O
32	O
independent	O
variables	O
,	O
1	O
dependent	O
variable	O
,	O
and	O
1	O
time	O
index	O
.	O
We	O
test	O
4	O
different	O
LSTM	B-DeepLearning
models:	I-DeepLearning
large/small	O
sample	O
size	O
that	O
the	O
predictor	O
variables	O
come	O
from	O
air	O
pollution	O
,	O
temperature	O
change	O
,	O
and	O
google	O
trends	O
datasets	B-DeepLearning
large/small	O
sample	O
size	O
that	O
only	O
use	O
google	O
trends	O
data	O
as	O
predictor	O
variables	O
.	O
The	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
was	O
equal	O
to	O
the	O
number	O
of	O
input	B-DeepLearning
data	I-DeepLearning
points	O
for	O
each	O
attribute	O
.	O
The	O
optimal	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
was	O
determined	O
through	O
experimentation	O
for	O
minimizing	O
the	O
error	O
at	O
the	O
best	O
epoch	B-DeepLearning
for	O
each	O
network	O
individually	O
.	O
An	O
upper	O
limit	O
for	O
the	O
total	O
number	B-DeepLearning
of	I-DeepLearning
weight	I-DeepLearning
connections	I-DeepLearning
was	O
set	O
to	O
half	O
of	O
the	O
total	O
number	O
of	O
input	B-DeepLearning
vectors	I-DeepLearning
to	O
avoid	O
overfitting	B-DeepLearning
,	O
as	O
suggested	O
previously	O
Andrea	O
and	O
Kalayeh	O
,	O
1991	O
.	O
During	O
predictions	O
,	O
the	O
network	O
was	O
fed	O
with	O
new	O
data	O
from	O
the	O
sequences	O
that	O
were	O
not	O
part	O
of	O
training	B-DeepLearning
set	I-DeepLearning
.	O
At	O
this	O
threshold	O
value	O
of	O
Pad	O
,	O
the	O
Matthew's	B-DeepLearning
correlation	I-DeepLearning
coefficient	I-DeepLearning
was	O
observed	O
to	O
be	O
highest	O
094	O
.	O
The	O
first	O
neural	B-DeepLearning
network	I-DeepLearning
NN1	B-DeepLearning
predicts	O
the	O
residue	O
contact	O
number	O
on	O
the	O
basis	O
of	O
the	O
PSSM	O
weights	O
.	O
The	O
structure	O
of	O
the	O
NN1	B-DeepLearning
is	O
as	O
follows	O
.	O
The	O
PSSM	O
values	O
are	O
the	O
input	B-DeepLearning
parameters	I-DeepLearning
.	O
The	O
complete	O
vector	B-DeepLearning
of	O
the	O
input	B-DeepLearning
data	I-DeepLearning
for	O
the	O
NN1	B-DeepLearning
thus	O
composed	O
of	O
2/z	O
+	O
1	O
x	O
1	O
=	O
11	O
x	O
21	O
=231	O
parameters	B-DeepLearning
.	O
Two	B-DeepLearning
internal	I-DeepLearning
layers	I-DeepLearning
with	O
50	O
and	O
3	B-DeepLearning
neurons	I-DeepLearning
were	O
implied	O
for	O
the	O
NN1	B-DeepLearning
.	O
A	O
single	O
number	O
was	O
at	O
the	O
NN1	B-DeepLearning
output	O
,	O
the	O
predicted	O
value	O
of	O
the	O
contact	O
number	O
of	O
the	O
rth	O
residue	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
at	O
the	O
second	O
level	O
NN2	B-DeepLearning
was	O
built	O
as	O
follows	O
.	O
The	O
contact	O
numbers	O
predicted	O
by	O
NN1	B-DeepLearning
served	O
as	O
its	O
input	B-DeepLearning
data	I-DeepLearning
41	O
values	O
were	O
estimated	O
for	O
each	O
position	O
,	O
namely	O
,	O
the	O
predicted	O
contact	O
numbers	O
at	O
the	O
/th	O
and	O
rth	O
±	O
20	O
positions	O
.	O
In	O
this	O
way	O
,	O
42	B-DeepLearning
parameters	I-DeepLearning
were	O
at	O
the	O
NN2	B-DeepLearning
input	O
the	O
NN2	B-DeepLearning
contained	O
one	O
internal	O
layer	O
of	O
10	B-DeepLearning
neurons	I-DeepLearning
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
of	O
the	O
algorithm	O
was	O
implemented	O
on	O
a	O
sample	O
of	O
the	O
234	O
monomeric	O
protein	O
chains	O
with	O
known	O
spatial	O
structures	O
extracted	O
from	O
the	O
PDB	B-DeepLearning
database	I-DeepLearning
Berman	O
et	O
al.	O
,	O
2000	O
and	O
belonging	O
to	O
different	O
protein	O
fold	O
types	O
according	O
to	O
the	O
SCOP	O
classification	O
Andreeva	O
et	O
al.	O
,	O
2004	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
was	O
trained	O
on	O
the	O
training	O
sample	O
by	O
the	O
backpropagation	B-DeepLearning
algorithm	O
Rumelhart	O
et	O
al.	O
,	O
1986	O
,	O
with	O
50	B-DeepLearning
epochs	I-DeepLearning
,	O
momentum	B-DeepLearning
of	I-DeepLearning
0.9	I-DeepLearning
,	O
and	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
001	O
.	O
The	O
prediction	O
accuracy	B-DeepLearning
was	O
estimated	O
on	O
the	O
testing	B-DeepLearning
data	I-DeepLearning
.	O
The	O
second	O
level	O
neural	B-DeepLearning
network	I-DeepLearning
NN2	I-DeepLearning
enables	O
the	O
improvement	O
of	O
prediction	O
accuracy	B-DeepLearning
.	O
NN2	B-DeepLearning
was	O
introduced	O
to	O
additionally	O
take	O
into	O
consideration	O
the	O
interdependence	O
of	O
the	O
contact	O
number	O
of	O
residues	O
neighboring	O
along	O
the	O
protein	O
chain	O
.	O
The	O
NN2	B-DeepLearning
introduces	O
,	O
although	O
slight	O
,	O
yet	O
regular	O
improvement	O
in	O
prediction	O
accuracy	B-DeepLearning
.	O
Similarly	O
to	O
VL-XT	O
,	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
of	O
ten	B-DeepLearning
neurons	I-DeepLearning
was	O
trained	O
on	O
the	O
specific	O
datasets	B-DeepLearning
and	O
it	O
outputs	O
a	O
value	O
for	O
the	O
central	O
amino	O
acid	O
in	O
the	O
window	O
.	O
Using	O
an	O
original	O
approach	O
,	O
RONN	B-DeepLearning
Regional	B-DeepLearning
Order	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
[	O
82	O
]	O
recognizes	O
disordered	O
segments	O
based	O
on	O
their	O
similarity	O
to	O
well-characterized	O
prototype	O
sequences	O
with	O
known	O
disordered	O
status	O
.	O
The	O
resulting	O
homology	O
scores	O
are	O
converted	O
into	O
distances	O
and	O
are	O
used	O
to	O
train	O
a	O
modified	O
version	O
of	O
radial	O
basis	O
function	O
networks	O
called	O
a	O
bio-basis	B-DeepLearning
function	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
.	O
Seven	O
physical	B-DeepLearning
parameters	I-DeepLearning
describing	O
the	O
physicochemical	O
properties	O
of	O
the	O
residue:	O
a	O
steric	O
parametergraph	O
shape	O
index	O
,	O
hydrophobicity	O
,	O
volume	O
,	O
polarizability	O
,	O
isoelectric	O
point	O
,	O
helix	O
probability	O
,	O
and	O
sheet	O
probability	O
.	O
Two	O
hidden	O
layers	O
,	O
each	O
with	O
71	B-DeepLearning
nodes	I-DeepLearning
,	O
were	O
used	O
for	O
a	O
preprocessing	O
network	O
.	O
Probability	O
predictions	O
from	O
this	O
network	O
were	O
further	O
refined	O
using	O
a	O
filter	O
network	O
with	O
a	O
single	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
51	B-DeepLearning
nodes	I-DeepLearning
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
for	O
all	O
neural	B-DeepLearning
networks	I-DeepLearning
considered	O
here	O
was	O
preformed	O
on	O
the	O
SPINE	B-DeepLearning
dataset	I-DeepLearning
[	O
18	O
]	O
.	O
In	O
general	O
the	O
networks	O
used	O
to	O
predict	O
ψ	O
angles	O
were	O
composed	O
of	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
,	O
each	O
with	O
51	B-DeepLearning
nodes	I-DeepLearning
.	O
Ten	B-DeepLearning
fold	I-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
will	O
be	O
used	O
to	O
judge	O
the	O
accuracy	B-DeepLearning
of	O
the	O
predictions	O
.	O
We	O
use	O
a	O
bipolar	O
activation	B-DeepLearning
function	I-DeepLearning
given	O
by	O
f	O
x	O
=	O
tanhαx	O
,	O
with	O
α	O
=	O
0.2	O
,	O
momentum	B-DeepLearning
of	I-DeepLearning
0.4	I-DeepLearning
,	O
and	O
the	O
back-propagation	B-DeepLearning
method	O
with	O
relatively	O
slow	O
learning	O
learning	B-DeepLearning
rate	I-DeepLearning
0.001	O
to	O
optimize	O
the	O
weights	B-DeepLearning
.	O
To	O
determine	O
the	O
quality	O
of	O
the	O
prediction	O
we	O
use	O
the	O
Mean	B-DeepLearning
Absolute	I-DeepLearning
Error	I-DeepLearning
MAE	B-DeepLearning
in	O
degrees	O
,	O
Pearson’s	O
correlation	O
coefficient	O
pc	O
,	O
and	O
the	O
probability	O
that	O
the	O
predicted	O
and	O
native	O
angles	O
are	O
separated	O
by	O
less	O
than	O
10%	O
Q10p	O
.	O
We	O
use	O
10-fold	B-DeepLearning
cross-validations	I-DeepLearning
[	O
35	O
]	O
to	O
estimate	O
the	O
accuracy	O
over	O
the	O
set	O
.	O
To	O
test	O
for	O
possible	O
overfit	B-DeepLearning
issues	O
we	O
take	O
secondary	O
structure	O
predictions	O
based	O
on	O
the	O
weights	B-DeepLearning
trained	O
for	O
the	O
first	O
cross-validation	B-DeepLearning
fold	O
and	O
compare	O
angle	O
prediction	O
between	O
the	O
folds	O
.	O
Firpi	O
et	O
al.	O
2010	O
developed	O
a	O
method	O
called	O
CSI-ANN	O
based	O
on	O
a	O
time-delayed	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
TDNN	B-DeepLearning
framework	O
to	O
predict	O
enhancers	O
in	O
HeLa	O
and	O
Human	O
CD4+	O
T	O
cells	O
.	O
A	O
model	B-DeepLearning
particularly	O
well	O
suited	O
for	O
the	O
tasks	O
of	O
interest	O
is	O
the	O
Bidirectional	B-DeepLearning
Recurrent	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
BRNN	B-DeepLearning
[	O
6	O
]	O
.	O
The	O
model	B-DeepLearning
they	O
used	O
is	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
Multi-Layer	B-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
with	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
[	O
4	O
]	O
Haykin	O
.	O
They	O
randomly	O
weighted	O
each	O
input	O
and	O
used	O
back-propagation	B-DeepLearning
[	O
4	O
]	O
to	O
train	O
the	O
system	O
on	O
a	O
PDB	O
file	O
chosen	O
from	O
the	O
PDB	O
.	O
Moreover	O
,	O
owing	O
to	O
the	O
rapid	O
growth	O
of	O
the	O
PDB	B-DeepLearning
database	I-DeepLearning
,	O
the	O
training	B-DeepLearning
sets	I-DeepLearning
for	O
ANNs	B-DeepLearning
have	O
also	O
increased	O
.	O
Since	O
these	O
artificial	O
intelligence	O
methods	O
are	O
based	O
on	O
an	O
empirical	O
risk-minimization	O
principle	O
,	O
they	O
have	O
some	O
disadvantages	O
,	O
for	O
example	O
,	O
finding	O
the	O
local	B-DeepLearning
minimal	I-DeepLearning
instead	O
of	O
global	B-DeepLearning
minimal	I-DeepLearning
having	O
low	O
convergence	B-DeepLearning
rate	O
more	O
prone	O
to	O
overfitting	B-DeepLearning
and	O
when	O
the	O
size	O
of	O
fault	O
samples	O
is	O
limited	O
,	O
it	O
might	O
cause	O
poor	O
generalization	B-DeepLearning
.	O
As	O
a	O
collaborative	O
filter	O
,	O
we	O
use	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
GRNN	B-DeepLearning
.	O
In	O
the	O
literature	O
,	O
GRNN	B-DeepLearning
was	O
shown	O
to	O
be	O
effective	O
in	O
noisy	O
environments	O
as	O
it	O
deals	O
with	O
sparse	B-DeepLearning
data	I-DeepLearning
effectively	O
.	O
The	O
method	O
makes	O
use	O
of	O
RDKit	B-DeepLearning
,	O
an	O
open-source	O
cheminformatics	O
software	O
,	O
allowing	O
the	O
incorporation	O
of	O
any	O
global	O
molecular	O
such	O
as	O
molecular	O
charge	O
and	O
local	O
such	O
as	O
atom	O
type	O
information	O
.	O
We	O
evaluate	O
the	O
method	O
on	O
the	O
Side	B-DeepLearning
Effect	I-DeepLearning
Resource	I-DeepLearning
SIDE	I-DeepLearning
v4.1	I-DeepLearning
dataset	I-DeepLearning
and	O
show	O
that	O
it	O
significantly	O
outperforms	O
another	O
recently	O
proposed	O
method	O
based	O
on	O
deep	O
convolutional	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
.	O
Surprisingly	O
,	O
while	O
being	O
one	O
of	O
the	O
best	O
performing	O
methods	O
on	O
all	O
other	O
datasets	B-DeepLearning
in	O
[	O
19	O
]	O
their	O
presented	O
DCNN	B-DeepLearning
performed	O
very	O
poorly	O
on	O
the	O
SIDER	B-DeepLearning
dataset	I-DeepLearning
and	O
were	O
outperformed	O
by	O
traditional	O
methods	O
such	O
as	O
random	O
forest	O
and	O
logistic	B-DeepLearning
regression	I-DeepLearning
.	O
Several	O
authors	O
overcome	O
this	O
flaw	O
by	O
either	O
using	O
a	O
recurrent	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
RNN	B-DeepLearning
or	O
a	O
DCNN	B-DeepLearning
.	O
Wallach	O
et	O
al.	O
[	O
17	O
]	O
for	O
example	O
apply	O
a	O
three	O
dimensional	O
DCNN	B-DeepLearning
on	O
the	O
spatial	O
structure	O
of	O
molecules	O
.	O
Due	O
to	O
the	O
conventions	O
of	O
the	O
field	O
,	O
F	O
will	O
be	O
defined	O
as	O
the	O
leaky	B-DeepLearning
ReLU	I-DeepLearning
function	O
[	O
12	O
]	O
in	O
the	O
rest	O
of	O
this	O
paper	O
.	O
The	O
H	O
in	O
Eq.	O
5	O
represents	O
an	O
arbitrary	O
activation	B-DeepLearning
function	I-DeepLearning
.	O
In	O
this	O
paper	O
the	O
Leaky	B-DeepLearning
ReLU	I-DeepLearning
function	O
will	O
be	O
used	O
for	O
Hl	O
except	O
in	O
the	O
final	B-DeepLearning
layer	I-DeepLearning
where	O
the	O
sigmoid	B-DeepLearning
function	O
will	O
be	O
used	O
instead	O
.	O
The	O
sigmoid	B-DeepLearning
function	O
is	O
selected	O
here	O
since	O
we	O
want	O
the	O
output	O
of	O
our	O
model	B-DeepLearning
to	O
be	O
in	O
the	O
range	O
of	O
0	O
to	O
1	O
.	O
To	O
train	O
and	O
test	O
our	O
proposed	O
model	B-DeepLearning
we	O
use	O
the	O
publicly	O
available	O
SIDER	B-DeepLearning
dataset	I-DeepLearning
presented	O
in	O
[	O
19	O
]	O
.	O
This	O
dataset	B-DeepLearning
contains	O
information	O
on	O
molecules	O
,	O
marketed	O
as	O
medicines	O
,	O
and	O
their	O
recorded	O
side	O
effects	O
.	O
The	O
data	O
originates	O
from	O
the	O
SIDER	B-DeepLearning
database	I-DeepLearning
.	O
The	O
original	O
data	O
consist	O
of	O
1430	B-DeepLearning
molecules	I-DeepLearning
and	O
5868	B-DeepLearning
different	I-DeepLearning
types	I-DeepLearning
of	I-DeepLearning
side	I-DeepLearning
effects	I-DeepLearning
.	O
However	O
,	O
in	O
the	O
dataset	B-DeepLearning
presented	O
by	O
Wu	O
et	O
al.	O
[	O
19	O
]	O
,	O
similar	O
side	O
effects	O
are	O
grouped	O
together	O
,	O
leaving	O
the	O
dataset	B-DeepLearning
with	O
28	O
groups	O
of	O
side	O
effects	O
.	O
RDKit	B-DeepLearning
is	O
also	O
used	O
to	O
extract	O
information	O
about	O
the	O
atoms	O
,	O
bonds	O
and	O
molecules	O
,	O
such	O
as	O
chirality	O
and	O
molecular	O
weight	O
.	O
The	O
model	B-DeepLearning
described	O
earlier	O
in	O
Sect.	O
3	O
is	O
implemented	O
in	O
Theano	B-DeepLearning
[	O
15	O
]	O
and	O
with	O
a	O
network	B-DeepLearning
architecture	I-DeepLearning
that	O
is	O
as	O
similar	O
as	O
possible	O
as	O
the	O
architecture	O
presented	O
by	O
Wu	O
et	O
al.	O
[	O
19	O
]	O
.	O
Two	B-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
,	O
each	O
with	O
64	B-DeepLearning
neurons	I-DeepLearning
,	O
will	O
therefore	O
be	O
used	O
.	O
These	O
layers	O
are	O
then	O
followed	O
by	O
a	O
single	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
,	O
with	O
128	B-DeepLearning
neurons	I-DeepLearning
.	O
Between	O
each	O
layer	O
in	O
our	O
model	B-DeepLearning
we	O
use	O
a	O
10%	O
dropout	B-DeepLearning
rate	O
[	O
14	O
]	O
.	O
The	O
models	B-DeepLearning
are	O
trained	O
by	O
minimizing	O
the	O
cross	B-DeepLearning
entropy	I-DeepLearning
error	I-DeepLearning
.	O
This	O
is	O
achieved	O
by	O
optimizing	O
the	O
values	O
of	O
all	O
free	O
parameters	B-DeepLearning
Wl	O
and	O
V	O
l	O
using	O
the	O
ADAM	B-DeepLearning
optimization	O
algorithm	O
[	O
7	O
]	O
.	O
Each	O
network	O
is	O
trained	O
for	O
200	B-DeepLearning
epochs	I-DeepLearning
using	O
a	O
batch	B-DeepLearning
size	I-DeepLearning
of	O
20	O
examples	O
.	O
The	O
performance	O
of	O
our	O
model	B-DeepLearning
on	O
the	O
presented	O
experimental	O
set-ups	O
is	O
measured	O
as	O
the	O
mean	O
AUC-ROC	B-DeepLearning
value	O
,	O
averaged	O
over	O
all	O
trials	O
and	O
all	O
target	O
variables	O
.	O
The	O
network	O
mainly	O
consists	O
of	O
three	B-DeepLearning
convolution	I-DeepLearning
blocks	I-DeepLearning
and	O
two	B-DeepLearning
fully-connected	I-DeepLearning
blocks	I-DeepLearning
,	O
as	O
well	O
as	O
flattening	O
and	O
concatenating	O
layers	O
that	O
bridge	O
the	O
two	O
types	O
of	O
blocks	O
.	O
Each	O
convolution	B-DeepLearning
block	I-DeepLearning
is	O
composed	O
of	O
a	O
convolution	B-DeepLearning
layer	I-DeepLearning
with	O
ReLU	B-DeepLearning
activation	O
,	O
followed	O
by	O
a	O
max	O
pooling	O
layer	O
,	O
and	O
a	O
dropout	O
layer	O
.	O
The	O
first	O
convolution	B-DeepLearning
layer	I-DeepLearning
filters	O
the	O
input	O
of	O
size	O
1280	O
×	O
1024	O
×	O
2	O
using	O
16	O
kernels	B-DeepLearning
of	O
size	O
10	O
×	O
10	O
with	O
a	O
horizontal	O
stride	B-DeepLearning
of	O
10	O
pixels	O
and	O
a	O
vertical	O
stride	B-DeepLearning
of	O
8	O
pixels	O
.	O
The	O
result	O
image	B-DeepLearning
of	O
size	O
128×128×16	O
is	O
then	O
shrunk	O
to	O
64×64×16	O
in	O
the	O
following	O
max	B-DeepLearning
pooling	I-DeepLearning
layer	O
.	O
The	O
second	O
convolution	B-DeepLearning
layer	I-DeepLearning
filters	O
it	O
using	O
32	O
kernels	B-DeepLearning
of	O
size	O
3	O
×	O
3	O
with	O
a	O
stride	B-DeepLearning
of	O
2	O
pixels	O
.	O
Through	O
a	O
similar	O
process	O
,	O
the	O
input	O
of	O
the	O
third	O
convolution	B-DeepLearning
layer	I-DeepLearning
becomes	O
of	O
size	O
16×16×32	O
and	O
at	O
the	O
end	O
of	O
the	O
block	O
,	O
the	O
input	O
is	O
flattened	O
to	O
a	O
vector	B-DeepLearning
of	O
length	O
1024	O
from	O
4	O
×	O
4	O
×	O
64	O
.	O
The	O
vector	B-DeepLearning
is	O
then	O
concatenated	O
with	O
the	O
second	O
input	O
,	O
a	O
vector	B-DeepLearning
of	O
eight	O
metrics	O
,	O
and	O
delivered	O
to	O
a	O
fully-connected	B-DeepLearning
layer	I-DeepLearning
of	O
512	B-DeepLearning
neurons	I-DeepLearning
.	O
Finally	O
,	O
the	O
vector	B-DeepLearning
of	O
length	O
512	O
goes	O
through	O
a	O
sigmoid	B-DeepLearning
function	O
to	O
generate	O
the	O
output	O
.	O
We	O
conducted	O
the	O
self-assembly	O
experiment	B-DeepLearning
for	O
four	O
hours	O
and	O
recorded	O
images	B-DeepLearning
of	O
size	O
1280	O
×	O
1024	O
at	O
1	O
fps	O
,	O
leading	O
to	O
over	O
14000	O
pairs	O
of	O
front	O
and	O
right	O
images	B-DeepLearning
.	O
In	O
total	O
,	O
725	O
positive	O
real	O
pairs	O
,	O
2475	O
negative	O
real	O
pairs	O
,	O
12000	O
positive	O
synthetic	O
pairs	O
,	O
and	O
12000	O
negative	O
synthetic	O
pairs	O
are	O
utilized	O
in	O
our	O
experiment	B-DeepLearning
.	O
Note	O
that	O
the	O
presented	O
accuracies	O
are	O
the	O
mean	O
accuracy	O
over	O
10-fold	B-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
except	O
the	O
case	O
when	O
only	O
the	O
synthetic	O
data	O
is	O
used	O
as	O
a	O
training	B-DeepLearning
set	I-DeepLearning
.	O
Our	O
algorithm	O
reaches	O
an	O
accuracy	B-DeepLearning
of	O
93.2%	O
when	O
it	O
is	O
trained	O
with	O
labeled	O
real	O
images	B-DeepLearning
.	O
It	O
justifies	O
our	O
utilization	O
of	O
synthetic	O
data	O
,	O
as	O
it	O
recorded	O
88.7%	O
accuracy	B-DeepLearning
when	O
only	O
the	O
real	O
data	O
are	O
used	O
as	O
a	O
training	B-DeepLearning
set	I-DeepLearning
.	O
Our	O
classifier	O
also	O
shows	O
promising	O
results	O
with	O
synthetic	O
images	B-DeepLearning
,	O
reaching	O
an	O
accuracy	B-DeepLearning
of	O
86%	O
.	O
