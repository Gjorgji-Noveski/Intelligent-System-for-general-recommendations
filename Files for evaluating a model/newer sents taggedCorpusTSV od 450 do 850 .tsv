When	O
new	O
data	O
is	O
added	O
,	O
the	O
ANN	B-DeepLearning
needs	O
to	O
be	O
retrained	B-DeepLearning
in	O
order	O
to	O
achieve	O
good	O
performance	O
.	O
The	O
set	O
of	O
selected	O
features	O
are	O
processed	O
to	O
accomplish	O
the	O
voxel	O
classification	O
by	O
means	O
of	O
a	O
Multilevel	B-DeepLearning
Artificial	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
MANN	B-DeepLearning
,	O
which	O
assures	O
various	O
computational	O
advantages	O
.	O
Then	O
we	O
discuss	O
the	O
issues	O
concerning	O
the	O
minimum	O
requirements	O
that	O
an	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
should	O
fulfill	O
in	O
order	O
that	O
it	O
would	O
be	O
capable	O
of	O
expressing	O
the	O
categories	O
and	O
mappings	O
between	O
them	O
,	O
underlying	O
the	O
MES	O
.	O
First	O
generation	O
networks	O
studied	O
the	O
McCulloch-Pitts	O
neuron	O
,	O
or	O
perceptrons	B-DeepLearning
,	O
using	O
digital	O
inputs	O
and	O
outputs	O
,	O
usually	O
binary	O
.	O
Multi-layer	B-DeepLearning
perceptrons	I-DeepLearning
with	O
a	O
single	O
hidden	B-DeepLearning
layer	I-DeepLearning
were	O
able	O
to	O
compute	O
any	O
binary	O
function	O
.	O
Second	O
generation	O
networks	O
had	O
neurons	O
with	O
activation	B-DeepLearning
functions	I-DeepLearning
,	O
usually	O
some	O
logistic	B-DeepLearning
function	I-DeepLearning
like	O
the	O
sigmoid	B-DeepLearning
,	O
with	O
continuous	O
inputs	O
and	O
outputs	O
,	O
having	O
probabilistic	O
weights	B-DeepLearning
in	O
the	O
synapses	O
,	O
and	O
learning	O
algorithms	O
based	O
on	O
gradient	B-DeepLearning
descent.	I-DeepLearning
.	O
Third	O
generation	O
networks	O
are	O
based	O
on	O
spiking	B-DeepLearning
or	I-DeepLearning
pulsating	I-DeepLearning
neurons	I-DeepLearning
,	O
that	O
incorporate	O
recent	O
neurobiology	O
discoveries	O
,	O
modeling	O
the	O
neuron	O
as	O
a	O
dynamic	O
electrical	O
system	O
with	O
bifurcation	O
in	O
its	O
equilibrium	O
,	O
usually	O
following	O
some	O
variant	O
of	O
the	O
Hodgkin-Huxley	O
model	O
.	O
The	O
network	B-DeepLearning
was	I-DeepLearning
trained	I-DeepLearning
with	O
a	O
100-sentence	B-DeepLearning
database	I-DeepLearning
and	O
was	O
generated	O
by	O
a	O
Kohonen	O
network	O
with	O
60	B-DeepLearning
x	I-DeepLearning
60	I-DeepLearning
neurons	I-DeepLearning
and	O
a	O
decreasing	O
neighborhood	O
.	O
It	O
is	O
needed	O
for	O
instance	O
,	O
when	O
channels	O
used	O
in	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
have	O
different	O
lengths	O
or	O
if	O
the	O
duration	O
of	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
simulation	O
is	O
different	O
for	O
some	O
reason	O
.	O
This	O
gives	O
an	O
accuracy	B-DeepLearning
rate	I-DeepLearning
of	O
0627	O
.	O
The	O
accuracy	B-DeepLearning
of	O
the	O
system	O
is	O
calculated	O
as	O
the	O
total	O
number	O
of	O
correctly	O
classified	O
objects	O
1094	O
divided	O
by	O
the	O
total	O
number	O
of	O
objects	O
1627	O
.	O
Neural	B-DeepLearning
networks	I-DeepLearning
are	O
generally	O
better	O
at	O
discriminating	B-DeepLearning
between	I-DeepLearning
classes	I-DeepLearning
,	O
as	O
is	O
shown	O
here	O
.	O
The	O
whole	O
data	B-DeepLearning
set	I-DeepLearning
was	O
divided	O
into	O
different	O
parts	O
in	O
order	O
to	O
get	O
different	O
validation	O
data	B-DeepLearning
sets	I-DeepLearning
.	O
EXP-1	O
,	O
which	O
contains	B-DeepLearning
all	I-DeepLearning
data	I-DeepLearning
from	O
the	O
first	O
clinical	B-DeepLearning
experiment	I-DeepLearning
with	O
2500	B-DeepLearning
measurements	I-DeepLearning
.	O
Accuracy	B-DeepLearning
is	O
computed	O
as	O
the	O
percentage	O
of	O
cases	O
correctly	O
classified	O
.	O
If	O
yes	O
,	O
with	O
probability	O
0.76	O
this	O
is	O
classified	B-DeepLearning
as	O
a	O
cancerous	O
regions	O
,	O
otherwise	O
,	O
the	O
region	O
is	O
certainly	O
cancerous	O
.	O
Results	O
show	O
that	O
on	O
both	O
data	O
sets	O
the	O
certainty	O
for	O
the	O
correctly	O
classified	O
lesions	B-DeepLearning
is	O
higher	O
lower	O
entropy	O
than	O
for	O
the	O
incorrectly	O
classified	O
lesions	B-DeepLearning
.	O
According	O
to	O
[	O
25	O
]	O
,	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
research	O
evolved	O
into	O
three	O
generations	O
.	O
Under	O
normal	O
circumstances	O
,	O
however	O
,	O
accuracy	B-DeepLearning
is	O
determined	O
by	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
of	O
the	O
inferred	O
rule	O
ie	O
,	O
the	O
percentage	O
of	O
actions	O
incorrectly	B-DeepLearning
classified	I-DeepLearning
as	O
either	O
legitimate	O
or	O
illegitimate	O
calculated	O
over	O
the	O
entire	O
uniformly-sampled	B-DeepLearning
set	I-DeepLearning
of	O
action	O
possibilities	O
within	O
the	O
a	O
priori	O
motor	O
space	O
.	O
As	O
a	O
result	O
,	O
the	O
data	B-DeepLearning
set	I-DeepLearning
is	O
optimally	O
classified	B-DeepLearning
into	I-DeepLearning
two	I-DeepLearning
classes	I-DeepLearning
,	O
targets	O
and	O
non	O
targets	O
following	O
a	O
validation	O
process	O
,	O
and	O
an	O
average	O
accuracy	B-DeepLearning
metric	I-DeepLearning
is	O
then	O
calculated	O
.	O
This	O
set	B-DeepLearning
was	I-DeepLearning
divided	I-DeepLearning
into	O
2	O
groups:	O
training	B-DeepLearning
320	O
thermographies	O
in	O
each	O
class	O
and	O
testing	B-DeepLearning
80	O
thermographies	O
in	O
reach	O
class	O
.	O
Therefore	O
,	O
a	O
total	O
of	O
2.411	O
images	B-DeepLearning
of	O
healthy	B-DeepLearning
patients	B-DeepLearning
and	O
534	O
images	B-DeepLearning
of	O
diseased	B-DeepLearning
patients	B-DeepLearning
were	O
used	O
.	O
A	O
balanced	B-DeepLearning
randomized	I-DeepLearning
selection	I-DeepLearning
was	O
performed	O
on	O
500	O
healthy	B-DeepLearning
patients	B-DeepLearning
and	O
500	O
sick	B-DeepLearning
patients	B-DeepLearning
.	O
80%	O
400	B-DeepLearning
Healthy	I-DeepLearning
and	I-DeepLearning
400	I-DeepLearning
Sick	I-DeepLearning
were	B-DeepLearning
allocated	I-DeepLearning
to	I-DeepLearning
training	I-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
,	O
that	O
is	O
,	O
the	O
processes	O
of	O
extracting	O
the	O
attributes	O
and	O
optimizing	O
the	O
weights	B-DeepLearning
of	O
the	O
filters	B-DeepLearning
.	O
20%	O
100	O
thermographies	B-DeepLearning
in	O
each	O
class	O
were	B-DeepLearning
reserved	I-DeepLearning
for	I-DeepLearning
blind	I-DeepLearning
validation	I-DeepLearning
and	O
establishing	O
the	O
final	O
predictive	O
accuracy	B-DeepLearning
of	O
these	O
architectures	O
using	B-DeepLearning
a	I-DeepLearning
database	I-DeepLearning
of	O
images	B-DeepLearning
that	O
was	O
not	O
used	O
for	O
learning	O
training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
.	O
Data	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
comes	O
from	O
a	O
simulation	O
experiment	B-DeepLearning
.	O
Before	O
giving	O
more	O
in	O
depth	O
description	O
of	O
these	O
data	O
,	O
it	O
is	O
good	O
to	O
acknowledge	O
that	O
the	O
input	B-DeepLearning
of	I-DeepLearning
a	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consists	O
of	O
several	O
cell	O
positions	O
and	O
output	O
is	O
the	O
velocity	O
of	O
that	O
cell	O
at	O
a	O
particular	O
time	O
.	O
Data	B-DeepLearning
for	I-DeepLearning
this	I-DeepLearning
study	I-DeepLearning
were	O
recorded	B-DeepLearning
every	I-DeepLearning
0.001	I-DeepLearning
s	I-DeepLearning
once	O
in	O
1000	O
simulation	B-DeepLearning
steps	I-DeepLearning
,	O
which	O
results	O
in	O
almost	O
9	O
200	O
records	B-DeepLearning
for	O
each	O
cell	O
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
sets	I-DeepLearning
used	I-DeepLearning
for	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
are	I-DeepLearning
extracted	I-DeepLearning
from	I-DeepLearning
simulations	I-DeepLearning
which	O
differ	O
only	O
in	O
initial	B-DeepLearning
seeding	I-DeepLearning
of	O
the	O
cells	O
.	O
As	O
was	O
already	O
mentioned	O
,	O
the	O
input	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
are	O
the	O
positions	O
of	O
the	O
cell	O
center	O
and	O
the	O
output	O
is	O
the	O
velocity	O
of	O
this	O
cell	O
at	O
given	O
position	O
.	O
The	O
learning	B-DeepLearning
set	I-DeepLearning
for	I-DeepLearning
our	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consist	O
of	O
the	O
simulation	O
output	O
which	O
also	O
includes	O
redundant	B-DeepLearning
data	I-DeepLearning
.	O
We	O
processed	B-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
in	O
the	O
following	O
steps	O
.	O
We	O
loaded	B-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
from	O
the	O
experiments	O
and	O
checked	O
their	O
correctness	O
.	O
Next	O
step	O
was	O
the	O
normalization	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
.	O
Normalization	B-DeepLearning
of	I-DeepLearning
data	I-DeepLearning
is	O
common	O
in	O
neural	O
networks.	O
.	O
We	O
made	O
the	O
desired	O
pairs	O
of	O
the	O
inputs	O
and	O
the	O
outputs	O
from	O
the	O
normalized	B-DeepLearning
data	I-DeepLearning
.	O
We	O
work	O
with	O
two	O
types	O
of	O
the	O
input	O
tensor	B-DeepLearning
in	I-DeepLearning
the	I-DeepLearning
form	I-DeepLearning
of	I-DeepLearning
a	I-DeepLearning
3âˆ’dimensional	I-DeepLearning
array	I-DeepLearning
.	O
The	O
horizontal	O
layers	O
of	O
the	O
tensor	B-DeepLearning
represent	O
different	O
cells	O
,	O
where	O
the	O
cell	O
we	O
are	O
focused	O
on	O
is	O
on	O
the	O
top	O
of	O
the	O
tensor	B-DeepLearning
.	O
For	O
our	O
tensorÑ“	B-DeepLearning
we	I-DeepLearning
used	I-DeepLearning
zero	I-DeepLearning
padding	I-DeepLearning
of	I-DeepLearning
width	I-DeepLearning
p	I-DeepLearning
.	O
Thus	O
the	O
dimension	B-DeepLearning
of	I-DeepLearning
this	I-DeepLearning
tensor	I-DeepLearning
is:	O
.	O
This	O
input	O
type	O
for	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
set	O
by	O
parameter	B-DeepLearning
called	O
spatial	B-DeepLearning
tensor	I-DeepLearning
is	O
inspired	O
by	O
the	O
image	B-DeepLearning
processing	O
.	O
We	O
obtain	O
a	O
small	O
tensor	B-DeepLearning
A	O
by	O
determining	O
if	O
the	O
cell	O
is	O
present	O
or	O
not	O
,	O
in	O
each	O
of	O
the	O
disc	O
x	O
Ã—	O
disc	O
y	O
Ã—	O
disc	O
z	O
channel	O
areas	O
.	O
Since	O
this	O
input	B-DeepLearning
is	I-DeepLearning
orthogonal	I-DeepLearning
the	O
neuron	B-DeepLearning
networks	I-DeepLearning
gives	O
us	O
more	O
accurate	O
output	O
values	O
.	O
Each	O
tensor	B-DeepLearning
records	O
the	O
position	O
of	O
the	O
centre	O
of	O
one	O
cell	O
.	O
Then	O
the	O
input	B-DeepLearning
tensor	I-DeepLearning
consists	O
of	O
small	O
tensors	B-DeepLearning
An...A1	O
,	O
Bn...B1	O
in	O
this	O
order	O
.	O
The	O
small	O
tensor	B-DeepLearning
An	O
is	O
at	O
the	O
top	O
horizontal	O
layer	O
of	O
the	O
input	O
tensor	B-DeepLearning
and	O
B1	O
is	O
at	O
the	O
bottom	O
layer	O
of	O
the	O
input	O
tensor	B-DeepLearning
.	O
Therefore	O
the	O
dimension	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
input	I-DeepLearning
tensor	I-DeepLearning
is:	O
.	O
We	O
also	O
used	O
the	O
gaussian	B-DeepLearning
kernel	I-DeepLearning
.	O
Hence	O
,	O
the	O
dimension	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
input	I-DeepLearning
tensor	I-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
was	I-DeepLearning
16	I-DeepLearning
Ã—	I-DeepLearning
16	I-DeepLearning
Ã—	I-DeepLearning
24	I-DeepLearning
.	O
Data	B-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
input	I-DeepLearning
tensors	I-DeepLearning
are	O
selected	O
as	O
follows	O
.	O
Besides	O
using	O
convolution	B-DeepLearning
or	O
fully	B-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
,	O
we	O
also	O
used	O
a	O
relatively	O
new	O
type	O
of	O
layers	O
,	O
the	O
dense	B-DeepLearning
convolution	I-DeepLearning
layers	I-DeepLearning
,	O
introduced	O
in	O
[	O
7	O
]	O
.	O
There	O
were	O
6	O
different	B-DeepLearning
architectures	I-DeepLearning
of	O
neural	O
networks	O
,	O
see	O
the	O
Table	O
2	O
.	O
First	O
four	O
experiments	B-DeepLearning
,	O
named	O
Experiment	B-DeepLearning
0	O
to	O
Experiment	B-DeepLearning
3	O
have	O
no	O
spatial	O
tensor	B-DeepLearning
input	O
with	O
the	O
same	O
parameters	B-DeepLearning
,	O
but	O
mutually	O
different	O
networks	B-DeepLearning
architectures	I-DeepLearning
.	O
The	O
network	B-DeepLearning
architectures	I-DeepLearning
for	O
experiment	B-DeepLearning
pairs	O
2	O
and	O
4	O
and	O
for	O
3	O
and	O
5	O
are	O
the	O
same	O
.	O
Comparing	O
the	O
difference	O
between	O
the	O
target	O
and	O
resulted	O
values	O
given	O
from	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
we	O
observed	O
that	O
networks	O
with	O
spatial	O
tensor	B-DeepLearning
parameter	B-DeepLearning
Experiments	B-DeepLearning
4	O
to	O
7	O
give	O
better	O
results	O
than	O
for	O
no	O
spatial	O
tensor	B-DeepLearning
parameter	B-DeepLearning
.	O
The	O
Experiment	B-DeepLearning
7	O
,	O
where	B-DeepLearning
the	I-DeepLearning
network	I-DeepLearning
has	I-DeepLearning
the	I-DeepLearning
most	I-DeepLearning
layers	I-DeepLearning
gave	I-DeepLearning
us	I-DeepLearning
the	I-DeepLearning
best	I-DeepLearning
result	I-DeepLearning
.	O
In	O
this	O
section	O
we	O
report	O
on	O
a	O
set	O
of	O
experiments	B-DeepLearning
performed	O
on	O
a	O
large	B-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
of	O
nanopore	O
FASTQ	O
files	O
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
is	I-DeepLearning
comprised	I-DeepLearning
of	I-DeepLearning
336	I-DeepLearning
different	I-DeepLearning
files	I-DeepLearning
,	O
with	O
sizes	O
ranging	O
from	O
7.2	O
KB	O
to	O
3.5	O
GB	O
,	O
including	O
reads	O
that	O
are	O
up	O
to	O
hundreds	O
of	O
thousands	O
base-pair	O
long	O
.	O
The	O
total	O
size	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
amounts	I-DeepLearning
to	I-DeepLearning
114.2	I-DeepLearning
GB	I-DeepLearning
and	O
the	O
dynamic	O
range	O
of	O
quality	O
scores	O
is	O
7	O
bits	O
.	O
Among	O
these	O
methods	O
,	O
the	O
credibility	B-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
back	I-DeepLearning
propagation	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
is	I-DeepLearning
up	I-DeepLearning
to	I-DeepLearning
98%.	I-DeepLearning
.	O
To	O
address	O
the	O
two	O
issues	O
,	O
in	O
this	O
paper	O
we	O
propose	O
to	O
use	O
a	O
deep	B-DeepLearning
ResNet	I-DeepLearning
structure	I-DeepLearning
with	I-DeepLearning
Convolutional	I-DeepLearning
Block	I-DeepLearning
Attention	I-DeepLearning
Module	I-DeepLearning
CBAM	B-DeepLearning
,	O
in	O
order	O
to	O
extract	O
richer	O
and	O
finer	O
features	O
from	O
pathological	O
images.	B-DeepLearning
.	O
Existing	O
deep	B-DeepLearning
learning	I-DeepLearning
approaches	I-DeepLearning
for	I-DeepLearning
Breast	I-DeepLearning
Cancer	I-DeepLearning
BC	I-DeepLearning
histology	I-DeepLearning
images	I-DeepLearning
classification	I-DeepLearning
task	O
include	O
cell	O
nuclei	O
segmentation	O
[	O
1415	O
]	O
and	O
the	O
patch-wise	O
classification	O
[	O
1216	O
]	O
.	O
In	O
order	O
to	O
classify	O
the	O
BC	O
pathological	O
images	O
steadily	O
and	O
accurately	O
,	O
in	O
this	O
paper	O
,	O
we	O
adopt	O
an	O
improved	O
ResNet	B-DeepLearning
architecture	I-DeepLearning
to	O
extract	O
local	O
and	O
global	O
features	O
from	O
pathological	B-DeepLearning
images	I-DeepLearning
and	O
perform	O
end-to-end	B-DeepLearning
training.	I-DeepLearning
.	O
ResNet	B-DeepLearning
[	O
17	O
]	O
is	O
a	O
recently	O
popular	O
CNN	B-DeepLearning
architecture	I-DeepLearning
,	O
it	O
prevents	O
vanishing	O
gradient	O
by	O
using	O
the	O
residual	O
connection	O
,	O
which	O
allows	O
the	O
network	O
architecture	O
to	O
be	O
deeper	O
to	O
obtain	O
richer	O
and	O
more	O
abstract	O
features	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
Convolutional	B-DeepLearning
Block	I-DeepLearning
Attention	I-DeepLearning
Module	I-DeepLearning
CBAM	B-DeepLearning
[	O
18	O
]	O
to	O
enhance	O
the	O
performance	O
of	O
the	O
ResNet	B-DeepLearning
.	O
To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
method	O
,	O
we	O
conduct	O
a	O
series	O
of	O
experiments	O
on	O
the	O
publicly	O
available	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
.	O
The	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
,	O
a	O
benchmark	O
proposed	O
by	O
[	O
11	O
]	O
for	O
the	O
BC	O
histological	O
images	O
classification	O
,	O
consists	B-DeepLearning
of	I-DeepLearning
7909	I-DeepLearning
breast	I-DeepLearning
histopathological	I-DeepLearning
images	I-DeepLearning
from	O
82	B-DeepLearning
patients	I-DeepLearning
.	O
We	O
use	O
the	O
deeper	O
ResNet	B-DeepLearning
CNN	I-DeepLearning
architecture	I-DeepLearning
to	O
extract	O
richer	O
and	O
more	O
abstract	O
features	O
of	O
patientsâ€™	B-DeepLearning
BC	I-DeepLearning
tissue	I-DeepLearning
.	O
We	O
use	O
CABM	B-DeepLearning
to	O
refine	O
the	O
tissue	O
features	O
extracted	O
from	O
each	O
layer	O
of	O
ResNet	B-DeepLearning
,	O
which	O
can	O
improve	O
the	O
classification	O
performance	O
.	O
We	O
have	O
significantly	O
improved	O
the	O
accuracy	O
of	O
classification	O
on	O
the	O
publicly	O
available	O
BreakHis	B-DeepLearning
Dataset	I-DeepLearning
.	O
Fortunately	O
,	O
Breast	B-DeepLearning
cancer	I-DeepLearning
histopathology	I-DeepLearning
database	I-DeepLearning
provided	I-DeepLearning
7909	I-DeepLearning
2480	I-DeepLearning
benign	I-DeepLearning
and	I-DeepLearning
5429	I-DeepLearning
malignant	I-DeepLearning
samples	I-DeepLearning
microscopic	O
images	O
of	O
breast	O
tumor	O
which	O
collected	O
from	O
82	B-DeepLearning
patients	I-DeepLearning
by	O
surgery	O
[	O
11	O
]	O
.	O
AlexNet	B-DeepLearning
is	O
a	O
typical	O
deep	B-DeepLearning
learning	I-DeepLearning
model	I-DeepLearning
,	O
which	O
[	O
22	O
]	O
achieves	O
a	O
winning	O
top-5	O
test	B-DeepLearning
error	I-DeepLearning
rate	I-DeepLearning
of	O
15.3%	O
,	O
which	O
is	O
10.9%	O
lower	O
than	O
the	O
second	O
one	O
who	O
uses	O
SIFT	O
[	O
23	O
]	O
and	O
FVS	O
[	O
24	O
]	O
.	O
The	O
Breast	B-DeepLearning
Cancer	I-DeepLearning
Histopathological	I-DeepLearning
Image	I-DeepLearning
Classification	I-DeepLearning
BreakHis	I-DeepLearning
dataset	I-DeepLearning
composes	O
of	O
7909	B-DeepLearning
microscopic	I-DeepLearning
images	I-DeepLearning
of	O
breast	O
tumor	O
tissue	O
.	O
They	O
are	O
collected	O
from	O
82	B-DeepLearning
patients	I-DeepLearning
in	O
different	O
magnifying	O
factors	O
include	O
40Ã—	O
,	O
100Ã—	O
,	O
200Ã—	O
,	O
and	O
400Ã—.	O
.	O
The	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
contains	O
2480	B-DeepLearning
benign	I-DeepLearning
and	I-DeepLearning
5429	I-DeepLearning
malignant	I-DeepLearning
samples	I-DeepLearning
700	O
Ã—	O
460	O
pixels	O
,	O
3-channel	O
RGB	O
,	O
8-bit	O
depth	O
in	O
each	O
channel	O
,	O
PNG	O
format	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
the	O
ResNet	B-DeepLearning
with	I-DeepLearning
50	I-DeepLearning
layers	I-DeepLearning
,	O
which	O
can	O
be	O
represented	O
by	O
ResNet-50	B-DeepLearning
.	O
Then	O
we	O
add	O
CBAM	B-DeepLearning
to	O
each	O
block	O
of	O
the	O
ResNet-50	B-DeepLearning
.	O
The	O
original	O
top	O
layer	B-DeepLearning
of	I-DeepLearning
ResNet-50	I-DeepLearning
is	I-DeepLearning
replaced	I-DeepLearning
by	I-DeepLearning
a	I-DeepLearning
global	I-DeepLearning
average-pooling	I-DeepLearning
layer	I-DeepLearning
.	O
A	O
dropout	B-DeepLearning
of	I-DeepLearning
0.3	I-DeepLearning
is	O
also	O
used	O
after	O
the	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
for	O
helping	O
reduce	O
overfitting	B-DeepLearning
.	O
The	O
loss	B-DeepLearning
is	I-DeepLearning
categorical	I-DeepLearning
cross-entropy	I-DeepLearning
and	O
the	O
optimizer	B-DeepLearning
is	O
Stochastic	B-DeepLearning
Gradient	I-DeepLearning
Descent	I-DeepLearning
SGD	B-DeepLearning
.	O
We	O
set	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
at	O
0001	O
.	O
We	O
pre-train	B-DeepLearning
our	O
model	O
on	O
the	O
ImageNet	B-DeepLearning
dataset	I-DeepLearning
,	O
and	O
then	O
fine-tune	B-DeepLearning
our	O
model	O
on	O
the	O
BreakHis	B-DeepLearning
dataset	I-DeepLearning
.	O
We	O
retrospectively	O
evaluate	O
our	O
model	O
on	O
the	O
data	O
of	O
142	B-DeepLearning
patients	I-DeepLearning
with	O
305	B-DeepLearning
lesions	I-DeepLearning
and	I-DeepLearning
70	I-DeepLearning
no-lesion	I-DeepLearning
volumes	O
,	O
and	O
it	O
significantly	O
outperforms	O
the	O
comparison	O
methods	O
with	O
the	O
sensitivity	O
of	O
92.1%	O
with	O
1.92	O
false	O
positives	O
per	O
volume	O
.	O
The	O
model	O
is	O
based	O
on	O
the	O
standard	O
UNet	B-DeepLearning
segmentation	I-DeepLearning
architecture	I-DeepLearning
with	O
a	O
down-sampling	O
and	O
upsampling	O
path	O
,	O
where	O
all	O
the	O
simply	O
stacked	O
convolutional	B-DeepLearning
layers	I-DeepLearning
are	O
replaced	O
with	O
residual	B-DeepLearning
blocks	I-DeepLearning
to	O
prevent	O
gradient	B-DeepLearning
vanishing	I-DeepLearning
.	O
We	O
combine	O
spatial	O
information	O
into	O
different	O
layers	O
of	O
UNet	B-DeepLearning
to	O
reduce	O
the	O
false	O
positive	O
rate	O
.	O
Finally	O
,	O
considering	O
the	O
blurred	O
boundaries	O
between	O
lesion	O
and	O
no-lesions	O
,	O
we	O
adopt	O
the	O
sub-hard	O
mining	O
strategy	O
in	O
loss	B-DeepLearning
function	I-DeepLearning
,	O
which	O
only	O
computes	O
the	O
loss	B-DeepLearning
of	O
the	O
specific	O
samples	O
to	O
update	O
the	O
parameter	B-DeepLearning
of	O
the	O
network	O
,	O
in	O
order	O
to	O
ensure	O
the	O
network	O
learn	O
correct	O
information	O
.	O
To	O
validate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
framework	O
,	O
we	O
conduct	O
extensive	O
experiments	O
on	O
an	O
ABUS	B-DeepLearning
dataset	I-DeepLearning
of	O
142	B-DeepLearning
patientsâ€™	I-DeepLearning
images	I-DeepLearning
with	O
375	B-DeepLearning
volumes	I-DeepLearning
.	O
We	O
introduce	O
the	O
spatial	B-DeepLearning
information	I-DeepLearning
into	I-DeepLearning
different	I-DeepLearning
layers	I-DeepLearning
of	O
the	O
segmentation	O
network	O
,	O
which	O
significantly	O
reduce	B-DeepLearning
the	I-DeepLearning
false	I-DeepLearning
positive	I-DeepLearning
rate	I-DeepLearning
.	O
Our	O
method	O
is	O
based	O
on	O
UNet	B-DeepLearning
.	O
The	O
image	O
resolution	O
reduces	O
by	O
half	O
after	O
max	B-DeepLearning
pooling	I-DeepLearning
along	O
the	O
downsampling	B-DeepLearning
path	I-DeepLearning
,	O
while	O
the	O
resolution	O
doubles	O
via	O
deconvolution	B-DeepLearning
operation	I-DeepLearning
along	O
the	O
upsampling	B-DeepLearning
path	I-DeepLearning
.	O
Finally	O
,	O
a	O
softmax	B-DeepLearning
layer	I-DeepLearning
is	O
used	O
to	O
transform	O
the	O
result	O
into	O
a	O
two-class	O
problem	O
.	O
We	O
improve	O
UNet	B-DeepLearning
by	O
replacing	O
the	O
simply	O
stacked	O
convolutional	B-DeepLearning
layers	I-DeepLearning
with	O
residual	B-DeepLearning
blocks	I-DeepLearning
to	O
prevent	O
vanishing	B-DeepLearning
gradient	I-DeepLearning
.	O
Furthermore	O
,	O
the	O
attention	B-DeepLearning
skip	I-DeepLearning
connection	I-DeepLearning
is	O
included	O
to	O
make	O
the	O
model	O
pay	O
more	O
attention	O
to	O
useful	O
spatial	O
areas	O
and	O
improve	O
the	O
ability	O
of	O
localization	O
.	O
As	O
is	O
shown	O
in	O
Fig.	O
3	O
,	O
it	O
contains	O
two	O
3	B-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
with	O
stride	B-DeepLearning
1	O
,	O
and	O
the	O
same	B-DeepLearning
padding	I-DeepLearning
is	O
used	O
to	O
ensure	O
the	O
output	O
has	O
the	O
same	O
size	O
with	O
the	O
input	O
.	O
The	O
reception	O
field	O
of	O
two	O
successive	O
3	B-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
is	O
the	O
same	O
with	O
that	O
of	O
a	O
5Ã—5Ã—5	B-DeepLearning
convolutional	I-DeepLearning
layer	I-DeepLearning
but	O
with	O
fewer	O
parameters	B-DeepLearning
to	O
be	O
computed	O
.	O
Since	O
the	O
extreme	O
imbalance	O
between	O
normal	O
regions	O
and	O
lesion	B-DeepLearning
regions	O
,	O
the	O
model	B-DeepLearning
tends	I-DeepLearning
to	I-DeepLearning
predict	I-DeepLearning
most	I-DeepLearning
areas	I-DeepLearning
as	I-DeepLearning
normal	I-DeepLearning
regions	I-DeepLearning
.	O
To	O
normally	O
represent	O
the	O
actual	O
shape	O
and	O
size	O
of	O
lesions	B-DeepLearning
,	O
we	O
firstly	O
normalize	B-DeepLearning
the	O
pixel	O
spacing	O
of	O
the	O
ABUS	O
volume	O
in	O
each	O
direction	O
.	O
Our	O
proposed	O
framework	O
is	O
implemented	O
with	O
PyTorch	B-DeepLearning
and	O
trained	O
on	O
NVIDIA	O
Tesla	O
M40	O
GPU	O
.	O
During	O
training	O
,	O
in	O
order	O
to	O
alleviate	O
the	O
computational	O
complexity	O
in	O
each	O
batch	O
and	O
increase	O
the	O
augmentation	O
,	O
we	O
randomly	O
crop	B-DeepLearning
the	I-DeepLearning
ABUS	I-DeepLearning
images	I-DeepLearning
into	O
small	O
patches	B-DeepLearning
with	I-DeepLearning
the	I-DeepLearning
size	I-DeepLearning
of	I-DeepLearning
160	I-DeepLearning
Ã—	I-DeepLearning
80	I-DeepLearning
Ã—	I-DeepLearning
160	I-DeepLearning
,	O
70%	O
of	O
which	O
are	O
lesion-centered	O
.	O
In	O
addition	O
,	O
we	O
adopt	O
various	O
data	B-DeepLearning
augmentations	I-DeepLearning
,	O
e.g.	O
,	O
contrast	O
adjustment	O
,	O
rotation	O
,	O
cropping	O
,	O
flipping	O
,	O
especially	O
the	O
elastic	O
transform	O
[	O
26	O
]	O
,	O
which	O
are	O
widely	O
used	O
in	O
small	O
medical	O
image	O
dataset	O
to	O
increase	O
the	O
data	O
diversity	O
.	O
Adam	B-DeepLearning
optimizer	I-DeepLearning
is	O
used	O
to	O
train	O
the	O
whole	O
network	O
,	O
and	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
is	O
set	O
as	O
1e-4	O
,	O
and	O
training	O
is	O
stopped	O
when	O
the	O
model	O
has	O
similar	O
performance	O
on	O
training	O
and	O
validation	O
dataset	O
.	O
Then	O
the	O
skip	B-DeepLearning
attention	I-DeepLearning
is	O
used	O
to	O
identify	O
the	O
image	O
regions	O
and	O
prune	O
the	O
irrelevant	O
feature	O
responses	O
so	O
as	O
to	O
preserve	O
only	O
the	O
activation	O
relevant	O
to	O
this	O
segmentation	O
task	O
.	O
The	O
architecture	B-DeepLearning
of	I-DeepLearning
AssCNV23â€™s	I-DeepLearning
network	I-DeepLearning
contains	O
four	B-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
intersected	O
with	O
three	B-DeepLearning
max	I-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
,	O
three	B-DeepLearning
fully	I-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
.	O
The	O
detected	O
results	O
were	O
compared	O
to	O
the	O
benchmark	O
data	O
,	O
and	O
the	O
performance	O
of	O
each	O
tool	O
was	O
evaluated	O
by	O
Precision	B-DeepLearning
Pre	I-DeepLearning
,	O
Sensitivity	B-DeepLearning
Sen	I-DeepLearning
,	O
F1-score	B-DeepLearning
F1	I-DeepLearning
and	O
Matthews	B-DeepLearning
correlation	I-DeepLearning
coefficient	I-DeepLearning
MCC	B-DeepLearning
.	O
They	O
used	O
Gold-standard	B-DeepLearning
dataset	I-DeepLearning
,	O
SCOP	O
version	O
1.75	O
and	O
6SSE	O
,	O
5SSE	O
,	O
4SSE	O
and	O
3SSE	O
[	O
10	O
]	O
.	O
Indeed	O
,	O
this	O
work	O
will	O
focus	O
on	O
the	O
development	O
of	O
a	O
diagnosis	B-DeepLearning
support	I-DeepLearning
system	I-DeepLearning
,	O
in	O
terms	O
of	O
its	O
knowledge	O
representation	O
and	O
reasoning	O
procedures	O
,	O
under	O
a	O
formal	O
framework	O
based	O
on	O
Logic	O
Programming	O
,	O
complemented	O
with	O
an	O
approach	O
to	O
computing	O
centered	O
on	O
Artificial	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
,	O
to	O
evaluate	O
ACS	O
predisposing	O
and	O
the	O
respective	O
Degree-of-Confidence	O
that	O
one	O
has	O
on	O
such	O
a	O
happening	O
.	O
In	O
this	O
study	O
627	B-DeepLearning
patients	I-DeepLearning
admitted	O
on	O
the	O
emergency	O
department	O
were	O
considered	O
,	O
during	O
the	O
period	O
of	O
three	O
months	O
from	O
February	O
to	O
May	O
of	O
2013	O
,	O
with	O
suspect	O
of	O
ACS	O
.	O
The	O
gender	B-DeepLearning
distribution	I-DeepLearning
was	O
48%	B-DeepLearning
and	I-DeepLearning
52%	I-DeepLearning
for	I-DeepLearning
female	I-DeepLearning
and	I-DeepLearning
male	I-DeepLearning
,	O
respectively	O
.	O
Seventeen	O
variables	O
were	O
selected	O
allowing	O
one	O
to	O
have	O
a	O
multivariable	O
dataset	B-DeepLearning
with	I-DeepLearning
627	I-DeepLearning
records	I-DeepLearning
.	O
To	O
ensure	O
statistical	O
significance	O
of	O
the	O
attained	O
results	O
,	O
20	B-DeepLearning
twenty	I-DeepLearning
experiments	I-DeepLearning
were	O
applied	O
in	O
all	O
tests	O
.	O
In	O
each	O
simulation	O
,	O
the	O
available	O
data	O
was	O
randomly	O
divided	O
into	O
two	O
mutually	O
exclusive	O
partitions	O
,	O
i.e.	O
,	O
the	O
training	B-DeepLearning
set	I-DeepLearning
with	O
67%	O
of	O
the	O
available	O
data	O
and	O
,	O
the	O
test	B-DeepLearning
set	I-DeepLearning
with	O
the	O
remaining	O
33%	O
of	O
the	O
cases	O
.	O
In	O
the	O
other	O
layers	O
we	O
used	O
the	O
sigmoid	B-DeepLearning
function	O
.	O
As	O
the	O
output	B-DeepLearning
function	I-DeepLearning
in	O
the	O
preprocessing	O
layer	O
it	O
was	O
used	O
the	O
identity	B-DeepLearning
one	O
.	O
Table	O
2	O
shows	O
that	O
the	O
model	B-DeepLearning
accuracy	I-DeepLearning
was	I-DeepLearning
96.9%	I-DeepLearning
for	I-DeepLearning
the	I-DeepLearning
training	I-DeepLearning
set	I-DeepLearning
410	O
correctly	O
classified	O
in	O
423	O
and	O
94.6%	B-DeepLearning
for	I-DeepLearning
test	I-DeepLearning
set	I-DeepLearning
193	O
correctly	O
classified	O
in	O
204	O
.	O
Thus	O
,	O
the	O
predictions	O
made	O
by	O
the	O
ANN	O
model	O
are	O
satisfactory	O
,	O
attaining	O
accuracies	B-DeepLearning
close	I-DeepLearning
to	I-DeepLearning
95%	I-DeepLearning
.	O
Huang	O
and	O
Liao	O
[	O
45	O
]	O
introduced	O
a	O
comprehensive	O
study	O
to	O
investigate	O
the	O
capability	O
of	O
the	O
probabilistic	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
PNN	B-DeepLearning
associated	O
with	O
a	O
feature	O
selection	O
method	O
,	O
the	O
so-called	O
signal-to-noise	O
statistic	O
,	O
in	O
cancer	B-DeepLearning
classification	I-DeepLearning
.	O
As	O
an	O
illustrated	O
in	O
Figure	O
1.6a	O
,	O
the	O
entire	O
data-set	O
of	O
all	O
88	B-DeepLearning
experiments	I-DeepLearning
was	O
first	O
quality	O
filtered	O
1	O
and	O
then	O
the	O
dimensionality	B-DeepLearning
was	I-DeepLearning
further	I-DeepLearning
reduced	I-DeepLearning
by	O
principal	B-DeepLearning
component	I-DeepLearning
analysis	I-DeepLearning
PCA	B-DeepLearning
to	O
10	B-DeepLearning
PC	I-DeepLearning
projections	I-DeepLearning
2	O
,	O
from	O
the	O
original	O
6567	O
expression	O
values	O
.	O
Next	O
,	O
the	O
25	B-DeepLearning
test	I-DeepLearning
experiments	I-DeepLearning
were	O
set	O
aside	O
and	O
the	O
63	B-DeepLearning
training	I-DeepLearning
experiments	I-DeepLearning
were	O
randomly	O
partitioned	O
into	O
3	O
groups	O
3	O
.	O
ANN	O
models	O
were	O
then	O
calibrated	O
using	O
for	O
each	O
sample	O
the	O
10	B-DeepLearning
PC	I-DeepLearning
values	I-DeepLearning
as	I-DeepLearning
input	I-DeepLearning
and	O
the	O
cancer	B-DeepLearning
category	I-DeepLearning
as	I-DeepLearning
output	I-DeepLearning
5	O
.	O
For	O
each	O
model	O
,	O
the	O
calibration	O
was	O
optimized	O
with	O
100	O
iterative	O
cycles	O
epochs	B-DeepLearning
.	O
In	O
addition	O
,	O
there	O
was	O
no	O
sign	O
of	O
over-training:	B-DeepLearning
if	O
the	O
models	O
begin	O
to	O
learn	O
features	O
in	O
the	O
training	B-DeepLearning
set	I-DeepLearning
,	O
which	O
are	O
not	O
present	O
in	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
this	O
would	O
result	O
in	O
an	O
increase	O
in	O
the	O
error	O
for	O
the	O
validation	O
at	O
that	O
point	O
and	O
the	O
curves	O
would	O
no	O
longer	O
remain	O
parallel	O
.	O
A	O
standard	O
approach	O
to	O
neural	B-DeepLearning
network	I-DeepLearning
training	O
is	O
the	O
use	O
of	O
backpropagation	B-DeepLearning
to	O
optimize	O
the	O
weight	B-DeepLearning
assignments	I-DeepLearning
for	O
a	O
fixed	O
neural	O
network	O
topology	O
.	O
Gene	O
selection	O
using	O
Feed	B-DeepLearning
Forward	I-DeepLearning
Back	I-DeepLearning
Propagation	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
as	O
a	O
classifier	O
is	O
illustrated	O
in	O
Figure	O
17	O
.	O
The	O
new	O
technique	O
is	O
based	O
on	O
Switching	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
SNN	B-DeepLearning
,	O
learning	O
machines	O
that	O
assign	O
a	O
relevance	O
value	O
to	O
each	O
input	O
variable	O
,	O
and	O
adopts	O
Recursive	O
Feature	O
Addition	O
RFA	O
for	O
performing	O
gene	O
selection	O
.	O
After	O
that	O
,	O
authors	O
classify	O
the	O
microarray	O
data	O
sets	O
with	O
a	O
Fuzzy	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
FNN	B-DeepLearning
.	O
Recently	O
,	O
Marylyn	O
et	O
al.	O
[	O
74	O
]	O
took	O
a	O
serious	O
attempt	O
to	O
introduce	O
a	O
genetic	B-DeepLearning
programming	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
GPNN	B-DeepLearning
as	O
a	O
method	O
for	O
optimizing	O
the	O
architecture	O
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
to	O
improve	O
the	O
identification	O
of	O
genetic	O
and	O
gene-environment	O
combinations	O
associated	O
with	O
a	O
disease	O
risk	O
.	O
This	O
empirical	O
studies	O
suggest	O
GPNN	B-DeepLearning
has	O
excellent	O
power	O
for	O
identifying	O
gene-gene	O
and	O
gene-environment	O
interactions	O
.	O
Using	O
simulated	O
dataauthors	O
show	O
that	O
GPNN	B-DeepLearning
has	O
higher	O
power	O
to	O
identify	O
gene-gene	O
and	O
gene-environment	O
interactions	O
than	O
SLR	O
and	O
CART	O
.	O
These	O
include	O
an	O
independent	O
variable	O
input	O
set	O
,	O
a	O
list	O
of	O
mathematical	O
functions	O
,	O
a	O
fitness	B-DeepLearning
function	I-DeepLearning
,	O
and	O
finally	O
the	O
operating	O
parameters	B-DeepLearning
of	O
the	O
GP	O
.	O
These	O
operating	O
parameters	B-DeepLearning
include	O
number	B-DeepLearning
of	I-DeepLearning
demes	I-DeepLearning
or	I-DeepLearning
populations	I-DeepLearning
,	O
population	B-DeepLearning
size	I-DeepLearning
,	O
number	B-DeepLearning
of	I-DeepLearning
generations	I-DeepLearning
,	O
reproduction	B-DeepLearning
rate	I-DeepLearning
,	O
crossover	B-DeepLearning
rate	I-DeepLearning
,	O
mutation	B-DeepLearning
rate	I-DeepLearning
,	O
and	O
migration	B-DeepLearning
[	O
90	O
]	O
.	O
Here	O
,	O
we	O
will	O
train	B-DeepLearning
the	I-DeepLearning
GPNN	I-DeepLearning
on	I-DeepLearning
9/10	I-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
to	O
develop	O
an	O
NN	O
model	O
.	O
They	O
test	B-DeepLearning
this	I-DeepLearning
model	I-DeepLearning
on	I-DeepLearning
the	I-DeepLearning
1/10	I-DeepLearning
of	I-DeepLearning
the	I-DeepLearning
data	I-DeepLearning
left	O
out	O
to	O
evaluate	O
the	O
predictive	O
ability	O
of	O
the	O
model	O
.	O
Another	O
work	O
introduced	O
by	O
Alison	O
et	O
al	O
[	O
74	O
]	O
which	O
developed	O
a	O
grammatical	B-DeepLearning
evolution	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
GENN	B-DeepLearning
approach	O
that	O
accounts	O
for	O
the	O
drawbacks	O
of	O
GPNN	B-DeepLearning
.	O
They	O
also	O
,	O
compare	O
the	O
performance	O
of	O
GENN	B-DeepLearning
to	O
GPNN	B-DeepLearning
,	O
a	O
traditional	O
Back-Propagation	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
BPNN	B-DeepLearning
and	O
a	O
random	O
search	O
algorithm	O
.	O
GENN	B-DeepLearning
outperforms	O
both	O
BPNN	B-DeepLearning
and	O
the	O
random	O
search	O
,	O
and	O
performs	O
at	O
least	O
as	O
well	O
as	O
GPNN	B-DeepLearning
.	O
Liang	O
and	O
Kelemen	O
[	O
60	O
]	O
proposed	O
a	O
Time	B-DeepLearning
Lagged	I-DeepLearning
Recurrent	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
with	O
trajectory	O
learning	O
for	O
identifying	O
and	O
classifying	O
the	O
gene	O
functional	O
patterns	O
from	O
the	O
heterogeneous	O
nonlinear	O
time	O
series	O
microarray	O
experiments	O
.	O
We	O
used	O
a	O
fuzzy	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
FNN	B-DeepLearning
proposed	O
earlier	O
for	O
cancer	B-DeepLearning
classification	I-DeepLearning
.	O
In	O
this	O
work	O
we	O
used	O
three	O
well-known	O
microarray	B-DeepLearning
databases	I-DeepLearning
,	O
i.e.	O
,	O
the	O
lymphoma	B-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
,	O
the	O
small	B-DeepLearning
round	I-DeepLearning
blue	I-DeepLearning
cell	I-DeepLearning
tumor	I-DeepLearning
SRBCT	I-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
,	O
and	O
the	O
ovarian	B-DeepLearning
cancer	I-DeepLearning
data	I-DeepLearning
set	I-DeepLearning
.	O
Our	O
result	O
shows	O
the	O
FNN	B-DeepLearning
classifier	O
not	O
only	O
improves	O
the	O
accuracy	O
of	O
cancer	O
classification	O
problem	O
but	O
also	O
helps	O
biologists	O
to	O
find	O
a	O
better	O
relationship	O
between	O
important	O
genes	O
and	O
development	O
of	O
cancers	O
.	O
An	O
adaptive	B-DeepLearning
learning	I-DeepLearning
rate	I-DeepLearning
provides	O
the	O
network	O
with	O
higher	O
convergence	B-DeepLearning
speed	O
and	O
learning	O
performance	O
,	O
i.e.	O
,	O
accuracy	O
.	O
Here	O
we	O
use	O
a	O
heuristic	O
for	O
tuning	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
Î·	I-DeepLearning
.	O
If	O
the	O
error	O
undergoes	O
five	O
consecutive	O
reductions	O
,	O
increase	B-DeepLearning
Î·	I-DeepLearning
by	I-DeepLearning
5%	I-DeepLearning
.	O
If	O
the	O
error	O
undergoes	O
three	O
consecutive	O
combinations	O
of	O
one	O
increase	O
and	O
one	O
reduction	O
,	O
decrease	B-DeepLearning
Î·	I-DeepLearning
by	I-DeepLearning
5%	I-DeepLearning
.	O
Since	O
we	O
dynamically	O
update	O
the	O
learning	B-DeepLearning
rate	I-DeepLearning
,	O
initial	O
value	O
for	O
Î·	B-DeepLearning
becomes	O
insignificant	O
as	O
long	O
as	O
it	O
is	O
not	O
too	O
large	O
.	O
For	O
the	O
SRBCT	O
data	O
,	O
the	O
FNN	B-DeepLearning
needs	O
only	O
8	O
genes	O
to	O
obtain	O
the	O
same	O
accuracy	O
,	O
i.e.	O
,	O
100%	O
,	O
while	O
the	O
well-known	O
evolutionary	O
algorithm	O
reported	O
by	O
Deutsch	O
[	O
8	O
]	O
used	O
12	O
genes	O
.	O
The	O
network	B-DeepLearning
configuration	I-DeepLearning
,	O
as	O
shown	O
in	O
Figure	O
4	O
,	O
is	O
composed	B-DeepLearning
of	I-DeepLearning
two	I-DeepLearning
layers	I-DeepLearning
one	B-DeepLearning
hidden	I-DeepLearning
and	I-DeepLearning
one	I-DeepLearning
output	I-DeepLearning
.	O
The	O
hidden	B-DeepLearning
layer	I-DeepLearning
consistes	O
on	O
five	B-DeepLearning
neurons	I-DeepLearning
,	O
while	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
has	O
only	O
one	B-DeepLearning
neuron	I-DeepLearning
.	O
Sigmoid	B-DeepLearning
type	O
of	O
neurons	O
have	O
been	O
employed	O
in	O
both	O
layers	O
,	O
the	O
hidden	O
and	O
the	O
output	O
layers	O
.	O
The	O
algorithm	O
used	O
for	O
training	O
process	O
was	O
the	O
Scaled	B-DeepLearning
Conjugate	I-DeepLearning
Gradient	I-DeepLearning
and	O
performance	O
is	O
optimized	O
by	O
the	O
function	O
Cross-Entropy	B-DeepLearning
.	O
In	O
this	O
paper	O
we	O
describe	O
use	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
for	I-DeepLearning
ECG	I-DeepLearning
signal	I-DeepLearning
prediction	I-DeepLearning
.	O
A	O
training	O
signal	O
figure	O
4	O
,	O
red	O
part	O
consists	O
of	O
620	B-DeepLearning
samples	I-DeepLearning
.	O
The	O
test	O
signal	O
is	O
also	O
10	O
second	O
long	O
1000	B-DeepLearning
samples	I-DeepLearning
.	O
We	O
used	O
the	O
log-sigmoid	B-DeepLearning
transfer	O
function	O
logsig	B-DeepLearning
in	O
each	O
layer	O
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
was	O
set	O
to	O
value	O
0.05	O
and	O
the	O
tolerable	O
error	O
to	O
510-5	O
.	O
The	O
said	O
algorithm	O
is	O
based	O
on	O
Levenberg-Marquardt	B-DeepLearning
optimization	O
.	O
In	O
this	O
study	O
a	O
novel	O
neural	B-DeepLearning
network-based	I-DeepLearning
method	O
for	O
peptide	B-DeepLearning
identification	I-DeepLearning
is	O
proposed	O
.	O
The	O
presented	O
here	O
Qual	O
method	O
uses	O
for	O
classification	O
purposes	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
,	O
feed-forward	I-DeepLearning
multilayer	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
[	O
6	O
]	O
with	O
6	B-DeepLearning
input	I-DeepLearning
nodes	I-DeepLearning
corresponding	O
to	O
the	O
spectral	O
features	O
,	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
,	O
and	O
one	B-DeepLearning
output	I-DeepLearning
node	I-DeepLearning
.	O
The	O
number	O
of	O
nodes	O
in	O
the	O
hidden	O
layer	O
was	O
determined	O
by	O
cross-validation	O
during	O
training	O
with	O
the	O
back-propagation	B-DeepLearning
Leveberg-Marquadt	I-DeepLearning
algorithm	O
.	O
The	O
final	O
neural	B-DeepLearning
network	I-DeepLearning
model	O
included	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
with	I-DeepLearning
8	I-DeepLearning
nodes	I-DeepLearning
.	O
The	O
training	B-DeepLearning
set	I-DeepLearning
consisted	O
of	O
184	B-DeepLearning
653	I-DeepLearning
tandem	I-DeepLearning
spectra	I-DeepLearning
acquired	O
with	O
an	O
high	O
resolution	O
LTQâ€“	O
FTICR	O
mass	O
spectrometer	O
and	O
searched	O
against	O
a	O
target/decoy	O
database	O
with	O
the	O
X!Tandem	O
engine	O
[	O
3	O
]	O
.	O
The	O
expected	O
neural	B-DeepLearning
network	I-DeepLearning
output	I-DeepLearning
values	I-DeepLearning
for	O
positive	O
and	O
negative	O
examples	O
were	O
equal	O
to	O
1	O
and	O
0	O
,	O
respectively	O
.	O
To	O
recognize	O
the	O
performed	O
activity	O
the	O
backpropagation	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
with	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
was	O
used	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
newly	O
proposed	O
COSMIC	B-DeepLearning
CNA	I-DeepLearning
dataset	I-DeepLearning
,	O
which	O
contains	O
25	B-DeepLearning
types	I-DeepLearning
of	I-DeepLearning
cancer	I-DeepLearning
.	O
We	O
obtained	B-DeepLearning
4731	I-DeepLearning
corn	I-DeepLearning
seed	I-DeepLearning
RGB	I-DeepLearning
images	I-DeepLearning
consisting	O
of	O
952	B-DeepLearning
haploid	I-DeepLearning
and	O
3779	B-DeepLearning
diploid	I-DeepLearning
seeds	O
from	O
several	O
different	O
proprietary	O
maize	O
inbred	O
lines	O
.	O
We	O
train	B-DeepLearning
our	I-DeepLearning
network	I-DeepLearning
using	O
the	O
image	B-DeepLearning
dataset	I-DeepLearning
that	O
was	O
randomly	O
split	O
into	O
4021	B-DeepLearning
training	I-DeepLearning
809	B-DeepLearning
haploid	I-DeepLearning
and	O
3212	B-DeepLearning
diploid	I-DeepLearning
seeds	O
and	O
710	B-DeepLearning
test	I-DeepLearning
143	B-DeepLearning
haploids	I-DeepLearning
and	O
567	B-DeepLearning
diploids	I-DeepLearning
images	O
with	O
20%	B-DeepLearning
haploids	I-DeepLearning
in	I-DeepLearning
both	I-DeepLearning
sets	I-DeepLearning
.	O
Input	O
images	O
of	O
the	O
corn	O
seeds	O
are	O
convolved	O
with	O
16	B-DeepLearning
filter	I-DeepLearning
kernels	I-DeepLearning
in	O
each	O
convolutional	O
layer	O
,	O
followed	O
by	O
two	B-DeepLearning
fully	I-DeepLearning
connected	I-DeepLearning
layers	I-DeepLearning
and	O
an	O
output	O
layer	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
Convolutional	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
CNN	B-DeepLearning
as	O
a	O
tool	O
to	O
improve	O
efficiency	O
and	O
accuracy	O
of	O
Osteosarcoma	B-DeepLearning
tumor	I-DeepLearning
classification	I-DeepLearning
into	O
tumor	B-DeepLearning
classes	I-DeepLearning
viable	O
tumor	O
,	O
necrosis	O
vs	O
non-tumor	O
.	O
The	O
proposed	O
CNN	B-DeepLearning
architecture	I-DeepLearning
contains	O
five	B-DeepLearning
learned	I-DeepLearning
layers:	I-DeepLearning
three	B-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
interspersed	O
with	O
max	B-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
for	O
feature	O
extraction	O
and	O
two	B-DeepLearning
fully-connected	I-DeepLearning
layers	I-DeepLearning
with	O
data	B-DeepLearning
augmentation	I-DeepLearning
strategies	O
to	O
boost	O
performance	O
.	O
The	O
convolution	B-DeepLearning
filters	I-DeepLearning
are	O
applied	O
to	O
small	O
patches	O
of	O
the	O
input	O
image	O
to	O
detect	O
and	O
extract	O
image	O
features	O
.	O
Our	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	I-DeepLearning
combines	O
features	O
of	O
AlexNet	B-DeepLearning
[	O
9	O
]	O
and	O
LeNet	B-DeepLearning
[	O
10	O
]	O
to	O
develop	O
a	O
fast	O
and	O
accurate	O
slide	O
classification	O
system	O
.	O
The	O
goal	O
of	O
this	O
paper	O
is	O
to	O
utilize	O
CNN	B-DeepLearning
to	O
identify	O
the	O
four	O
regions	B-DeepLearning
of	I-DeepLearning
interest	I-DeepLearning
Fig.	O
2	O
,	O
namely	O
,	O
1	O
Viable	B-DeepLearning
tumor	I-DeepLearning
,	O
2	O
Coagulative	B-DeepLearning
necrosis	I-DeepLearning
,	O
3	O
fibrosis	B-DeepLearning
or	I-DeepLearning
osteoid	I-DeepLearning
,	O
and	O
4	O
Non	B-DeepLearning
tumor	I-DeepLearning
Bone	O
,	O
cartilage	O
.	O
These	O
four	O
regions	O
are	O
used	O
to	O
extract	O
information	O
about	O
the	O
three	O
main	O
classes	B-DeepLearning
of	I-DeepLearning
interest:	I-DeepLearning
viable	B-DeepLearning
tumor	I-DeepLearning
,	O
necrosis	B-DeepLearning
coagulative	O
necrosis	O
,	O
osteiod	O
,	O
and	O
fibrosis	O
,	O
and	O
other	O
tissue	O
bone	O
,	O
blood	O
vessels	O
,	O
cartilage	O
,	O
etc	O
.	O
Spanhol	O
et	O
al.	O
[	O
15	O
]	O
developed	O
on	O
existing	O
AlexNet	B-DeepLearning
for	O
different	O
segmentation	O
and	O
classification	O
tasks	O
in	O
breast	O
cancer	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
deep	O
learning	O
approach	O
capable	O
of	O
assigning	O
tumor	B-DeepLearning
classes	I-DeepLearning
viable	O
tumor	O
,	O
necrosis	O
vs	O
non-tumor	O
directly	O
to	O
input	O
slides	O
in	O
osteosarcoma	O
,	O
a	O
type	O
of	O
cancer	O
with	O
significantly	O
more	O
variability	O
in	O
tumor	O
description	O
.	O
We	O
extend	O
the	O
successful	O
Alexnet	B-DeepLearning
proposed	O
by	O
Krizhevsky	O
see	O
[	O
9	O
]	O
and	O
LeNet	B-DeepLearning
network	O
architectures	O
introduced	O
by	O
LeCun	O
see	O
[	O
10	O
]	O
which	O
uses	O
gradient	B-DeepLearning
based	I-DeepLearning
learning	I-DeepLearning
with	O
back	B-DeepLearning
propogation	I-DeepLearning
algorithm	O
.	O
The	O
typical	O
CNN	B-DeepLearning
architecture	I-DeepLearning
for	O
image	O
classification	O
consists	O
of	O
a	O
series	O
of	O
convolution	B-DeepLearning
filters	I-DeepLearning
paired	O
with	O
pooling	B-DeepLearning
layers	I-DeepLearning
.	O
It	O
is	O
trained	O
to	O
classify	O
patches	O
into	O
three	O
bins:	O
viable	B-DeepLearning
tumor	I-DeepLearning
,	O
necrosis	B-DeepLearning
coagulative	O
necrosis	O
,	O
osteoid	O
,	O
fibrosis	O
and	O
non-tumor	B-DeepLearning
.	O
We	O
develop	O
on	O
existing	O
proven	O
networks	O
LeNet	B-DeepLearning
and	O
AlexNet	B-DeepLearning
because	O
finding	O
a	O
successful	O
network	O
configuration	O
for	O
a	O
given	O
problem	O
can	O
be	O
a	O
difficult	O
challenge	O
given	O
the	O
total	O
number	O
of	O
possible	O
configurations	O
that	O
can	O
be	O
defined	O
.	O
The	O
data	B-DeepLearning
augmentation	I-DeepLearning
methods	O
to	O
reduce	O
over-fitting	B-DeepLearning
on	O
image	O
data	O
as	O
described	O
by	O
Krizhevsky	O
[	O
9	O
]	O
has	O
been	O
proclaimed	O
for	O
its	O
success	O
rate	O
in	O
various	O
object	O
recognition	O
applications	O
.	O
We	O
start	O
with	O
a	O
simple	O
3	B-DeepLearning
layer	I-DeepLearning
network	I-DeepLearning
INPUT	B-DeepLearning
-	I-DeepLearning
CONVOLUTION	I-DeepLearning
-	I-DeepLearning
MAX	I-DeepLearning
POOL	I-DeepLearning
-	I-DeepLearning
MLP	I-DeepLearning
.	O
INPUT	B-DeepLearning
128	I-DeepLearning
Ã—	I-DeepLearning
128	I-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
will	O
hold	O
the	O
raw	O
pixel	O
values	O
of	O
the	O
image	O
,	O
i.e.	O
an	O
image	O
of	O
width	O
128	O
,	O
height	O
128	O
,	O
and	O
with	O
three	O
color	O
channels	O
RGB	O
.	O
This	O
may	O
result	O
in	O
volume	O
such	O
as	O
124	B-DeepLearning
Ã—	I-DeepLearning
124	I-DeepLearning
Ã—	I-DeepLearning
4	I-DeepLearning
for	O
4	B-DeepLearning
filters	I-DeepLearning
.	O
MAX	B-DeepLearning
POOL	I-DeepLearning
layer	I-DeepLearning
will	O
down-sample	O
along	O
the	O
spatial	O
dimensions	O
width	O
,	O
height	O
,	O
resulting	O
in	O
volume	O
62	B-DeepLearning
Ã—	I-DeepLearning
62	I-DeepLearning
Ã—	I-DeepLearning
4	I-DeepLearning
.	O
MLP	B-DeepLearning
layer	I-DeepLearning
will	O
compute	O
the	O
class	O
scores	O
,	O
resulting	O
in	O
volume	O
of	O
size	O
1	B-DeepLearning
Ã—	I-DeepLearning
1	I-DeepLearning
Ã—	I-DeepLearning
4	I-DeepLearning
,	O
where	O
each	O
of	O
the	O
4	O
numbers	O
correspond	O
to	O
a	O
class	O
score	O
for	O
the	O
4	B-DeepLearning
tumor	I-DeepLearning
regions	I-DeepLearning
.	O
The	O
different	O
layers	O
in	O
the	O
network	O
are	O
3	B-DeepLearning
Convolution	I-DeepLearning
layer	I-DeepLearning
C	O
,	O
3	B-DeepLearning
Sub-Sampling	I-DeepLearning
layer	I-DeepLearning
P	O
,	O
and	O
2	B-DeepLearning
fully	I-DeepLearning
connected	I-DeepLearning
multi-level	I-DeepLearning
perceptrons	I-DeepLearning
M	O
.	O
We	O
worked	O
with	O
different	O
number	O
of	O
hidden	B-DeepLearning
layers	I-DeepLearning
to	O
define	O
the	O
best	O
output	O
in	O
terms	O
of	O
tumor	O
identification	O
and	O
computational	O
resources	O
needed	O
see	O
Table	O
1	O
.	O
The	O
detailed	O
architecture	O
of	O
the	O
five	B-DeepLearning
level	I-DeepLearning
CNN	I-DeepLearning
for	O
tumor	B-DeepLearning
classification	I-DeepLearning
is	O
shown	O
in	O
Fig.	O
3	O
.	O
Our	O
architecture	O
combines	O
the	O
simplicity	O
of	O
Lenet	B-DeepLearning
architecture	I-DeepLearning
with	O
the	O
data	B-DeepLearning
augmentation	I-DeepLearning
methods	O
used	O
by	O
AlexNet	B-DeepLearning
architecture	I-DeepLearning
.	O
The	O
lower	O
3	B-DeepLearning
layers	I-DeepLearning
are	O
comprised	O
of	O
alternating	O
convolution	B-DeepLearning
and	I-DeepLearning
max-pooling	I-DeepLearning
layers	I-DeepLearning
.	O
The	O
first	O
convolution	B-DeepLearning
layer	I-DeepLearning
has	O
filter	B-DeepLearning
size	I-DeepLearning
5	I-DeepLearning
Ã—	I-DeepLearning
5	I-DeepLearning
used	O
to	O
detect	O
low	O
level	O
features	O
like	O
edges	O
which	O
is	O
followed	O
by	O
a	O
max	B-DeepLearning
pooling	I-DeepLearning
layer	I-DeepLearning
of	I-DeepLearning
scale	I-DeepLearning
2	I-DeepLearning
to	O
down-sample	O
the	O
data.	O
.	O
This	O
data	O
is	O
then	O
sent	O
to	O
second	O
layer	O
of	O
5	B-DeepLearning
Ã—	I-DeepLearning
5	I-DeepLearning
filters	I-DeepLearning
to	O
detect	O
higher	O
order	O
features	O
like	O
texture	O
and	O
spatial	O
connectivity	O
followed	O
by	O
a	O
max-pooling	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
last	O
convolution	B-DeepLearning
layer	I-DeepLearning
uses	O
a	O
filter	B-DeepLearning
of	I-DeepLearning
size	I-DeepLearning
3	I-DeepLearning
Ã—	I-DeepLearning
3	I-DeepLearning
and	O
max-pooling	B-DeepLearning
size	I-DeepLearning
2	I-DeepLearning
for	O
down-	O
sampling	O
to	O
generate	O
more	O
higher	O
order	O
features	O
.	O
The	O
upper	O
2	O
layers	O
are	O
fully-connected	B-DeepLearning
multi-level	I-DeepLearning
perceptron	I-DeepLearning
MLP	B-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
hidden	B-DeepLearning
layer	I-DeepLearning
+	O
logistic	B-DeepLearning
regression	I-DeepLearning
.	O
The	O
second	O
layer	O
of	O
the	O
MLP	B-DeepLearning
is	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
consisting	O
of	O
four	B-DeepLearning
neurons	I-DeepLearning
see	O
Table	O
2	O
.	O
The	O
sum	O
of	O
the	O
output	O
probabilities	O
from	O
the	O
MLP	B-DeepLearning
is	O
1	O
,	O
ensured	O
by	O
the	O
use	O
of	O
Softmax	B-DeepLearning
algorithm	O
as	O
the	O
activation	B-DeepLearning
function	I-DeepLearning
in	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
of	O
the	O
MLP	O
.	O
The	O
convolution	B-DeepLearning
and	O
max	B-DeepLearning
pooling	I-DeepLearning
layers	I-DeepLearning
are	O
feature	O
extractors	O
and	O
the	O
MLP	B-DeepLearning
is	O
the	O
classifier	O
.	O
The	O
easiest	O
and	O
most	O
common	O
method	O
to	O
reduce	O
overfitting	B-DeepLearning
of	O
data	O
is	O
to	O
artificially	O
augment	O
the	O
dataset	O
using	O
label-preserving	O
transformations	O
.	O
We	O
use	O
two	O
distinct	O
data	B-DeepLearning
augmentation	I-DeepLearning
techniques	O
both	O
of	O
which	O
allow	O
transformed	O
images	O
to	O
be	O
produced	O
from	O
the	O
original	O
images	O
with	O
very	O
little	O
computation	O
,	O
so	O
the	O
transformed	O
images	O
do	O
not	O
need	O
to	O
be	O
stored	O
on	O
disk	O
.	O
The	O
second	O
technique	O
for	O
data	B-DeepLearning
augmentation	I-DeepLearning
alters	O
the	O
intensities	O
of	O
the	O
RGB	O
channels	O
in	O
training	B-DeepLearning
images	I-DeepLearning
[	O
9	O
]	O
.	O
We	O
perform	O
Principal	B-DeepLearning
component	I-DeepLearning
analysis	I-DeepLearning
PCA	B-DeepLearning
on	O
the	O
set	O
of	O
RGB	O
pixel	O
values	O
throughout	O
the	O
training	B-DeepLearning
set	I-DeepLearning
and	O
then	O
,	O
for	O
each	O
training	O
image	O
,	O
we	O
add	O
the	O
following	O
quantity	O
to	O
each	O
RGB	O
image	O
pixel	O
i.e.	O
,	O
Ixy	O
=	O
[	O
IR	O
xy	O
,	O
IG	O
xy	O
,	O
IB	O
xy	O
]	O
T	O
:	O
[	O
p1	O
,	O
p2	O
,	O
p3}{Î±1Î»1	O
,	O
Î±2Î»2	O
,	O
Î±3Î»3	O
]	O
T	O
where	O
pi	O
and	O
Î»i	O
are	O
the	O
i-th	O
eigenvector	O
and	O
eigenvalue	O
of	O
the	O
3	O
Ã—	O
3	O
covariance	O
matrix	O
of	O
RGB	O
pixel	O
values	O
,	O
respectively	O
,	O
and	O
Î±i	O
is	O
a	O
random	O
variable	O
drawn	O
from	O
a	O
Gaussian	O
with	O
mean	O
0	O
and	O
standard	O
deviation	O
01	O
.	O
Data	B-DeepLearning
augmentation	I-DeepLearning
helps	O
alleviate	O
over-fitting	B-DeepLearning
by	O
considerably	O
increasing	O
the	O
amount	O
of	O
training	B-DeepLearning
data	I-DeepLearning
,	O
removing	O
rotation	O
dependency	O
and	O
making	O
the	O
training	O
images	O
invariant	O
to	O
changes	O
in	O
the	O
color	O
brightness	O
and	O
intensity	O
through	O
PCA	O
.	O
The	O
network	O
is	O
trained	O
with	O
stochastic	B-DeepLearning
gradient	I-DeepLearning
descent	I-DeepLearning
.	O
We	O
initialized	O
all	O
weights	B-DeepLearning
with	O
0	O
mean	O
by	O
assigning	O
them	O
small	O
,	O
random	O
and	O
unique	O
numbers	O
from	O
10âˆ’2	O
standard	O
deviation	O
Gaussian	O
random	O
numbers	O
,	O
so	O
that	O
each	O
layer	O
calculates	O
unique	O
updates	O
and	O
integrate	O
themselves	O
as	O
different	O
units	O
of	O
the	O
full	O
network	O
.	O
Each	O
case	O
consists	O
of	O
an	O
average	O
of	O
25	B-DeepLearning
individual	I-DeepLearning
svs	I-DeepLearning
images	I-DeepLearning
representing	O
different	O
sections	O
of	O
the	O
microscopic	O
slide	O
.	O
From	O
these	O
9	B-DeepLearning
svs	I-DeepLearning
slides	I-DeepLearning
,	O
81	B-DeepLearning
random	I-DeepLearning
tiles	I-DeepLearning
of	I-DeepLearning
size	I-DeepLearning
1024	I-DeepLearning
Ã—	I-DeepLearning
1024	I-DeepLearning
that	O
represent	O
different	O
tissue	O
and	O
cellular	O
regions	O
with	O
appearance	O
of	O
both	O
normal	O
and	O
malignant	O
regions	O
were	O
used	O
.	O
The	O
pathologists	O
then	O
used	O
an	O
in-house	O
tool	O
that	O
we	O
developed	O
to	O
annotate	O
these	O
81	B-DeepLearning
tiles	I-DeepLearning
as	O
viable	B-DeepLearning
tumor	I-DeepLearning
,	O
necrosis	B-DeepLearning
,	O
non-viable	B-DeepLearning
tumor	I-DeepLearning
,	O
and	O
non-tumor	B-DeepLearning
.	O
The	O
256	B-DeepLearning
Ã—	I-DeepLearning
256	I-DeepLearning
patches	I-DeepLearning
limited	O
the	O
CNN	B-DeepLearning
due	O
to	O
memory	O
issues	O
and	O
the	O
64	B-DeepLearning
Ã—	I-DeepLearning
64	I-DeepLearning
patch	I-DeepLearning
size	I-DeepLearning
had	O
very	O
low	O
accuracy	B-DeepLearning
.	O
Hence	O
we	O
decided	O
on	O
a	O
128	B-DeepLearning
Ã—	I-DeepLearning
128	I-DeepLearning
patch	I-DeepLearning
size	I-DeepLearning
.	O
This	O
resulted	O
in	O
about	O
5000	B-DeepLearning
image	I-DeepLearning
patches	I-DeepLearning
in	O
the	O
dataset	B-DeepLearning
.	O
Only	O
60%	B-DeepLearning
patches	I-DeepLearning
were	O
used	O
for	O
training	B-DeepLearning
,	O
and	O
20%	B-DeepLearning
data	I-DeepLearning
was	O
used	O
as	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
the	O
remaining	O
20%	B-DeepLearning
data	I-DeepLearning
was	O
use	O
for	O
test	B-DeepLearning
set	I-DeepLearning
.	O
The	O
architecture	O
was	O
developed	O
in	O
JAVA	O
using	O
dl4j	B-DeepLearning
deep	B-DeepLearning
learning	I-DeepLearning
for	I-DeepLearning
java	I-DeepLearning
libraries	O
[	O
1	O
]	O
.	O
The	O
training	B-DeepLearning
data	I-DeepLearning
was	O
fed	O
to	O
the	O
network	O
in	O
batch	B-DeepLearning
sizes	I-DeepLearning
of	O
100	O
to	O
utilize	O
parallelism	O
and	O
improve	O
the	O
network	O
efficiency	O
.	O
The	O
objective	O
of	O
the	O
network	O
was	O
to	O
classify	O
the	O
input	B-DeepLearning
images	I-DeepLearning
tiles	I-DeepLearning
into	O
one	O
of	O
the	O
four	O
regions	O
viable	B-DeepLearning
tumor	I-DeepLearning
,	O
coagulative	B-DeepLearning
necrosis	I-DeepLearning
,	O
osteoid	B-DeepLearning
or	I-DeepLearning
fibrosis	I-DeepLearning
,	O
non-tumor	B-DeepLearning
mentioned	O
before	O
.	O
The	O
performance	O
of	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
was	O
monitored	O
by	O
assessing	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
,	O
once	O
the	O
error	B-DeepLearning
rate	I-DeepLearning
saturated	O
after	O
10	B-DeepLearning
epochs	I-DeepLearning
,	O
training	B-DeepLearning
was	O
stopped	O
.	O
Our	O
implementation	O
gives	O
F1-score	B-DeepLearning
of	O
0.86	O
and	O
an	O
accuracy	B-DeepLearning
of	O
084	O
.	O
In	O
this	O
section	O
we	O
present	O
and	O
compare	O
the	O
qualitative	O
output	O
of	O
three	O
architectures:	O
AlexNet	B-DeepLearning
,	O
Lenet	B-DeepLearning
,	O
and	O
our	O
proposed	O
architecture	O
.	O
We	O
find	O
that	O
the	O
running	O
time	O
of	O
Lenet	B-DeepLearning
is	O
fastest	O
but	O
the	O
accuracy	B-DeepLearning
and	O
precision	B-DeepLearning
of	O
our	O
proposed	O
architecture	O
is	O
better	O
than	O
both	O
AlexNet	B-DeepLearning
and	O
Lenet	B-DeepLearning
see	O
Table	O
3	O
.	O
We	O
can	O
continue	O
to	O
explore	O
different	O
architectures	O
and	O
strategies	O
for	O
the	O
training	O
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
by	O
changing	O
the	O
hyper-parameters	B-DeepLearning
or	O
pre-processing	O
the	O
input	B-DeepLearning
data	I-DeepLearning
like	O
using	O
the	O
LAB	O
color	O
space	O
instead	O
of	O
RGB	O
space	O
or	O
by	O
augmenting	O
the	O
results	O
of	O
initial	O
segmentation	O
otsu	O
segmentation	O
in	O
the	O
input	B-DeepLearning
data	I-DeepLearning
.	O
This	O
can	O
be	O
done	O
by	O
applying	O
the	O
full	O
convolution	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
to	O
generate	O
color	O
coded	O
likelihood	O
maps	O
for	O
the	O
pathologists	O
.	O
As	O
far	O
as	O
the	O
authors	O
are	O
aware	O
,	O
this	O
is	O
the	O
first	O
paper	O
describing	O
the	O
applicability	O
of	O
convolutional	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
for	O
diagnostic	O
analysis	B-DeepLearning
of	I-DeepLearning
osteosarcoma	I-DeepLearning
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
computational	O
method	O
for	O
predicting	O
DTIs	O
from	O
drug	O
molecular	O
structure	O
and	O
protein	O
sequence	O
by	O
using	O
the	O
stacked	O
autoencoder	B-DeepLearning
of	O
deep	O
learning	O
which	O
can	O
adequately	O
extracts	O
the	O
raw	O
data	O
information	O
.	O
The	O
experimental	O
results	O
of	O
5-fold	B-DeepLearning
cross-validation	I-DeepLearning
indicate	O
that	O
the	O
proposed	O
method	O
achieves	O
superior	O
performance	O
on	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
with	O
accuracy	B-DeepLearning
of	O
0.9414	O
,	O
0.9116	O
,	O
0.8669	O
and	O
0.8056	O
,	O
respectively	O
.	O
In	O
the	O
experiment	O
,	O
we	O
make	O
the	O
predictions	O
on	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
drug-target	O
interactions	O
datasets	O
involving	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
In	O
this	O
experiment	O
,	O
we	O
used	O
the	O
chemical	O
structure	O
of	O
the	O
molecular	O
substructure	O
fingerprints	O
from	O
PubChem	B-DeepLearning
database	I-DeepLearning
.	O
It	O
defines	O
an	O
881	B-DeepLearning
dimensional	I-DeepLearning
binary	I-DeepLearning
vector	I-DeepLearning
to	O
represent	O
the	O
molecular	O
substructure	O
.	O
Stacked	B-DeepLearning
Auto-Encoder	I-DeepLearning
SAE	B-DeepLearning
is	O
a	O
popular	O
depth	O
learning	O
model	O
,	O
which	O
uses	O
autoencoders	B-DeepLearning
as	O
building	O
blocks	O
to	O
create	O
deep	O
neural	B-DeepLearning
network	I-DeepLearning
[	O
17	O
]	O
.	O
The	O
auto-encoder	B-DeepLearning
AE	O
can	O
be	O
considered	O
as	O
a	O
special	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
one	B-DeepLearning
input	I-DeepLearning
layer	I-DeepLearning
,	O
one	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
and	O
one	B-DeepLearning
output	I-DeepLearning
layer	I-DeepLearning
,	O
as	O
shown	O
in	O
Fig.	O
1	O
.	O
The	O
combination	O
of	O
multiple	O
auto-encoders	B-DeepLearning
together	O
constitutes	O
the	O
stacked	O
auto-encoders	B-DeepLearning
,	O
which	O
has	O
the	O
characteristics	O
of	O
deep	O
learning	O
.	O
Figure	O
2	O
shows	O
the	O
structure	O
of	O
the	O
stacked	O
auto-encoder	B-DeepLearning
with	O
h-level	O
auto-encoders	B-DeepLearning
which	O
are	O
trained	O
in	O
the	O
layer-wise	B-DeepLearning
and	O
bottom-up	O
manner	O
.	O
The	O
activation	B-DeepLearning
function	I-DeepLearning
is	O
usually	O
the	O
sigmoid	B-DeepLearning
function	O
or	O
tanh	B-DeepLearning
function	O
.	O
In	O
this	O
paper	O
,	O
we	O
set	O
up	O
a	O
3	B-DeepLearning
layer	I-DeepLearning
auto-encoder	I-DeepLearning
,	O
and	O
use	O
the	O
rotation	O
forest	O
as	O
the	O
final	O
classifier	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
5-fold	B-DeepLearning
cross-validation	I-DeepLearning
to	O
assess	O
the	O
predictive	O
ability	O
of	O
our	O
model	O
in	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
involving	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
The	O
proposed	O
model	O
performs	O
well	O
in	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets:	I-DeepLearning
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
.	O
Table	O
1	O
lists	O
the	O
experimental	O
results	O
on	O
the	O
enzyme	O
dataset	O
,	O
it	O
yielded	O
an	O
accuracy	B-DeepLearning
of	O
0.9414	O
,	O
sensitivity	B-DeepLearning
of	O
0.9555	O
,	O
precision	B-DeepLearning
of	O
0.9293	O
,	O
MCC	B-DeepLearning
of	O
0.8832	O
and	O
AUC	B-DeepLearning
of	O
09425	O
.	O
The	O
highest	O
accuracy	B-DeepLearning
of	O
the	O
five	O
models	O
reached	O
0.9462	O
,	O
and	O
the	O
lowest	O
also	O
reached	O
09385	O
.	O
The	O
accuracy	B-DeepLearning
,	O
sensitivity	B-DeepLearning
,	O
precision	B-DeepLearning
,	O
MCC	B-DeepLearning
and	O
AUC	B-DeepLearning
of	O
cross-validation	B-DeepLearning
are	O
0.9116	O
,	O
0.9569	O
,	O
0.8778	O
,	O
0.8271	O
and	O
0.9107	O
,	O
respectively	O
.	O
The	O
average	O
accuracy	B-DeepLearning
,	O
sensitivity	B-DeepLearning
,	O
precision	B-DeepLearning
,	O
MCC	B-DeepLearning
and	O
AUC	B-DeepLearning
are	O
0.8669	O
,	O
0.8164	O
,	O
0.9102	O
,	O
0.7396	O
and	O
0.8743	O
,	O
respectively	O
.	O
The	O
highest	O
accuracy	B-DeepLearning
of	O
the	O
five	O
models	O
reached	O
09331	O
.	O
We	O
achieved	O
an	O
accuracy	B-DeepLearning
of	O
0.8056	O
,	O
sensitivity	B-DeepLearning
of	O
0.7627	O
,	O
precision	B-DeepLearning
of	O
0.8410	O
,	O
MCC	B-DeepLearning
of	O
0.6188	O
and	O
AUC	B-DeepLearning
of	O
08176	O
.	O
Figures	O
4	O
,	O
5	O
,	O
6	O
and	O
7	O
show	O
the	O
ROC	B-DeepLearning
curves	I-DeepLearning
obtained	O
on	O
the	O
enzymes	O
,	O
ion	O
channels	O
,	O
GPCRs	O
and	O
nuclear	O
receptors	O
datasets	O
by	O
the	O
proposed	O
method	O
.	O
The	O
results	O
of	O
the	O
comparison	O
show	O
that	O
the	O
stacked	O
auto-encoder	B-DeepLearning
combined	O
with	O
the	O
rotation	O
forest	O
classifier	O
can	O
improve	O
the	O
prediction	O
ability	O
on	O
the	O
golden	B-DeepLearning
standard	I-DeepLearning
datasets	I-DeepLearning
Table	O
6	O
.	O
CAST	O
uses	O
Multi-Layer	B-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
,	O
a	O
type	O
of	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
that	O
is	O
often	O
applied	O
to	O
classification	O
problems	O
[	O
17	O
]	O
.	O
The	O
retina	B-DeepLearning
training	I-DeepLearning
set	I-DeepLearning
is	O
comprised	O
of	O
5891	B-DeepLearning
pixel	I-DeepLearning
examples	I-DeepLearning
.	O
The	O
accuracy	B-DeepLearning
of	O
the	O
training	B-DeepLearning
set	I-DeepLearning
was	O
tested	O
using	O
Weka	O
Explorer	O
,	O
which	O
calculated	O
the	O
accuracy	B-DeepLearning
of	O
correctly	O
classifying	O
pixels	O
to	O
be	O
96.4013%	O
[	O
19	O
]	O
.	O
The	O
network	O
used	O
consisted	O
of	O
97	B-DeepLearning
input	I-DeepLearning
nodes	I-DeepLearning
,	O
1	B-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
with	I-DeepLearning
49	I-DeepLearning
nodes	I-DeepLearning
,	O
and	O
2	B-DeepLearning
output	I-DeepLearning
nodes	I-DeepLearning
.	O
The	O
network	O
was	O
trained	O
using	O
ten-fold	B-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
.	O
The	O
learning	O
method	O
used	O
was	O
the	O
standard	O
method	O
of	O
backpropagation	B-DeepLearning
gradient	B-DeepLearning
decent	I-DeepLearning
.	O
In	O
order	O
to	O
develop	O
CAST	O
we	O
utilized	O
an	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
ANN	B-DeepLearning
and	O
tested	O
its	O
ability	O
against	O
fast	O
marching	O
.	O
Classifiers	O
such	O
as	O
perceptrons	B-DeepLearning
,	O
the	O
basic	O
component	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
,	O
have	O
been	O
found	O
useful	O
for	O
identifying	B-DeepLearning
membranes	I-DeepLearning
in	I-DeepLearning
EM	I-DeepLearning
images	I-DeepLearning
[	O
6	O
]	O
.	O
One	O
kind	O
of	O
neural	B-DeepLearning
network	I-DeepLearning
that	O
has	O
been	O
widely	O
used	O
for	O
image	O
segmentation	O
is	O
the	O
Convolutional	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
CNN	B-DeepLearning
.	O
ANNs	B-DeepLearning
require	O
significant	O
computing	O
power	O
,	O
which	O
can	O
be	O
an	O
important	O
factor	O
if	O
the	O
data	B-DeepLearning
set	I-DeepLearning
is	O
large	O
.	O
Use	O
of	O
ANNs	B-DeepLearning
has	O
become	O
common	O
in	O
image	B-DeepLearning
processing	I-DeepLearning
due	O
to	O
generally	O
high	O
performance	O
,	O
and	O
our	O
results	O
do	O
not	O
differ	O
in	O
that	O
we	O
also	O
find	O
classification	O
by	O
ANN	B-DeepLearning
to	O
provide	O
superior	O
performance	O
.	O
The	O
artificial	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
examines	O
each	O
pixel	O
in	O
the	O
selected	O
area	O
.	O
This	O
allows	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
to	O
make	O
an	O
accurate	O
classification	O
despite	O
low	O
contrasts	O
and	O
fragmented	O
boundaries	O
.	O
Outlines	O
made	O
using	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
are	O
subjectively	O
much	O
more	O
accurate	O
than	O
those	O
made	O
using	O
fast	O
marching	O
.	O
Expand	O
Area	O
works	O
in	O
three	O
dimensions	O
[	O
21	O
]	O
,	O
but	O
the	O
current	O
neural	B-DeepLearning
network	I-DeepLearning
only	O
works	O
in	O
two	O
dimensions	O
.	O
If	O
part	O
of	O
the	O
structure	O
of	O
interest	O
shifted	O
out	O
of	O
the	O
area	O
being	O
examined	O
by	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
somewhere	O
in	O
the	O
stack	O
,	O
the	O
results	O
produced	O
by	O
the	O
network	O
would	O
be	O
off	O
.	O
One	O
possible	O
solution	O
would	O
be	O
to	O
move	O
the	O
region	O
being	O
examined	O
by	O
the	O
neural	B-DeepLearning
network	I-DeepLearning
on	O
a	O
slice-to-slice	O
basis	O
.	O
These	O
words	O
are	O
filtered	O
by	O
the	O
NLTK	B-DeepLearning
toolkit	I-DeepLearning
[	O
9	O
]	O
.	O
However	O
,	O
Bengio	O
et	O
al.	O
[	O
11	O
]	O
developed	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
language	O
model	O
NNLM	B-DeepLearning
that	O
learns	O
a	O
probability	O
distribution	O
over	O
words	O
in	O
the	O
corpus	O
and	O
the	O
model	O
is	O
trained	O
to	O
produce	O
the	O
vector	O
representations	O
of	O
word	O
.	O
In	O
contrast	O
to	O
the	O
one-hot	O
encoding	O
,	O
the	O
NNLM	B-DeepLearning
generates	O
word	B-DeepLearning
vectors	I-DeepLearning
with	O
low	O
and	O
dense	O
vector	B-DeepLearning
,	O
it	O
is	O
easier	O
to	O
capture	O
the	O
wordâ€™s	O
property	O
.	O
Mikolov	O
et	O
al.	O
[	O
12	O
]	O
extended	O
the	O
Bengioâ€™	O
work	O
and	O
built	O
a	O
new	O
neural	B-DeepLearning
network	I-DeepLearning
language	O
leaning	O
model	O
,	O
Word2Ve	B-DeepLearning
,	O
using	O
different	O
learning	O
methods:	O
Continues	O
Bag	O
of	O
Words	O
CBOW	O
and	O
Skip-gram	O
.	O
The	O
reason	O
that	O
we	O
use	O
the	O
Word2Vec	B-DeepLearning
due	O
to	O
it	O
consume	O
less	O
time	O
than	O
Benhioâ€™s	O
NNLM	B-DeepLearning
while	O
training	O
.	O
We	O
applied	O
the	O
Word2Vec	B-DeepLearning
to	O
generate	O
the	O
good	O
quality	O
of	O
word	B-DeepLearning
vectors	I-DeepLearning
through	O
training	O
the	O
whole	O
data	B-DeepLearning
set	I-DeepLearning
.	O
In	O
order	O
to	O
explore	O
the	O
performances	O
of	O
different	O
combinations	O
of	O
the	O
nine	O
features	O
,	O
we	O
trained	O
5621000	B-DeepLearning
pattern	I-DeepLearning
recognition	I-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
[	O
11	O
]	O
.	O
45667	B-DeepLearning
models	I-DeepLearning
were	O
selected	O
to	O
test	O
the	O
new	O
discriminants	O
for	O
near	O
native	O
protein-protein	O
interface	O
recognition	O
according	O
to	O
the	O
performances	O
for	O
interface	O
residue	O
pair	O
prediction	O
.	O
In	O
this	O
article	O
,	O
we	O
study	O
the	O
impacts	O
of	O
encoding	O
schemes	O
on	O
several	O
variant	O
algorithms	O
in	O
auto-encoders	B-DeepLearning
by	O
applying	O
deep	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
to	O
gene	O
annotation	O
.	O
Four	O
variant	O
auto-encoder	B-DeepLearning
algorithms	O
include	O
orthodox	B-DeepLearning
auto-encoder	I-DeepLearning
,	O
denoising	B-DeepLearning
auto-encoder	I-DeepLearning
,	O
hidden-layer	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
and	O
double	B-DeepLearning
denoising	I-DeepLearning
auto-encoder.	I-DeepLearning
.	O
The	O
data	B-DeepLearning
sets	I-DeepLearning
are	O
the	O
standard	O
benchmark	O
from	O
fruitfly.org	O
for	O
predicting	B-DeepLearning
gene	I-DeepLearning
splicing	I-DeepLearning
sites	I-DeepLearning
on	O
human	O
genome	O
sequences	O
[	O
16	O
]	O
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
I	O
is	O
the	O
Acceptor	O
locations	O
containing	O
6877	B-DeepLearning
sequences	I-DeepLearning
with	O
90	B-DeepLearning
features	I-DeepLearning
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
II	O
is	O
the	O
Donor	O
locations	O
including	O
6246	B-DeepLearning
sequences	I-DeepLearning
with	O
15	B-DeepLearning
features	I-DeepLearning
.	O
The	O
data	B-DeepLearning
set	I-DeepLearning
of	O
cleaned	O
269	O
genes	O
is	O
divided	O
into	O
a	O
test	O
and	O
a	O
training	O
data	O
set	O
[	O
16	O
]	O
.	O
Denoising	B-DeepLearning
auto-encoder	I-DeepLearning
seems	O
not	O
fit	O
to	O
the	O
application	O
of	O
DNA	B-DeepLearning
structure	I-DeepLearning
prediction	I-DeepLearning
because	O
corrupted	O
input	B-DeepLearning
data	I-DeepLearning
DNA	O
features	O
at	O
each	O
location	O
may	O
have	O
a	O
high	O
dependency	O
with	O
others	O
such	O
that	O
denoising	O
makes	O
the	O
prediction	O
messed	O
.	O
Compared	O
with	O
the	O
performance	O
of	O
input-layer	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
in	O
Fig.	O
2c	O
,	O
d	O
,	O
in	O
hidden-layer	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
model	O
,	O
corrupting	O
some	O
nodes	O
on	O
hidden	B-DeepLearning
layers	I-DeepLearning
makes	O
a	O
less	O
impact	O
than	O
corrupting	O
nodes	O
on	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
Figure	O
2g	O
,	O
h	O
shows	O
the	O
performance	O
of	O
double	B-DeepLearning
denoising	I-DeepLearning
auto-encoder	I-DeepLearning
.	O
On	O
the	O
other	O
side	O
,	O
it	O
shows	O
that	O
auto-encoder	B-DeepLearning
method	O
without	O
denoising	O
is	O
better	O
than	O
other	O
three	O
variants	O
.	O
The	O
over-fitting	B-DeepLearning
issues	O
occur	O
in	O
Denoising	B-DeepLearning
Auto-encoder	I-DeepLearning
more	O
frequently	O
than	O
others	O
.	O
The	O
over-fitting	B-DeepLearning
occurrence	O
is	O
correlated	O
to	O
the	O
auto-encoder	B-DeepLearning
algorithm	O
and	O
the	O
encoding	O
schemes	O
.	O
The	O
DAE	O
shows	O
the	O
poorest	O
performance	O
among	O
the	O
autoencoder	B-DeepLearning
algorithms	O
.	O
In	O
case	O
study	O
section	O
given	O
below	O
,	O
we	O
demonstrate	O
the	O
application	O
of	O
a	O
Neural	B-DeepLearning
Network	I-DeepLearning
for	O
identifying	O
multimarker	O
panels	O
for	O
breast	O
cancer	O
based	O
on	O
liquid	O
chromatography	O
tandem	O
mass	O
spectrometry	O
LC/MS/MS	O
proteomics	O
profiles	O
.	O
Lancashire	O
et	O
al.	O
24	O
described	O
with	O
recent	O
literature	O
that	O
artificial	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
can	O
cope	O
with	O
highly	O
dimensional	O
,	O
complex	O
datasets	O
such	O
as	O
those	O
generated	O
by	O
protein	O
mass	O
spectrometry	O
and	O
DNA	O
microarray	O
experiments	O
,	O
and	O
can	O
also	O
be	O
used	O
to	O
solve	O
problems	O
,	O
such	O
as	O
disease	O
classification	O
and	O
identification	O
of	O
biomarkers	O
.	O
We	O
used	O
a	O
data	O
analysis	O
method	O
based	O
on	O
a	O
Feed-Forward	B-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
FFNN	B-DeepLearning
to	O
identify	O
multiprotein	O
biomarker	O
panels	O
,	O
with	O
which	O
we	O
are	O
capable	O
of	O
separating	O
plasma	O
samples	O
regarding	O
reference	O
and	O
cancerous	O
with	O
high	O
predictive	O
performance	O
.	O
According	O
to	O
an	O
area-under-the-curve	B-DeepLearning
AUC	B-DeepLearning
criterion	O
the	O
optimal	O
combination	O
of	O
a	O
panel	O
of	O
five	O
markers	O
was	O
determined	O
,	O
using	O
both	O
a	O
twovariable	O
output	O
encoding	O
scheme	O
and	O
a	O
single-variable	O
encoding	O
scheme	O
.	O
We	O
compared	O
the	O
Receiver	B-DeepLearning
Operating	I-DeepLearning
Characteristics	I-DeepLearning
ROC	B-DeepLearning
performance	O
and	O
verified	O
that	O
the	O
best	O
five-marker	O
panel	O
performed	O
well	O
in	O
both	O
training	B-DeepLearning
dataset	I-DeepLearning
and	O
test	B-DeepLearning
dataset	I-DeepLearning
,	O
achieving	O
more	O
than	O
82.5%	O
in	O
sensitivity	B-DeepLearning
and	O
specificity	B-DeepLearning
.	O
The	O
algorithm	O
NNPP	B-DeepLearning
Neural	B-DeepLearning
Network	I-DeepLearning
Promoter	I-DeepLearning
Prediction	I-DeepLearning
,	O
which	O
is	O
discussed	O
in	O
Section	O
10.5	O
,	O
was	O
developed	O
for	O
the	O
Berkeley	O
Drosophila	O
Genome	O
Project	O
BDGP	O
,	O
and	O
uses	O
time-delay	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
architecture	O
and	O
recognition	O
of	O
specific	O
sequence	O
patterns	O
to	O
predict	O
promoters	O
see	O
Figure	O
10.14	O
for	O
details	O
.	O
Each	O
sequence	O
position	O
is	O
represented	O
by	O
four	B-DeepLearning
input	I-DeepLearning
units	I-DeepLearning
,	O
each	O
representing	O
one	O
base	O
,	O
with	O
a	O
total	O
of	O
51	O
bases	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
Each	O
TATA-box	O
hidden	B-DeepLearning
unit	I-DeepLearning
has	O
inputs	O
from	O
15	O
consecutive	O
bases	O
,	O
and	O
the	O
Inr	O
hidden	B-DeepLearning
units	I-DeepLearning
also	O
receive	O
signals	O
from	O
15	O
consecutive	O
bases	O
.	O
The	O
same	O
weighting	O
scheme	O
is	O
applied	O
to	O
all	O
the	O
TATA-box	O
hidden	B-DeepLearning
units	I-DeepLearning
and	O
similarly	O
with	O
another	O
set	B-DeepLearning
of	I-DeepLearning
weights	I-DeepLearning
for	O
the	O
Inr	O
hidden	B-DeepLearning
units	I-DeepLearning
.	O
The	O
four	O
layers	O
of	O
squares	O
underneath	O
the	O
sequence	O
represent	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
units	O
and	O
encode	O
the	O
sequence	O
in	O
a	O
very	O
simple	O
way	O
.	O
Each	O
of	O
the	O
units	O
in	O
the	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
the	O
TATA	O
and	O
Inr	O
layers	O
receives	O
input	O
from	O
a	O
consecutive	O
set	O
of	O
15	O
bases	O
.	O
A	O
variety	O
of	O
measures	O
,	O
such	O
as	O
the	O
weight	O
matrix	O
score	O
for	O
TATA-box	O
signal	O
and	O
simple	O
measurements	O
such	O
as	O
the	O
GC	O
content	O
and	O
the	O
distance	O
between	O
different	O
signals	O
,	O
are	O
used	O
to	O
assign	O
values	O
to	O
the	O
units	O
in	O
an	O
input	B-DeepLearning
layer	I-DeepLearning
of	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
.	O
These	O
feed	O
via	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
to	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
unit	O
,	O
which	O
predicts	O
the	O
presence	O
of	O
RNA	O
polymerase	O
II	O
promoters	O
in	O
the	O
sequence	O
window	O
.	O
The	O
donor	O
sites	O
are	O
modeled	O
with	O
a	O
set	O
of	O
10	B-DeepLearning
networks	I-DeepLearning
,	O
each	O
with	O
23	O
bases	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
represented	O
by	O
four	O
units	O
each	O
,	O
as	O
in	O
Figure	O
10.14	O
,	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
10	O
units	O
,	O
and	O
a	O
single	O
output	B-DeepLearning
layer	I-DeepLearning
unit	O
.	O
The	O
10	B-DeepLearning
networks	I-DeepLearning
were	O
separately	O
trained	O
from	O
different	O
starting	O
points	O
,	O
and	O
their	O
outputs	O
from	O
0	O
to	O
1	O
averaged	O
to	O
get	O
the	O
final	O
score	O
.	O
A	O
similar	O
set	O
of	O
10	B-DeepLearning
networks	I-DeepLearning
was	O
used	O
to	O
score	O
acceptor	O
sites	O
,	O
except	O
that	O
these	O
have	O
a	O
61-base	O
input	O
window	O
and	O
15	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	O
.	O
The	O
set	O
of	O
neural	B-DeepLearning
networks	I-DeepLearning
used	O
to	O
predict	O
coding	O
regions	O
in	O
NetPlantGene	O
has	O
as	O
output	O
a	O
score	O
for	O
each	O
base	O
of	O
its	O
likelihood	O
of	O
being	O
in	O
a	O
coding	O
region	O
.	O
A	O
neural	B-DeepLearning
network	I-DeepLearning
predicts	O
the	O
three	O
secondary	B-DeepLearning
structure	I-DeepLearning
types	I-DeepLearning
a	O
,	O
b	O
,	O
and	O
coil	O
using	O
real	O
numbers	O
from	O
the	O
output	O
units	O
that	O
make	O
up	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
first	O
network	O
is	O
a	O
sequence-to-structure	B-DeepLearning
network	I-DeepLearning
whose	O
input	O
represents	O
the	O
local	O
alignment	O
,	O
residue	O
conservation	O
,	O
and	O
additionally	O
long-range	O
sequence	O
information	O
see	O
Figure	O
1122	O
.	O
The	O
output	O
of	O
the	O
first	O
network	O
forms	O
the	O
input	O
to	O
the	O
second	O
neural	B-DeepLearning
network	I-DeepLearning
,	O
a	O
structure-to-structure	B-DeepLearning
network	I-DeepLearning
.	O
NNSSP	B-DeepLearning
Neural	B-DeepLearning
Net	I-DeepLearning
Nearest	I-DeepLearning
Neighbor	I-DeepLearning
is	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
development	O
of	O
a	O
statistical	O
and	O
knowledge-based	O
program	O
,	O
SSP	O
,	O
which	O
,	O
like	O
PREDATOR	O
,	O
used	O
the	O
nearestneighbor	O
approach	O
.	O
NNSSP	B-DeepLearning
has	O
an	O
advanced	O
scoring	O
system	O
that	O
combines	O
sequence	O
similarity	O
,	O
local	O
sequence	O
information	O
,	O
and	O
knowledge-based	O
information	O
on	O
b-turns	O
and	O
the	O
amino-	O
and	O
carboxy-terminal	O
properties	O
of	O
a-helices	O
and	O
b-strands	O
.	O
Schematic	O
representation	O
of	O
the	O
double	O
neural	B-DeepLearning
network	I-DeepLearning
architecture	O
frequently	O
used	O
in	O
protein	B-DeepLearning
secondary	I-DeepLearning
structure	I-DeepLearning
prediction	I-DeepLearning
.	O
The	O
three	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	O
receive	O
the	O
same	O
signals	O
from	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
,	O
but	O
apply	O
different	O
weights	B-DeepLearning
to	O
them	O
.	O
The	O
first	O
hidden	B-DeepLearning
layer	I-DeepLearning
will	O
contain	O
39	B-DeepLearning
units	I-DeepLearning
for	O
a	O
13-residue	O
window	O
,	O
and	O
these	O
send	O
signals	O
to	O
units	O
in	O
a	O
second	O
hidden	B-DeepLearning
layer	I-DeepLearning
,	O
which	O
then	O
signals	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
see	O
Figure	O
1226	O
.	O
The	O
communication	O
from	O
the	O
first	O
hidden	B-DeepLearning
layer	I-DeepLearning
to	O
the	O
second	O
has	O
a	O
repeat	O
pattern	O
intended	O
to	O
mimic	O
the	O
helical	O
structure	O
,	O
and	O
has	O
been	O
colored	O
for	O
clarity	O
.	O
The	O
output	B-DeepLearning
layer	I-DeepLearning
Oi	O
has	O
three	B-DeepLearning
units	I-DeepLearning
,	O
corresponding	O
to	O
predicting	O
a-helix	O
,	O
b-strand	O
,	O
or	O
coil	O
for	O
residue	O
i	O
.	O
These	O
feed	O
signals	O
to	O
a	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
15	B-DeepLearning
units	I-DeepLearning
that	O
in	O
turn	O
send	O
signals	O
to	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
of	O
seven	B-DeepLearning
units	I-DeepLearning
.	O
The	O
initial	O
neural	B-DeepLearning
network	I-DeepLearning
has	O
a	O
sevenstate	B-DeepLearning
output	I-DeepLearning
,	O
namely	O
a-helix	O
H	O
,	O
b-strand	O
E	O
,	O
coil	O
C	O
,	O
and	O
the	O
single	O
residue	O
at	O
the	O
beginning	O
and	O
end	O
of	O
a	O
helix	O
or	O
strand	O
Hb	O
,	O
He	O
,	O
Eb	O
,	O
and	O
Ee	O
,	O
respectively	O
.	O
A	O
15-residue	O
sequence	O
window	O
is	O
used	O
with	O
20	B-DeepLearning
input	I-DeepLearning
layer	I-DeepLearning
units	O
per	O
residue	O
representing	O
the	O
PSSM	O
and	O
an	O
additional	O
spacer	O
unit	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
used	O
is	O
a	O
standard	O
feed-forward	B-DeepLearning
model	I-DeepLearning
as	O
described	O
in	O
Section	O
12.4	O
,	O
with	O
six	O
hidden	B-DeepLearning
layer	I-DeepLearning
units	I-DeepLearning
and	O
two	O
output	B-DeepLearning
units	I-DeepLearning
.	O
Indeed	O
,	O
this	O
work	O
will	O
focus	O
on	O
the	O
development	O
of	O
a	O
diagnosis	O
support	O
system	O
,	O
in	O
terms	O
of	O
its	O
knowledge	O
representation	O
and	O
reasoning	O
procedures	O
,	O
under	O
a	O
formal	O
framework	O
based	O
on	O
Logic	O
Programming	O
,	O
complemented	O
with	O
an	O
approach	O
to	O
computing	O
centered	O
on	O
Artificial	B-DeepLearning
Neural	I-DeepLearning
Networks	I-DeepLearning
,	O
to	O
evaluate	O
ACS	O
predisposing	O
and	O
the	O
respective	O
Degree-of-Confidence	O
that	O
one	O
has	O
on	O
such	O
a	O
happening	O
.	O
Each	O
base	O
pair	O
A	O
,	O
C	O
,	O
T	O
,	O
G	O
was	O
denoted	O
as	O
a	O
one-hot	B-DeepLearning
vector	I-DeepLearning
[	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
]	O
,	O
[	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
]	O
,	O
[	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
]	O
and	O
[	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
]	O
respectively	O
.	O
For	O
DNase-seq	O
data	O
and	O
each	O
histone	O
modification	O
data	O
,	O
TFBSs	O
and	O
non-TFBSs	O
were	O
described	O
as	O
one	B-DeepLearning
channel	I-DeepLearning
vector	I-DeepLearning
at	O
each	O
nucleotide	O
position	O
,	O
For	O
HMS	O
,	O
existing	O
methods	O
calculated	O
the	O
statistical	O
values	O
such	O
average	O
reads	O
number	O
in	O
each	O
base	O
pair	O
within	O
the	O
range	O
of	O
hundreds	O
or	O
thousands	O
nucleotide	O
.	O
For	O
example	O
,	O
if	O
we	O
used	O
DNase-seq	O
data	O
with	O
101	O
bp	O
,	O
the	O
vector	B-DeepLearning
was	I-DeepLearning
of	I-DeepLearning
size	I-DeepLearning
1	I-DeepLearning
x	I-DeepLearning
101	I-DeepLearning
for	O
a	O
sample	O
.	O
The	O
CNN	B-DeepLearning
consists	O
of	O
a	O
convolutional	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
max-pooling	B-DeepLearning
layer	I-DeepLearning
,	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
,	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
[	O
28	O
]	O
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
DNN	B-DeepLearning
consists	O
of	O
one	O
or	O
two	O
full	B-DeepLearning
connection	I-DeepLearning
layers	I-DeepLearning
,	O
a	O
dropout	B-DeepLearning
layer	I-DeepLearning
after	O
each	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
and	O
an	O
output	B-DeepLearning
layer	I-DeepLearning
.	O
For	O
CNN	B-DeepLearning
models	O
,	O
we	O
vary	O
the	O
number	B-DeepLearning
of	I-DeepLearning
kernels	I-DeepLearning
,	O
the	O
size	B-DeepLearning
of	I-DeepLearning
kernel	I-DeepLearning
window	O
,	O
and	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
.	O
For	O
DNN	B-DeepLearning
models	O
,	O
we	O
vary	O
the	O
number	B-DeepLearning
of	I-DeepLearning
layers	I-DeepLearning
,	O
and	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neural	I-DeepLearning
in	O
each	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
.	O
For	O
training	O
,	O
we	O
used	O
the	O
cross-entropy	B-DeepLearning
as	O
the	O
loss	B-DeepLearning
function	I-DeepLearning
.	O
Given	O
this	O
loss	B-DeepLearning
function	I-DeepLearning
and	O
different	O
hyper-parameters	B-DeepLearning
see	O
below	O
,	O
the	O
models	O
were	O
trained	O
using	O
the	O
standard	B-DeepLearning
error	I-DeepLearning
back-propagation	I-DeepLearning
algorithm	O
and	O
AdaDetla	B-DeepLearning
method	O
[	O
29	O
]	O
.	O
We	O
set	O
each	O
model	O
for	O
100	B-DeepLearning
epochs	I-DeepLearning
and	O
128	O
mini-batch	B-DeepLearning
size	I-DeepLearning
and	O
validated	O
the	O
model	O
after	O
each	O
epoch	O
.	O
Then	O
the	O
early-stop	B-DeepLearning
trick	O
was	O
used	O
to	O
stop	O
training	O
as	O
the	O
error	O
on	O
validation	B-DeepLearning
set	I-DeepLearning
is	O
higher	O
than	O
the	O
last	O
four	O
epochs	B-DeepLearning
.	O
The	O
best	O
model	O
was	O
chosen	O
according	O
to	O
the	O
accuracy	B-DeepLearning
on	O
the	O
validation	B-DeepLearning
set	I-DeepLearning
.	O
For	O
KNN	O
,	O
LR	O
,	O
RF	O
,	O
we	O
implemented	O
these	O
baselines	O
using	O
the	O
python	O
based	O
scikit-learn	B-DeepLearning
package	O
.	O
We	O
used	O
python	B-DeepLearning
and	O
Keras	B-DeepLearning
framework	O
to	O
train	O
neural	B-DeepLearning
networks	I-DeepLearning
.	O
We	O
used	O
python	B-DeepLearning
and	O
skcikit-learn	B-DeepLearning
to	O
train	O
conventional	O
machine	O
learning	O
methods	O
[	O
30	O
]	O
.	O
We	O
used	O
different	O
sample	B-DeepLearning
lengths	I-DeepLearning
to	O
train	O
the	O
CNN	B-DeepLearning
models	O
and	O
different	O
hyper-parameters	B-DeepLearning
for	O
each	O
length	O
.	O
For	O
each	O
length	O
,	O
we	O
selected	O
the	O
results	O
of	O
best	O
hyper-parameters	B-DeepLearning
.	O
Performance	O
evaluation	O
of	O
CNN	B-DeepLearning
with	O
respect	O
to	O
sample	B-DeepLearning
length	I-DeepLearning
and	O
model	O
structure	O
using	O
HMS	O
data	O
in	O
terms	O
of	O
the	O
distribution	O
of	O
AUCs	B-DeepLearning
across	O
256	B-DeepLearning
experiments	I-DeepLearning
.	O
The	O
effect	O
of	O
kernel	B-DeepLearning
number	I-DeepLearning
.	O
The	O
effect	O
of	O
neuron	B-DeepLearning
number	I-DeepLearning
.	O
The	O
effect	O
of	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
.	O
The	O
effect	O
of	O
sample	B-DeepLearning
length	I-DeepLearning
and	O
DNN	B-DeepLearning
model	I-DeepLearning
structure	I-DeepLearning
.	O
The	O
performance	O
comparison	O
of	O
DNN	B-DeepLearning
versus	O
CNN	B-DeepLearning
.	O
First	O
,	O
more	O
convolutional	B-DeepLearning
kernels	I-DeepLearning
could	O
also	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
2B	O
.	O
However	O
,	O
when	O
more	O
than	O
64	B-DeepLearning
kernels	I-DeepLearning
were	O
used	O
,	O
the	O
improvement	O
seemed	O
to	O
be	O
saturated	O
for	O
the	O
256	B-DeepLearning
experiments	I-DeepLearning
Fig.	O
2B	O
.	O
Second	O
,	O
more	O
neurons	O
in	O
the	O
full	B-DeepLearning
connection	I-DeepLearning
layer	I-DeepLearning
of	O
CNN	B-DeepLearning
could	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
2C	O
.	O
We	O
observe	O
that	O
small	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
achieves	O
better	O
performance	O
than	O
using	O
large	O
ones	O
Fig.	O
2D	O
while	O
big	O
kernel	B-DeepLearning
window	I-DeepLearning
size	I-DeepLearning
usually	O
used	O
in	O
sequence-based	O
CNN	B-DeepLearning
models	O
.	O
We	O
find	O
that	O
deeper	O
neural	B-DeepLearning
networks	I-DeepLearning
and	O
longer	O
sample	O
length	O
work	O
better	O
too	O
for	O
DNN	B-DeepLearning
Fig.	O
2E	O
.	O
However	O
,	O
the	O
performance	O
of	O
DNN	B-DeepLearning
is	O
still	O
slightly	O
worse	O
than	O
that	O
of	O
CNN	B-DeepLearning
,	O
indicating	O
the	O
importance	O
of	O
combining	O
convolution	O
operation	O
with	O
HMS	O
data	O
Fig.	O
2F	O
.	O
We	O
conducted	O
leave-one-feature-out	O
feature	O
selection	O
experiments	B-DeepLearning
to	O
train	O
the	O
CNN	B-DeepLearning
models	O
by	O
using	O
merely	O
four	O
histone	O
modifications	O
data	O
with	O
the	O
same	O
hyperparameters	B-DeepLearning
in	O
previous	O
section	O
.	O
For	O
model	O
architectures	O
,	O
more	O
convolutional	B-DeepLearning
kernels	I-DeepLearning
could	O
also	O
improve	O
the	O
prediction	O
performance	O
Fig.	O
4B	O
.	O
Thus	O
,	O
no	O
matter	O
what	O
the	O
data	O
type	O
is	O
,	O
the	O
additional	O
kernels	B-DeepLearning
are	O
beneficial	O
to	O
enhance	O
power	O
in	O
extracting	O
features	O
and	O
improve	O
model	O
performance	O
.	O
By	O
changing	O
the	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
last	O
dense	B-DeepLearning
layer	I-DeepLearning
of	O
CNN	B-DeepLearning
,	O
we	O
can	O
see	O
that	O
models	O
with	O
more	O
hidden	B-DeepLearning
neurons	I-DeepLearning
achieve	O
better	O
performance	O
Fig.	O
4C	O
.	O
We	O
also	O
see	O
that	O
CNN	B-DeepLearning
models	O
with	O
small	O
and	O
large	O
kernel	B-DeepLearning
window	I-DeepLearning
sizes	I-DeepLearning
4	O
and	O
24	O
achieve	O
almost	O
the	O
same	O
performance	O
for	O
different	O
sample	O
lengths	O
Fig.	O
4D	O
.	O
This	O
suggests	O
that	O
kernel	B-DeepLearning
window	I-DeepLearning
sizes	I-DeepLearning
4	O
and	O
24	O
could	O
not	O
distinctly	O
influence	O
DHS	O
data	O
information	O
.	O
For	O
comparison	O
with	O
CNN	B-DeepLearning
,	O
we	O
also	O
trained	O
DNN	B-DeepLearning
using	O
different	O
sequence	O
lengths	O
and	O
hyper-parameters	B-DeepLearning
for	O
the	O
DHS	O
data	O
.	O
Moreover	O
,	O
the	O
performance	O
of	O
DNN	B-DeepLearning
is	O
slightly	O
worse	O
than	O
that	O
of	O
CNN	B-DeepLearning
,	O
indicating	O
the	O
importance	O
of	O
combining	O
convolution	O
operation	O
with	O
DHS	O
data	O
Fig.	O
4F	O
.	O
In	O
both	O
HMS	O
and	O
DHS	O
cases	O
,	O
CNN	B-DeepLearning
perform	O
significantly	O
better	O
than	O
conventional	O
classifiers	O
in	O
term	O
of	O
the	O
distribution	O
of	O
AUCs	B-DeepLearning
across	O
256	B-DeepLearning
experiments	I-DeepLearning
Fig.	O
5	O
.	O
With	O
3D	O
convolution	B-DeepLearning
kernel	I-DeepLearning
in	O
3DCNN	B-DeepLearning
could	O
learn	O
more	O
spatial	O
information	O
than	O
the	O
standard	O
2D	O
convolutional	O
neural	B-DeepLearning
networks	I-DeepLearning
with	O
the	O
2D	B-DeepLearning
kernels	I-DeepLearning
.	O
Our	O
3DCNN	B-DeepLearning
is	O
a	O
seven	B-DeepLearning
layers	I-DeepLearning
deep	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
consists	O
the	O
layers	O
of	O
3D	B-DeepLearning
convolution	I-DeepLearning
,	O
batch	B-DeepLearning
normalization	I-DeepLearning
,	O
and	O
Softmax	B-DeepLearning
.	O
As	O
the	O
input	O
is	O
a	O
high-resolution	O
3D	O
MRI	O
data	O
,	O
3DCNN	B-DeepLearning
is	O
applied	O
deep	O
convolution	B-DeepLearning
layers	I-DeepLearning
with	O
small	O
kernel	B-DeepLearning
numbers	O
to	O
overcome	O
the	O
overfitting	B-DeepLearning
problem	O
.	O
Meanwhile	O
,	O
3DMaxPooling	B-DeepLearning
is	O
introduced	O
to	O
dimension	B-DeepLearning
reduction	I-DeepLearning
.	O
Except	O
for	O
the	O
output	B-DeepLearning
layer	I-DeepLearning
,	O
all	O
the	O
layers	O
are	O
3D	B-DeepLearning
convolution	I-DeepLearning
layers	I-DeepLearning
,	O
which	O
have	O
a	O
much	O
stronger	O
nonlinear	O
mapping	O
power	O
and	O
feature	O
extraction	O
capacity	O
than	O
traditional	O
2D	B-DeepLearning
convolution	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
.	O
At	O
last	O
,	O
due	O
to	O
the	O
3D	O
high	O
dimension	O
in	O
MRI	O
data	O
,	O
all	O
the	O
parameters	B-DeepLearning
and	O
convolution	B-DeepLearning
kernels	I-DeepLearning
in	O
3DCNN	B-DeepLearning
are	O
small	O
to	O
prevent	O
the	O
overfitting	B-DeepLearning
.	O
There	O
are	O
80	B-DeepLearning
samples	I-DeepLearning
in	O
our	O
MRI	O
dataset	B-DeepLearning
which	O
consists	O
40	O
BECT	O
instances	O
and	O
40	O
control	O
instances	O
.	O
Therefore	O
,	O
we	O
construct	O
a	O
traditional	O
2D	B-DeepLearning
convolution	I-DeepLearning
neural	B-DeepLearning
network	I-DeepLearning
2DCNN	B-DeepLearning
as	O
the	O
baseline	O
model	O
for	O
BECT	O
prediction	O
.	O
The	O
convolution	O
kernel	B-DeepLearning
sizes	I-DeepLearning
of	O
2DCNN	B-DeepLearning
are	O
set	O
the	O
same	O
3DCNN	B-DeepLearning
.	O
And	O
the	O
stride	B-DeepLearning
of	O
2DCNN	B-DeepLearning
is	O
modified	O
to	O
get	O
a	O
full	O
convolution	O
architecture	O
which	O
reduces	O
the	O
number	O
of	O
weights	B-DeepLearning
and	O
overcomes	O
the	O
overfitting	B-DeepLearning
problem	O
.	O
However	O
,	O
3DCNN	B-DeepLearning
extends	O
one	O
more	O
dimension	O
as	O
the	O
data	O
channel	O
dimension	O
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
.	O
The	O
detailed	O
comparison	O
of	O
2DCNN	B-DeepLearning
and	O
3DCNN	B-DeepLearning
are	O
shown	O
in	O
Table	O
1	O
.	O
We	O
evaluate	O
3DCNN	B-DeepLearning
on	O
our	O
MRI	O
dataset	B-DeepLearning
in	O
five-fold	B-DeepLearning
cross-validation	I-DeepLearning
.	O
The	O
whole	O
80-cases	O
dataset	B-DeepLearning
is	O
divided	O
into	O
training	B-DeepLearning
dataset	I-DeepLearning
54	B-DeepLearning
instances	I-DeepLearning
and	O
testing	B-DeepLearning
dataset	I-DeepLearning
16	B-DeepLearning
instances	I-DeepLearning
,	O
and	O
the	O
evaluation	O
criterion	O
is	O
the	O
prediction	O
accuracy	B-DeepLearning
.	O
The	O
learning	B-DeepLearning
rate	I-DeepLearning
is	O
set	O
to	O
1e	O
âˆ’	O
4	O
under	O
Adam	B-DeepLearning
optimizer	I-DeepLearning
.	O
Table	O
2	O
shows	O
the	O
performance	O
of	O
3DCNN	B-DeepLearning
and	O
2DCNN	B-DeepLearning
on	O
the	O
test	B-DeepLearning
dataset	I-DeepLearning
.	O
The	O
result	O
shows	O
3DCNN	B-DeepLearning
achieving	O
a	O
prediction	O
accuracy	B-DeepLearning
of	O
89.80%	O
on	O
the	O
five-fold	B-DeepLearning
cross-validation	I-DeepLearning
evaluation	O
,	O
which	O
outperforms	O
the	O
2DCNN	B-DeepLearning
baseline	O
model	O
8250%	O
.	O
Finally	O
,	O
the	O
dataset	B-DeepLearning
for	O
model	O
contains	O
32	B-DeepLearning
independent	I-DeepLearning
variables	I-DeepLearning
,	O
1	B-DeepLearning
dependent	I-DeepLearning
variable	I-DeepLearning
,	O
and	O
1	B-DeepLearning
time	I-DeepLearning
index	I-DeepLearning
.	O
We	O
test	O
4	O
different	O
LSTM	B-DeepLearning
models:	O
large/small	O
sample	O
size	O
that	O
the	O
predictor	O
variables	O
come	O
from	O
air	O
pollution	O
,	O
temperature	O
change	O
,	O
and	O
google	O
trends	O
datasets	B-DeepLearning
large/small	O
sample	O
size	O
that	O
only	O
use	O
google	O
trends	O
data	O
as	O
predictor	O
variables	O
.	O
The	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
input	B-DeepLearning
layer	I-DeepLearning
was	O
equal	O
to	O
the	O
number	O
of	O
input	B-DeepLearning
data	I-DeepLearning
points	O
for	O
each	O
attribute	O
.	O
The	O
optimal	O
number	B-DeepLearning
of	I-DeepLearning
neurons	I-DeepLearning
in	O
the	O
hidden	B-DeepLearning
layer	I-DeepLearning
was	O
determined	O
through	O
experimentation	O
for	O
minimizing	O
the	O
error	O
at	O
the	O
best	O
epoch	B-DeepLearning
for	O
each	O
network	O
individually	O
.	O
An	O
upper	O
limit	O
for	O
the	O
total	O
number	B-DeepLearning
of	I-DeepLearning
weight	I-DeepLearning
connections	I-DeepLearning
was	O
set	O
to	O
half	O
of	O
the	O
total	O
number	O
of	O
input	B-DeepLearning
vectors	I-DeepLearning
to	O
avoid	O
overfitting	B-DeepLearning
,	O
as	O
suggested	O
previously	O
Andrea	O
and	O
Kalayeh	O
,	O
1991	O
.	O
During	O
predictions	O
,	O
the	O
network	O
was	O
fed	O
with	O
new	O
data	O
from	O
the	O
sequences	O
that	O
were	O
not	O
part	O
of	O
training	B-DeepLearning
set	I-DeepLearning
.	O
At	O
this	O
threshold	O
value	O
of	O
Pad	O
,	O
the	O
Matthew's	B-DeepLearning
correlation	I-DeepLearning
coefficient	I-DeepLearning
was	O
observed	O
to	O
be	O
highest	O
094	O
.	O
The	O
first	O
neural	B-DeepLearning
network	I-DeepLearning
NN1	B-DeepLearning
predicts	O
the	O
residue	O
contact	O
number	O
on	O
the	O
basis	O
of	O
the	O
PSSM	O
weights	O
.	O
The	O
structure	O
of	O
the	O
NN1	B-DeepLearning
is	O
as	O
follows	O
.	O
The	O
PSSM	O
values	O
are	O
the	O
input	B-DeepLearning
parameters	I-DeepLearning
.	O
The	O
complete	O
vector	O
of	O
the	O
input	B-DeepLearning
data	I-DeepLearning
for	O
the	O
NN1	B-DeepLearning
thus	O
composed	O
of	O
2/z	O
+	O
1	O
x	O
1	O
=	O
11	O
x	O
21	O
=231	B-DeepLearning
parameters	I-DeepLearning
.	O
Two	B-DeepLearning
internal	I-DeepLearning
layers	I-DeepLearning
with	O
50	O
and	O
3	B-DeepLearning
neurons	I-DeepLearning
were	O
implied	O
for	O
the	O
NN1	B-DeepLearning
.	O
A	O
single	O
number	O
was	O
at	O
the	O
NN1	B-DeepLearning
output	O
,	O
the	O
predicted	O
value	O
of	O
the	O
contact	O
number	O
of	O
the	O
rth	O
residue	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
at	O
the	O
second	O
level	O
NN2	B-DeepLearning
was	O
built	O
as	O
follows	O
.	O
The	O
contact	O
numbers	O
predicted	O
by	O
NN1	B-DeepLearning
served	O
as	O
its	O
input	B-DeepLearning
data	I-DeepLearning
41	O
values	O
were	O
estimated	O
for	O
each	O
position	O
,	O
namely	O
,	O
the	O
predicted	O
contact	O
numbers	O
at	O
the	O
/th	O
and	O
rth	O
Â±	O
20	O
positions	O
.	O
In	O
this	O
way	O
,	O
42	B-DeepLearning
parameters	I-DeepLearning
were	O
at	O
the	O
NN2	B-DeepLearning
input	O
the	O
NN2	B-DeepLearning
contained	O
one	O
internal	O
layer	O
of	O
10	B-DeepLearning
neurons	I-DeepLearning
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
of	O
the	O
algorithm	O
was	O
implemented	O
on	O
a	O
sample	O
of	O
the	O
234	O
monomeric	O
protein	O
chains	O
with	O
known	O
spatial	O
structures	O
extracted	O
from	O
the	O
PDB	B-DeepLearning
database	I-DeepLearning
Berman	O
et	O
al.	O
,	O
2000	O
and	O
belonging	O
to	O
different	O
protein	O
fold	O
types	O
according	O
to	O
the	O
SCOP	O
classification	O
Andreeva	O
et	O
al.	O
,	O
2004	O
.	O
The	O
neural	B-DeepLearning
network	I-DeepLearning
was	O
trained	O
on	O
the	O
training	O
sample	O
by	O
the	O
backpropagation	B-DeepLearning
algorithm	O
Rumelhart	O
et	O
al.	O
,	O
1986	O
,	O
with	O
50	B-DeepLearning
epochs	I-DeepLearning
,	O
momentum	B-DeepLearning
of	I-DeepLearning
0.9	I-DeepLearning
,	O
and	O
learning	B-DeepLearning
rate	I-DeepLearning
of	O
001	O
.	O
The	O
prediction	O
accuracy	B-DeepLearning
was	O
estimated	O
on	O
the	O
testing	B-DeepLearning
data	I-DeepLearning
.	O
The	O
second	O
level	O
neural	B-DeepLearning
network	I-DeepLearning
NN2	I-DeepLearning
enables	O
the	O
improvement	O
of	O
prediction	O
accuracy	B-DeepLearning
.	O
NN2	B-DeepLearning
was	O
introduced	O
to	O
additionally	O
take	O
into	O
consideration	O
the	O
interdependence	O
of	O
the	O
contact	O
number	O
of	O
residues	O
neighboring	O
along	O
the	O
protein	O
chain	O
.	O
The	O
NN2	B-DeepLearning
introduces	O
,	O
although	O
slight	O
,	O
yet	O
regular	O
improvement	O
in	O
prediction	O
accuracy	B-DeepLearning
.	O
Similarly	O
to	O
VL-XT	O
,	O
a	O
neural	B-DeepLearning
network	I-DeepLearning
with	O
a	O
fully	B-DeepLearning
connected	I-DeepLearning
hidden	I-DeepLearning
layer	I-DeepLearning
of	O
ten	B-DeepLearning
neurons	I-DeepLearning
was	O
trained	O
on	O
the	O
specific	O
datasets	O
and	O
it	O
outputs	O
a	O
value	O
for	O
the	O
central	O
amino	O
acid	O
in	O
the	O
window	O
.	O
Using	O
an	O
original	O
approach	O
,	O
RONN	B-DeepLearning
Regional	B-DeepLearning
Order	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
[	O
82	O
]	O
recognizes	O
disordered	O
segments	O
based	O
on	O
their	O
similarity	O
to	O
well-characterized	O
prototype	O
sequences	O
with	O
known	O
disordered	O
status	O
.	O
The	O
resulting	O
homology	O
scores	O
are	O
converted	O
into	O
distances	O
and	O
are	O
used	O
to	O
train	O
a	O
modified	O
version	O
of	O
radial	O
basis	O
function	O
networks	O
called	O
a	O
bio-basis	B-DeepLearning
function	I-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
.	O
Seven	O
physical	B-DeepLearning
parameters	I-DeepLearning
describing	O
the	O
physicochemical	O
properties	O
of	O
the	O
residue:	O
a	O
steric	O
parametergraph	O
shape	O
index	O
,	O
hydrophobicity	O
,	O
volume	O
,	O
polarizability	O
,	O
isoelectric	O
point	O
,	O
helix	O
probability	O
,	O
and	O
sheet	O
probability	O
.	O
Two	O
hidden	O
layers	O
,	O
each	O
with	O
71	B-DeepLearning
nodes	I-DeepLearning
,	O
were	O
used	O
for	O
a	O
preprocessing	O
network	O
.	O
Probability	O
predictions	O
from	O
this	O
network	O
were	O
further	O
refined	O
using	O
a	O
filter	O
network	O
with	O
a	O
single	O
hidden	B-DeepLearning
layer	I-DeepLearning
of	O
51	B-DeepLearning
nodes	I-DeepLearning
.	O
Training	B-DeepLearning
and	I-DeepLearning
testing	I-DeepLearning
for	O
all	O
neural	B-DeepLearning
networks	I-DeepLearning
considered	O
here	O
was	O
preformed	O
on	O
the	O
SPINE	B-DeepLearning
dataset	I-DeepLearning
[	O
18	O
]	O
.	O
In	O
general	O
the	O
networks	O
used	O
to	O
predict	O
Ïˆ	O
angles	O
were	O
composed	O
of	O
two	O
hidden	B-DeepLearning
layers	I-DeepLearning
,	O
each	O
with	O
51	B-DeepLearning
nodes	I-DeepLearning
.	O
Ten	B-DeepLearning
fold	I-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
will	O
be	O
used	O
to	O
judge	O
the	O
accuracy	B-DeepLearning
of	O
the	O
predictions	O
.	O
We	O
use	O
a	O
bipolar	O
activation	B-DeepLearning
function	I-DeepLearning
given	O
by	O
f	O
x	O
=	O
tanhÎ±x	O
,	O
with	O
Î±	O
=	O
0.2	O
,	O
momentum	B-DeepLearning
of	I-DeepLearning
0.4	I-DeepLearning
,	O
and	O
the	O
back-propagation	B-DeepLearning
method	O
with	O
relatively	O
slow	O
learning	O
learning	B-DeepLearning
rate	I-DeepLearning
0.001	O
to	O
optimize	O
the	O
weights	B-DeepLearning
.	O
To	O
determine	O
the	O
quality	O
of	O
the	O
prediction	O
we	O
use	O
the	O
Mean	B-DeepLearning
Absolute	I-DeepLearning
Error	I-DeepLearning
MAE	B-DeepLearning
in	O
degrees	O
,	O
Pearsonâ€™s	B-DeepLearning
correlation	I-DeepLearning
coefficient	I-DeepLearning
pc	O
,	O
and	O
the	O
probability	O
that	O
the	O
predicted	O
and	O
native	O
angles	O
are	O
separated	O
by	O
less	O
than	O
10%	O
Q10p	O
.	O
We	O
use	O
10-fold	B-DeepLearning
cross-validations	I-DeepLearning
[	O
35	O
]	O
to	O
estimate	O
the	O
accuracy	O
over	O
the	O
set	O
.	O
To	O
test	O
for	O
possible	O
overfit	B-DeepLearning
issues	O
we	O
take	O
secondary	O
structure	O
predictions	O
based	O
on	O
the	O
weights	B-DeepLearning
trained	O
for	O
the	O
first	O
cross-validation	B-DeepLearning
fold	O
and	O
compare	O
angle	O
prediction	O
between	O
the	O
folds	O
.	O
Firpi	O
et	O
al.	O
2010	O
developed	O
a	O
method	O
called	O
CSI-ANN	O
based	O
on	O
a	O
time-delayed	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
TDNN	B-DeepLearning
framework	O
to	O
predict	O
enhancers	O
in	O
HeLa	O
and	O
Human	O
CD4+	O
T	O
cells	O
.	O
A	O
model	O
particularly	O
well	O
suited	O
for	O
the	O
tasks	O
of	O
interest	O
is	O
the	O
Bidirectional	B-DeepLearning
Recurrent	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
BRNN	B-DeepLearning
[	O
6	O
]	O
.	O
The	O
model	O
they	O
used	O
is	O
a	O
fully	O
connected	O
Multi-Layer	B-DeepLearning
Perceptron	I-DeepLearning
MLP	B-DeepLearning
with	O
one	O
hidden	B-DeepLearning
layer	I-DeepLearning
[	O
4	O
]	O
Haykin	O
.	O
They	O
randomly	O
weighted	O
each	O
input	O
and	O
used	O
back-propagation	B-DeepLearning
[	O
4	O
]	O
totrain	O
the	O
system	O
on	O
a	O
PDB	O
file	O
chosen	O
from	O
the	O
PDB	O
.	O
Moreover	O
,	O
owing	O
to	O
the	O
rapid	O
growth	O
of	O
the	O
PDB	B-DeepLearning
database	I-DeepLearning
,	O
the	O
training	B-DeepLearning
sets	I-DeepLearning
for	O
ANNs	B-DeepLearning
have	O
also	O
increased	O
.	O
Since	O
these	O
artificial	O
intelligence	O
methods	O
are	O
based	O
on	O
an	O
empirical	O
risk-minimization	O
principle	O
,	O
they	O
have	O
some	O
disadvantages	O
,	O
for	O
example	O
,	O
finding	O
the	O
local	B-DeepLearning
minimal	I-DeepLearning
instead	O
of	O
global	B-DeepLearning
minimal	I-DeepLearning
having	O
low	O
convergence	B-DeepLearning
rate	O
more	O
prone	O
to	O
overfitting	B-DeepLearning
and	O
when	O
the	O
size	O
of	O
fault	O
samples	O
is	O
limited	O
,	O
it	O
might	O
cause	O
poor	O
generalization	B-DeepLearning
.	O
As	O
a	O
collaborative	O
filter	O
,	O
we	O
use	O
General	B-DeepLearning
Regression	I-DeepLearning
Neural	I-DeepLearning
Network	I-DeepLearning
GRNN	B-DeepLearning
.	O
In	O
the	O
literature	O
,	O
GRNN	B-DeepLearning
was	O
shown	O
to	O
be	O
effective	O
in	O
noisy	O
environments	O
as	O
it	O
deals	O
with	O
sparse	B-DeepLearning
data	I-DeepLearning
effectively	O
.	O
The	O
method	O
makes	O
use	O
of	O
RDKit	B-DeepLearning
,	O
an	O
open-source	O
cheminformatics	O
software	O
,	O
allowing	O
the	O
incorporation	O
of	O
any	O
global	O
molecular	O
such	O
as	O
molecular	O
charge	O
and	O
local	O
such	O
as	O
atom	O
type	O
information	O
.	O
We	O
evaluate	O
the	O
method	O
on	O
the	O
Side	B-DeepLearning
Effect	I-DeepLearning
Resource	I-DeepLearning
SIDE	I-DeepLearning
v4.1	I-DeepLearning
dataset	I-DeepLearning
and	O
show	O
that	O
it	O
significantly	O
outperforms	O
another	O
recently	O
proposed	O
method	O
based	O
on	O
deep	O
convolutional	B-DeepLearning
neural	I-DeepLearning
networks	I-DeepLearning
.	O
Surprisingly	O
,	O
while	O
being	O
one	O
of	O
the	O
best	O
performing	O
methods	O
on	O
all	O
other	O
datasets	O
in	O
[	O
19	O
]	O
their	O
presented	O
DCNN	B-DeepLearning
performed	O
very	O
poorly	O
on	O
the	O
SIDER	B-DeepLearning
dataset	I-DeepLearning
and	O
were	O
outperformed	O
by	O
traditional	O
methods	O
such	O
as	O
random	O
forest	O
and	O
logistic	B-DeepLearning
regression	I-DeepLearning
.	O
Several	O
authors	O
overcome	O
this	O
flaw	O
by	O
either	O
using	O
a	O
recurrent	B-DeepLearning
neural	I-DeepLearning
network	I-DeepLearning
RNN	B-DeepLearning
or	O
a	O
DCNN	B-DeepLearning
.	O
Wallach	O
et	O
al.	O
[	O
17	O
]	O
for	O
example	O
apply	O
a	O
three	O
dimensional	O
DCNN	B-DeepLearning
on	O
the	O
spatial	O
structure	O
of	O
molecules	O
.	O
Due	O
to	O
the	O
conventions	O
of	O
the	O
field	O
,	O
F	O
will	O
be	O
defined	O
as	O
the	O
leaky	B-DeepLearning
ReLU	I-DeepLearning
function	O
[	O
12	O
]	O
in	O
the	O
rest	O
of	O
this	O
paper	O
.	O
The	O
H	O
in	O
Eq.	O
5	O
represents	O
an	O
arbitrary	O
activation	B-DeepLearning
function	I-DeepLearning
.	O
In	O
this	O
paper	O
the	O
Leaky	B-DeepLearning
ReLU	I-DeepLearning
function	O
will	O
be	O
used	O
for	O
Hl	O
except	O
in	O
the	O
final	B-DeepLearning
layer	I-DeepLearning
where	O
the	O
sigmoid	B-DeepLearning
function	O
will	O
be	O
used	O
instead	O
.	O
The	O
sigmoid	B-DeepLearning
function	O
is	O
selected	O
here	O
since	O
we	O
want	O
the	O
output	O
of	O
our	O
model	O
to	O
be	O
in	O
the	O
range	O
of	O
0	O
to	O
1	O
.	O
To	O
train	O
and	O
test	O
our	O
proposed	O
model	O
we	O
use	O
the	O
publicly	O
available	O
SIDER	B-DeepLearning
dataset	I-DeepLearning
presented	O
in	O
[	O
19	O
]	O
.	O
This	O
dataset	B-DeepLearning
contains	O
information	O
on	O
molecules	O
,	O
marketed	O
as	O
medicines	O
,	O
and	O
their	O
recorded	O
side	O
effects	O
.	O
The	O
data	O
originates	O
from	O
the	O
SIDER	B-DeepLearning
database	I-DeepLearning
.	O
The	O
original	O
data	O
consist	O
of	O
1430	B-DeepLearning
molecules	I-DeepLearning
and	O
5868	B-DeepLearning
different	I-DeepLearning
types	I-DeepLearning
of	I-DeepLearning
side	I-DeepLearning
effects	I-DeepLearning
.	O
However	O
,	O
in	O
the	O
dataset	B-DeepLearning
presented	O
by	O
Wu	O
et	O
al.	O
[	O
19	O
]	O
,	O
similar	O
side	O
effects	O
are	O
grouped	O
together	O
,	O
leaving	O
the	O
dataset	O
with	O
28	B-DeepLearning
groups	I-DeepLearning
of	I-DeepLearning
side	I-DeepLearning
effects	I-DeepLearning
.	O
RDKit	B-DeepLearning
is	O
also	O
used	O
to	O
extract	O
information	O
about	O
the	O
atoms	O
,	O
bonds	O
and	O
molecules	O
,	O
such	O
as	O
chirality	O
and	O
molecular	O
weight	O
.	O
The	O
model	O
described	O
earlier	O
in	O
Sect.	O
3	O
is	O
implemented	O
in	O
Theano	B-DeepLearning
[	O
15	O
]	O
and	O
with	O
a	O
network	O
architecture	O
that	O
is	O
as	O
similar	O
as	O
possible	O
as	O
the	O
architecture	O
presented	O
by	O
Wu	O
et	O
al.	O
[	O
19	O
]	O
.	O
Two	B-DeepLearning
convolutional	I-DeepLearning
layers	I-DeepLearning
,	O
each	O
with	O
64	B-DeepLearning
neurons	I-DeepLearning
,	O
will	O
therefore	O
be	O
used	O
.	O
These	O
layers	O
are	O
then	O
followed	O
by	O
a	O
single	O
fully	B-DeepLearning
connected	I-DeepLearning
layer	I-DeepLearning
,	O
with	O
128	B-DeepLearning
neurons	I-DeepLearning
.	O
Between	O
each	O
layer	O
in	O
our	O
model	O
we	O
use	O
a	O
10%	O
dropout	B-DeepLearning
rate	O
[	O
14	O
]	O
.	O
The	O
models	O
are	O
trained	O
by	O
minimizing	O
the	O
cross	B-DeepLearning
entropy	I-DeepLearning
error	I-DeepLearning
.	O
This	O
is	O
achieved	O
by	O
optimizing	O
the	O
values	O
of	O
all	O
free	O
parameters	B-DeepLearning
Wl	O
and	O
V	O
l	O
using	O
the	O
ADAM	B-DeepLearning
optimization	O
algorithm	O
[	O
7	O
]	O
.	O
Each	O
network	O
is	O
trained	O
for	O
200	B-DeepLearning
epochs	I-DeepLearning
using	O
a	O
batch	B-DeepLearning
size	I-DeepLearning
of	O
20	O
examples	O
.	O
The	O
performance	O
of	O
our	O
model	O
on	O
the	O
presented	O
experimental	O
set-ups	O
is	O
measured	O
as	O
the	O
mean	O
AUC-ROC	B-DeepLearning
value	O
,	O
averaged	O
over	O
all	O
trials	O
and	O
all	O
target	O
variables	O
.	O
The	O
network	O
mainly	O
consists	O
of	O
three	B-DeepLearning
convolution	I-DeepLearning
blocks	I-DeepLearning
and	O
two	B-DeepLearning
fully-connected	I-DeepLearning
blocks	I-DeepLearning
,	O
as	O
well	O
as	O
flattening	O
and	O
concatenating	O
layers	O
that	O
bridge	O
the	O
two	O
types	O
of	O
blocks	O
.	O
Each	O
convolution	B-DeepLearning
block	I-DeepLearning
is	O
composed	O
of	O
a	O
convolution	B-DeepLearning
layer	I-DeepLearning
with	O
ReLU	B-DeepLearning
activation	O
,	O
followed	O
by	O
a	O
max	O
pooling	O
layer	O
,	O
and	O
a	O
dropout	O
layer	O
.	O
The	O
first	O
convolution	B-DeepLearning
layer	I-DeepLearning
filters	O
the	O
input	O
of	O
size	O
1280	O
Ã—	O
1024	O
Ã—	O
2	O
using	O
16	B-DeepLearning
kernels	I-DeepLearning
of	O
size	O
10	O
Ã—	O
10	O
with	O
a	O
horizontal	O
stride	B-DeepLearning
of	O
10	O
pixels	O
and	O
a	O
vertical	O
stride	B-DeepLearning
of	O
8	O
pixels	O
.	O
The	O
result	O
image	O
of	O
size	O
128Ã—128Ã—16	O
is	O
then	O
shrunk	O
to	O
64Ã—64Ã—16	O
in	O
the	O
following	O
max	B-DeepLearning
pooling	I-DeepLearning
layer	O
.	O
The	O
second	O
convolution	B-DeepLearning
layer	I-DeepLearning
filters	O
it	O
using	O
32	B-DeepLearning
kernels	I-DeepLearning
of	O
size	O
3	O
Ã—	O
3	O
with	O
a	O
stride	B-DeepLearning
of	O
2	O
pixels	O
.	O
Through	O
a	O
similar	O
process	O
,	O
the	O
input	O
of	O
the	O
third	O
convolution	B-DeepLearning
layer	I-DeepLearning
becomes	O
of	O
size	O
16Ã—16Ã—32	O
and	O
at	O
the	O
end	O
of	O
the	O
block	O
,	O
the	O
input	O
is	O
flattened	O
to	O
a	O
vector	B-DeepLearning
of	O
length	O
1024	O
from	O
4	O
Ã—	O
4	O
Ã—	O
64	O
.	O
The	O
vector	B-DeepLearning
is	O
then	O
concatenated	O
with	O
the	O
second	O
input	O
,	O
a	O
vector	B-DeepLearning
of	O
eight	O
metrics	O
,	O
and	O
delivered	O
to	O
a	O
fully-connected	B-DeepLearning
layer	I-DeepLearning
of	O
512	B-DeepLearning
neurons	I-DeepLearning
.	O
Finally	O
,	O
the	O
vector	B-DeepLearning
of	O
length	O
512	O
goes	O
through	O
a	O
sigmoid	B-DeepLearning
function	O
to	O
generate	O
the	O
output	O
.	O
We	O
conducted	O
the	O
self-assembly	O
experiment	B-DeepLearning
for	O
four	O
hours	O
and	O
recorded	O
images	O
of	O
size	O
1280	O
Ã—	O
1024	O
at	O
1	O
fps	O
,	O
leading	O
to	O
over	O
14000	O
pairs	O
of	O
front	O
and	O
right	O
images	O
.	O
In	O
total	O
,	O
725	O
positive	O
real	O
pairs	O
,	O
2475	O
negative	O
real	O
pairs	O
,	O
12000	O
positive	O
synthetic	O
pairs	O
,	O
and	O
12000	O
negative	O
synthetic	O
pairs	O
are	O
utilized	O
in	O
our	O
experiment	B-DeepLearning
.	O
Note	O
that	O
the	O
presented	O
accuracies	O
are	O
the	O
mean	O
accuracy	O
over	O
10-fold	B-DeepLearning
cross	I-DeepLearning
validation	I-DeepLearning
except	O
the	O
case	O
when	O
only	O
the	O
synthetic	O
data	O
is	O
used	O
as	O
a	O
training	B-DeepLearning
set	I-DeepLearning
.	O
Our	O
algorithm	O
reaches	O
an	O
accuracy	B-DeepLearning
of	O
93.2%	O
when	O
it	O
is	O
trained	O
with	O
labeled	O
real	O
images	O
.	O
It	O
justifies	O
our	O
utilization	O
of	O
synthetic	O
data	O
,	O
as	O
it	O
recorded	O
88.7%	O
accuracy	B-DeepLearning
when	O
only	O
the	O
real	O
data	O
are	O
used	O
as	O
a	O
training	B-DeepLearning
set	I-DeepLearning
.	O
Our	O
classifier	O
also	O
shows	O
promising	O
results	O
with	O
synthetic	O
images	O
,	O
reaching	O
an	O
accuracy	B-DeepLearning
of	O
86%	O
.	O
