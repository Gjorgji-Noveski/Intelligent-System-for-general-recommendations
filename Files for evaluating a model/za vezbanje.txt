Current neural network models, mostly concerned with bottom-up processes, such as finding optimal parameters for a given set of data (which correspond to the stimuli of experiments), do not incorporate top-down information, such as preselecting features or internal knowledge. New experimental results, however, show that attention and other higher cortical processes play an important role in perceptual learning issues. One important topic is the question of how specific the improvement achieved through learning is, and how much it can generalize. [3] The Gestalt’s prior rule and Marr’s constraint can also be viewed as top-down information. Models of perceptual learning can be broadly divided into two classes: feedforward versus feedback. The best-known model of feedforward type is that conceived by Poggio on visual hyperacuity. The main appeal of the model is that it can perform the specific of perceptual learning, use very limited number of neurons and learned quickly. However, it has some drawbacks that it needs a teaching signal. One of the feedback network model is Adini’s contrast discrimination model. An attractive feature of this model is that it does not require a teaching signal, but it cannot account for task-specificity. Combined model and reinforcement learning algorithm also proposed by researchers. In general, perceptual learning is the real brain’s challenges to models: perceptual learning is highly specific, it requires repetition but not feedback, and it involves top-down influences. This idea was proposed by many learning theories. Regularization theory [4] demonstrated that the not “self-evident” method of minimizing the regularized functional does work. Moreover, Vapnik’s statistical learning theory [5] uses the Structural Risk Minimization (SRM) inductive principle to substitute the Empirical Risk Minimization (ERM) inductive principal. The Minimum Description Length (MDL) principle [6] originated in algebra coding theory states that the best model for describing a set of data is the one that permits the greatest compression of the data description. Many researcher proposed the comprise method to interpret and realize this occam’ razor. These methods are similar at fitting term but are quite different at complexity term. Akaike Information Criterion (AIC) [7] only use the number of parameters as the term of complexity; Bayesian Information Criterion (BIC) [8] include the sample size in its complexity term; Rianen’s Stochastic Complexity (SC) based on MDL [9,10], Most of these theories fail to meet the crucial requirement of being invariant under reparameterization of the model, the condition that any meaningfully interpretable method of model selection must satisfy.  3 Top-Down Information Used in Model Selection In differential geometric perspective, a statistical model is a set of probability distributions described by a parameter set [11]. This parameter set forms a manifold which is called model manifold. It is the submanifold of the manifold formed by all the probability distributions. The probability distributions describing true information process can also form a submanifold but it is generally not the same with the model manifold. Thus to approximate the true information process with statistical model can be changed into research the geometrical relation (shape and relative position) of the two submanifolds in their enveloping manifold. A Neural Networks (NN) including modifiable parameters (connection weights and thresholds) ( ... ) θ = θ 1 θ n can be described by a statistical model, thus the set of all the possible neural networks realized by changing θ in the parameter space Θ forms a n-dimensional manifold S called the neuro-manifold, whereθ plays the role of a coordinate system of S [11]. S is a flat dual Riemannian manifold and the submanifold of manifold P formed by all probability distributions. Each point in S represents a network depicted by a probability distribution p(x;θ ) or ) p( y | x;θ . A manifold usually are not flat, it can be analogy with the curved surface. When we define the Riemannian metric on the manifold, we can get the unique Riemannian curvature tensor, and can use the curvature to describe the property of this manifold. If we denote the volume element of n-dimension manifold with ds , then the r th integral of mean curvature and Gauss-Kronecker curvature [12].