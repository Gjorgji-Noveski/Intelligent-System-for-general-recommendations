{"content": "The initial learning rate was set to 0.0005 as shown in Table 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 12, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 21, "end": 24}]}]}
{"content": "Deep Voice 3 is a fully convolutional architecture for speech synthesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 24, "end": 36}]}]}
{"content": "Its character-to-spectrogram architecture enables fully parallel computation and the training is much faster than at the RNN architectures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 121, "end": 123}]}]}
{"content": "Those features are in a key value form and they are fed into the attention-based decoder ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "attention-based", "start": 65, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decoder", "start": 81, "end": 87}]}]}
{"content": "The hidden layers of the decoder are fed into the third converter layer which is capable of predicting the acoustic features for waveform synthesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decoder", "start": 25, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 11, "end": 16}]}]}
{"content": "The training of the model is followed by validation which means that each 10K steps are evaluated before the training process proceeds ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}, {"start": 109, "end": 116, "text": "training"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 41, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "steps", "start": 78, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 13, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 16, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 20, "end": 24}]}]}
{"content": "The evaluation is done on external unknown sentences that provide insight into the advancement of the learnt dependencies between the dataset and the hidden layer weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 150, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 157, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 163, "end": 169}]}]}
{"content": "Deep Voice 3 suggested that parameters worked perfectly for our model thus we used the same hyperparameters without increasing the demand due to our resource limitations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 28, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 92, "end": 106}]}]}
{"content": "The model started to produce an intelligible understandable and partially human-like speech after 50 K steps as observed from the figure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "steps", "start": 103, "end": 107}]}]}
{"content": "Loss function is a metric that refers to the accuracy of the prediction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Loss", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 45, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "prediction", "start": 61, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 5, "end": 12}]}]}
{"content": "The main objective is to minimize the model errors or minimize the loss function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 38, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 67, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 72, "end": 79}]}]}
{"content": "In our case the loss functions behaves in a desired manner it gradually decreases converging to a value of 0.1731 after 162.2 K steps in four days and 11 h of training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "converging", "start": 82, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "steps", "start": 128, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 159, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 21, "end": 29}]}]}
{"content": "Learning rate plays a vital role in minimizing the loss function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 51, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 9, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 56, "end": 63}]}]}
{"content": "The gradient norm that is presented in the same figure calculates the L2 norm of the gradients of the last layer of the Deep learning network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 70, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradients", "start": 85, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "last", "start": 102, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 120, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "norm", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 125, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 134, "end": 140}]}]}
{"content": "It is an indicator showing whether the weights of the Deep learning network are properly updated ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 39, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 54, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 59, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 68, "end": 74}]}]}
{"content": "This problem affects the upper layers of the Deep learning network making it really hard for the network to learn and tune the parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "layers", "start": 31, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 45, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 127, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 50, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 59, "end": 65}]}]}
{"content": "In such case the model is unstable and it is not able to learn from data since the accumulation of large error gradients during the training process result in very large updates in the Deep learning model weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 105, "end": 109}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 132, "end": 139}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 185, "end": 188}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradients", "start": 111, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "process", "start": 141, "end": 147}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 190, "end": 197}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 199, "end": 203}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 205, "end": 211}]}]}
{"content": "By principles of transfer learning we tried to fine-tune the Russian TTS model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "transfer", "start": 17, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fine-tune", "start": 47, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "TTS", "start": 69, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 26, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "model", "start": 73, "end": 77}]}]}
{"content": "So as to automatically and accurately classify the FCGR encoded data we experimentally compared Multilayer Perceptron Artificial Neural Network MLP-ANN Support Vector Machine SVM and Na\u00c3\u00afve Bayes NB which are frontline pattern recognition tools in machine learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 96, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 118, "end": 127}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP-ANN", "start": 144, "end": 150}]}, {"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 248, "end": 254}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 107, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 129, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 136, "end": 142}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 256, "end": 263}]}]}
{"content": "The MLP-ANN contains 64 neurons in the input layer 64 element FCGR 6 neurons in the output layer 5 mutation classes and 1 normal class and two hidden layers with the neurons experimentally varied from 10 to 100 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP-ANN", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neurons", "start": 24, "end": 30}, {"start": 69, "end": 75, "text": "neurons"}, {"start": 166, "end": 172, "text": "neurons"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 39, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 84, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 143, "end": 148}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 45, "end": 49}, {"start": 91, "end": 95, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 150, "end": 155}]}]}
{"content": "The result obtained by varying the neurons in the hidden layer is reported in Sect 3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neurons", "start": 35, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 57, "end": 61}]}]}
{"content": "Deep neural networks DNN or deep learning models were proved to be able to extract automatically useful features from input patterns ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 21, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 28, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 12, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 33, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 42, "end": 47}]}]}
{"content": "Under this framework Long Short-Term Memory LSTM is a recurrent unit that reads a sequence one step at a time and can exploit long range relations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 21, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 44, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 54, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-Term", "start": 26, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 37, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 64, "end": 67}]}]}
{"content": "In this work we propose a DNN model for nucleosome identification on sequences from three different species ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 26, "end": 28}]}]}
{"content": "Recently deep neural networks or deep learning models were proved to be able to automatically extract useful features from input patterns with no a priori information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}, {"start": 33, "end": 36, "text": "deep"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 21, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 38, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 47, "end": 52}]}]}
{"content": "The two main categories of deep neural models are Convolutional Neural Networks CNN and Recurrent Neural Networks RNN ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 27, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 50, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 80, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Recurrent", "start": 88, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 114, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 32, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 39, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 64, "end": 69}, {"start": 98, "end": 103, "text": "Neural"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 71, "end": 78}, {"start": 105, "end": 112, "text": "Networks"}]}]}
{"content": "CNNs are characterized by an initial layer of convolutional filters followed by a non Linearity a sub-sampling and a fully connected layer which realized the final classification ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 29, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 46, "end": 58}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 117, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 37, "end": 41}, {"start": 133, "end": 137, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "filters", "start": 60, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 123, "end": 131}]}]}
{"content": "Conversely in this work we want to avoid the feature extraction step in order to fully exploit the capabilities of DNNs making use of a convolutional layer for extracting features from local sequences of nucleotides and an LSTM to take into account longer-range positional information ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 115, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 136, "end": 148}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 223, "end": 226}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 150, "end": 154}]}]}
{"content": "Another important component of a deep neural network is the max-pooling layer that usually follows the recurrent or convolutional layers in the computation flow ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 33, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 60, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 116, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 38, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 45, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 130, "end": 135}]}]}
{"content": "The dropout layer randomly sets to zero the output from the preceding layer during training with a probability p given as a fixed parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 4, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 12, "end": 16}]}]}
{"content": "When p = 0.5 it is equivalent to training 2|W| networks with shared parameters where |W| is the number of neurons subject to dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neurons", "start": 106, "end": 112}]}]}
{"content": "This results in a strong regularization effect which helps in preventing overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 25, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 73, "end": 83}]}]}
{"content": "We propose three kind of architectures obtained by the composition of six kinds of neural layers: a convolutional layer a max pooling layer a dropout layer a long short-term memory LSTM layer a fully connected layer and a softmax layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 83, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 100, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max", "start": 122, "end": 124}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 142, "end": 148}]}, {"label": ["B-DeepLearning"], "points": [{"text": "long", "start": 158, "end": 161}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 181, "end": 184}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 194, "end": 198}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 222, "end": 228}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers:", "start": 90, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 114, "end": 118}, {"start": 134, "end": 138, "text": "layer"}, {"start": 150, "end": 154, "text": "layer"}, {"start": 210, "end": 214, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 126, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "short-term", "start": 163, "end": 172}]}, {"label": ["I-DeepLearning"], "points": [{"text": "memory", "start": 174, "end": 179}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 200, "end": 208}]}]}
{"content": "The Max Pooling operation with width and stride 2 helps to capture the most salient features extracted by the previous layer and reduces the output size from 145 to 72 vectors ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Max", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stride", "start": 41, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Pooling", "start": 8, "end": 14}]}]}
{"content": "The Dropout operation with probability p = 0.5 is used to prevent overfitting during the training phase ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 66, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 89, "end": 96}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 98, "end": 102}]}]}
{"content": "We notice that the best architecture is the CONV-LSTM-FCX2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CONV-LSTM-FCX2", "start": 44, "end": 57}]}]}
{"content": "We have proposed a novel model parameter training scheme based on the concepts of quantum computing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "model", "start": 25, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 31, "end": 39}]}]}
{"content": "We apply deep learning methods in this paper namely we use convolutional neural networks CNNs for description and prediction of the red blood cells\u00e2\u20ac\u2122 trajectory which is crucial in modeling of a blood flow ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 59, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 89, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 14, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "methods", "start": 23, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 73, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 80, "end": 87}]}]}
{"content": "Training and testing sets used for neural network are extracted from simulations which differ only in initial seeding of the cells ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Training", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 35, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "seeding", "start": 110, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 9, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 13, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 21, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 42, "end": 48}]}]}
{"content": "Besides using convolution or fully connected layers we also used a relatively new type of layers the dense convolution layers introduced in ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 14, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 101, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 35, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 45, "end": 50}, {"start": 119, "end": 124, "text": "layers"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 107, "end": 117}]}]}
{"content": "Used neural network architecture hyperparameters are these: Weights are initialized in the range xavier bias is set to 0. Learning rate is 0.0002 \u00ce\u00bb1 = \u00ce\u00bb2 = 0.000001 dropout = 0.02 and minibatch size is 32 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 33, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Weights", "start": 60, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 104, "end": 107}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 122, "end": 129}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 167, "end": 173}]}, {"label": ["B-DeepLearning"], "points": [{"text": "minibatch", "start": 186, "end": 194}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 12, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 131, "end": 134}]}]}
{"content": "The training phase lasts about 6 hours for more complicated network architectures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 13, "end": 17}]}]}
{"content": "The convolution region of the CNNs includes the convolution the activation and the pooling layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolution", "start": 4, "end": 14}, {"start": 48, "end": 58, "text": "convolution"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 30, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 64, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pooling", "start": 83, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "region", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 91, "end": 96}]}]}
{"content": "80% 400 Healthy and 400 Sick were allocated to training and testing that is the processes of extracting the attributes and optimizing the weights of the filters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 47, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 138, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 56, "end": 58}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 60, "end": 66}]}]}
{"content": "20% 100 thermographies in each class were reserved for blind validation and establishing the final predictive accuracy of these architectures using a database of images that was not used for learning training and testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 61, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 110, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 200, "end": 207}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 209, "end": 211}]}, {"label": ["I-DeepLearning"], "points": [{"text": "testing", "start": 213, "end": 219}]}]}
{"content": "Different techniques were used to improve the accuracy: Learning rate tuning which controls the update of the CNNs weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy:", "start": 46, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Learning", "start": 56, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 110, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 65, "end": 68}]}]}
{"content": "This parameter greatly impacts the result and performance of the CNN model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 5, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 65, "end": 67}]}]}
{"content": "The neural networks are updated via the stochastic gradient ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 40, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 11, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 51, "end": 58}]}]}
{"content": "Data Augmentation that consists in the realization of different random operations of rotation translation and zoom on the images to avoid overfitting and improve generalization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 138, "end": 148}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 162, "end": 175}]}]}
{"content": "It is applied using a differential learning rate that is introducing three different and successively higher values of the learning rate thus taking into account the differential knowledge of the layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 35, "end": 42}, {"start": 123, "end": 130, "text": "learning"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 44, "end": 47}, {"start": 132, "end": 135, "text": "rate"}]}]}
{"content": "Therefore although Resnet50 provided the highest accuracy is less stable than Resnet 34 1.09% vs 063% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Resnet50", "start": 19, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 49, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Resnet", "start": 78, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "34", "start": 85, "end": 86}]}]}
{"content": "Table 2 provides the corresponding confusion matrices in validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "confusion", "start": 35, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 57, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "matrices", "start": 45, "end": 52}]}]}
{"content": "With respect to the tune of models\u00e2\u20ac\u2122 hyper-parameters the R package mlrMBO was used to perform a Bayesian optimization within the train set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 38, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "train", "start": 131, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 137, "end": 139}]}]}
{"content": "This package implements a Bayesian optimization of black-box functions which allows to find faster an optimal hyper-parameters setting in contrast to traditional hyperparameters search strategies such as grid search highly time consuming when more than 3 hyper-parameters are tuned or random search not efficient enough since similar or non-sense hyper-parameters settings might be tested ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 110, "end": 125}, {"start": 255, "end": 270, "text": "hyper-parameters"}, {"start": 347, "end": 362, "text": "hyper-parameters"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 162, "end": 176}]}, {"label": ["B-DeepLearning"], "points": [{"text": "grid", "start": 204, "end": 207}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 285, "end": 290}]}, {"label": ["I-DeepLearning"], "points": [{"text": "search", "start": 209, "end": 214}, {"start": 292, "end": 297, "text": "search"}]}]}
{"content": "Table 1 shows the average AUC performance standard deviation and number of genes retained by the different models tested over the test sets of the cross-validation setting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 26, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 147, "end": 162}]}]}
{"content": "The proposed Convolutional Neural Network CNN is consisting of two parallel convolutional layers taking as inputs transversal coronal and axial slices acquired before and after chemotherapy for each patient ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 13, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 42, "end": 44}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 76, "end": 88}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 27, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 34, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 90, "end": 95}]}]}
{"content": "As illustrated in Fig. 2 this architecture contains two similar branches each one contains 4 blocks of 2D convolution followed by an activation function ReLU and a Max pooling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "2D", "start": 103, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 133, "end": 142}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 153, "end": 156}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Max", "start": 164, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convolution", "start": 106, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 144, "end": 151}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pooling", "start": 168, "end": 174}]}]}
{"content": "In the first and second blocks 32 kernels were used for each convolutional layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "kernels", "start": 34, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 61, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 75, "end": 79}]}]}
{"content": "The parallel Deep learning architecture was applied for each view using corresponding slices before and after the first chemotherapy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 18, "end": 25}]}]}
{"content": "Consequently we used the Stochastic Gradient Descent SGD with a learning rate of 00052 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Stochastic", "start": 25, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SGD", "start": 53, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 64, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Gradient", "start": 36, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Descent", "start": 45, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 73, "end": 76}]}]}
{"content": "A learning rate decay of 3.46e\u00e2\u02c6\u20195 was used to schedule a best accuracy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 2, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 63, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 11, "end": 14}]}]}
{"content": "To compile the model a categorical cross entropy was used as loss function and standard accuracy was used as a metric ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 35, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 61, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 41, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 66, "end": 73}]}]}
{"content": "To avoid results\u00e2\u20ac\u2122 bias a 5-Fold stratified cross validation with AUC as metric was used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 45, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "AUC", "start": 67, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 51, "end": 60}]}]}
{"content": "Within 150 epochs with 5-fold stratified cross validation an accuracy of 90.03 was obtained using 20% of 3D validation data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 11, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 41, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 61, "end": 68}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 108, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 47, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 119, "end": 122}]}]}
{"content": "An overfitting was observed during training when using only one of the views without data augmentation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 3, "end": 13}]}]}
{"content": "Besides the work we did on building other types of agents we have also tried to explore in more depth different cognitive and affective models of agents including symbolic BDI models as well as neural network models ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 194, "end": 199}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 201, "end": 207}]}, {"label": ["I-DeepLearning"], "points": [{"text": "models", "start": 209, "end": 214}]}]}
{"content": "While we are defining FSTs and after having introduced the use of HMMs as stochastic FSTs it is worth noticing that regular transduction rules can also be implemented in the form of so-called multilayer perceptrons MLP  a particular type of artificial neural networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 192, "end": 201}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 215, "end": 217}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 241, "end": 250}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 203, "end": 213}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 252, "end": 257}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 259, "end": 266}]}]}
{"content": "Artificial neural networks are based on simplistic models for biological neuron behavior and the interconnections between these neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 0, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 18, "end": 25}]}]}
{"content": "Most often the nonlinear function is a limiter or a sigmoid function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 52, "end": 58}]}]}
{"content": "The MLP is the far most widely used network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 4, "end": 6}]}]}
{"content": "It is composed of an input layer and an output layer separated by one or more hidden layers of nodes with each layer connected to the next layer feeding its node values forward Fig. 24 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 21, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 40, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 78, "end": 83}]}, {"label": ["B-DeepLearning"], "points": [{"text": "nodes", "start": 95, "end": 99}]}, {"label": ["B-DeepLearning"], "points": [{"text": "node", "start": 157, "end": 160}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 27, "end": 31}, {"start": 47, "end": 51, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 85, "end": 90}]}]}
{"content": "In many domains neural networks are an effective alternative to statistical methods ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 23, "end": 30}]}]}
{"content": "James Henderson has identified a suitable neural network architecture for natural language parsing called Simple Synchrony Networks SSNs which he discusses in chapter 6 A Neural Network Parser that Handles Sparse Data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 42, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 106, "end": 111}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 132, "end": 135}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 171, "end": 176}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 113, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 123, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 178, "end": 184}]}]}
{"content": "Because neural networks learn their own internal representations\u00e2\u20ac\u0161 neural networks can decide automatically what features to count and how reliable they are for estimating the desired probabilities ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 8, "end": 13}, {"start": 68, "end": 73, "text": "neural"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 15, "end": 22}, {"start": 75, "end": 82, "text": "networks"}]}]}
{"content": "This generalization ability is a result of using a neural network method for representing sets of objects\u00e2\u20ac\u0161 called Temporal Synchrony Variable Binding TSVB Shastri and Ajjanagadde\u00e2\u20ac\u0161 1993 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 58, "end": 64}]}]}
{"content": "SRNs can learn generalizations over positions in an input sequence and thus can handle unbounded input sequences\u00e2\u20ac\u0161 which has made them of interest in natural language processing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 15, "end": 29}]}]}
{"content": "By using TSVB to represent the constituents in a syntactic structure\u00e2\u20ac\u0161 SSNs also learn generalizations over structural constituents ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 72, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 88, "end": 102}]}]}
{"content": "The linguistic relevance of this class of generalizations is what accounts for the fact that SSNs generalize from training set to testing set in an appropriate way\u00e2\u20ac\u0161 as demonstrated in Section 4 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalizations", "start": 42, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs", "start": 93, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 114, "end": 121}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 130, "end": 136}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 123, "end": 125}, {"start": 138, "end": 140, "text": "set"}]}]}
{"content": "In this section we briefly outline the SSN architecture and how it can be used to estimate the parameters of a probability model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 39, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 95, "end": 104}]}]}
{"content": "Standard pattern-recognition neural networks such as Multi-Layered Perceptrons MLPs take a vector of real values as input\u00e2\u20ac\u0161 compute hidden internal representation which is also a vector of real values\u00e2\u20ac\u0161 and output a third vector of real values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 29, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multi-Layered", "start": 53, "end": 65}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLPs", "start": 79, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 36, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptrons", "start": 67, "end": 77}]}]}
{"content": "Simple Recurrent Networks extend MLPs to sequences by using the hidden representations as representations of the network\u00e2\u20ac\u2122s state at a given point in the sequence ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLPs", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 7, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 17, "end": 24}]}]}
{"content": "Simple Synchrony Networks extend SRNs by computing one of these sequences for each object in a set of objects ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 33, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 7, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 17, "end": 24}]}]}
{"content": "The most important feature of any learning architecture is how it generalizes from training data to testing data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalizes", "start": 66, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 83, "end": 90}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 100, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 92, "end": 95}, {"start": 108, "end": 111, "text": "data"}]}]}
{"content": "SRNs are popular for sequence processing because they inherently generalize over sequence positions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 0, "end": 3}]}]}
{"content": "Because inputs are fed to an SRN one at a time\u00e2\u20ac\u0161 and the same trained parameters called link weights apply at every time\u00e2\u20ac\u0161 information learned at one sequence position will inherently be generalized to other sequence positions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SRN", "start": 29, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 71, "end": 80}]}]}
{"content": "This generalization ability manifests itself in the fact that SRNs can handle arbitrarily long sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SRNs", "start": 62, "end": 65}]}]}
{"content": "This generalization ability manifests itself in the fact that these networks can handle arbitrarily many constituents\u00e2\u20ac\u0161 and therefore unbounded phrase structure trees ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}]}
{"content": "After the parent of a terminal is chosen it is used by the SSN to estimate the parameters for latter portions of the sentence\u00e2\u20ac\u0161 including latter parent estimates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 59, "end": 61}]}]}
{"content": "We also bias the network training by providing a new-nonterminal input unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 8, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 25, "end": 32}]}]}
{"content": "To test the ability of Simple Synchrony Networks to handle sparse data we train the SSN parser described in the previous section on a relatively small set of sentences and then test how well it generalizes to a set of previously unseen sentences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSN", "start": 84, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Synchrony", "start": 30, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 40, "end": 47}]}]}
{"content": "This process can be continued until no more changes are made but to avoid over-fitting it is better to check the performance of the network on a validation set and stop training when the performance on the validation set reaches a maximum ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 74, "end": 85}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 145, "end": 154}, {"start": 206, "end": 215, "text": "validation"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 156, "end": 158}, {"start": 217, "end": 219, "text": "set"}]}]}
{"content": "This is why we have split the corpus into three datasets one for training one for validation and one for testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 65, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 82, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 105, "end": 111}]}]}
{"content": "This technique also allows multiple versions of the network to be trained and then evaluated using the validation set without ever using the testing set until a single network has been chosen ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 103, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 141, "end": 147}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 114, "end": 116}, {"start": 149, "end": 151, "text": "set"}]}]}
{"content": "A variety of hidden layer sizes and random initial weight seeds were used in the different networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 51, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 20, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "seeds", "start": 58, "end": 62}]}]}
{"content": "Larger hidden layers result in the network being able to fit the training data more precisely but can lead to over-fitting and therefore bad performance on the validation set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 7, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fit", "start": 57, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "over-fitting", "start": 110, "end": 121}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 160, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 61, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 65, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 74, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 171, "end": 173}]}]}
{"content": "From the multiple networks trained the best network was chosen on the basis of its performance on the validation set and this one network was used in testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 102, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 113, "end": 115}]}]}
{"content": "The best network had 100 hidden units and trained for a total of 145 passes through the training set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 88, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 97, "end": 99}]}]}
{"content": "Because this process does not require a validation set we estimate these parameters using the combination of the training set and the validation set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 40, "end": 49}, {"start": 134, "end": 143, "text": "validation"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 73, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 113, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 51, "end": 53}, {"start": 122, "end": 124, "text": "set"}, {"start": 145, "end": 147, "text": "set"}]}]}
{"content": "This generalization performance is due to SSNs\u00e2\u20ac\u2122 ability to generalize across constituents as well as across sequence positions plus the ability of neural networks in general to learn what input features are important as well as what they imply about the output ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 5, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "SSNs\u00e2\u20ac\u2122", "start": 42, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 149, "end": 154}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 156, "end": 163}]}]}
{"content": "In particular we used a momentum of 0.9 and weight decay regularization of between 0.1 and 0.0. Both the learning rate and the weight decay were decreased as the learning proceeded based on training error and validation error respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 24, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 44, "end": 49}, {"start": 127, "end": 132, "text": "weight"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 57, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 105, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 190, "end": 197}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 209, "end": 218}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 51, "end": 55}, {"start": 134, "end": 138, "text": "decay"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 114, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "error", "start": 199, "end": 203}, {"start": 220, "end": 224, "text": "error"}]}]}
{"content": "We mean feature here in the sense of features which can be computed from discourse as input to machine learning algorithms for classification tasks such as topic segmentation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 95, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 103, "end": 110}]}]}
{"content": "Moreover generic tools are provided for iterating over discourses processing them and extracting sets of feature values at regular intervals which can then be piped directly into learners like decision trees neural nets or support vector machines ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 208, "end": 213}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 215, "end": 218}]}]}
{"content": "We have found the visualiser to be invaluable in debugging algorithms for feature extractors tweaking parameter values and hypothesizing new interesting features ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 102, "end": 110}]}]}
{"content": "A variety of learning parameters were explored and the best-performing parameter set was selected: initial Q values set to 0 exploration parameter \u00ce\u00b5 = 0.2 and the learning rate \u00ce\u00b1 set to 1/k where k is the number of visits to the Qs a being updated ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 13, "end": 20}, {"start": 164, "end": 171, "text": "learning"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 71, "end": 79}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 22, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 173, "end": 176}]}]}
{"content": "Schmid 1994a presents a neural network tagger based on multilayer perceptron networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 55, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 31, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 66, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 77, "end": 84}]}]}
{"content": "An artificial neural network consist of simple units each associated with an activation value and directed links for passing the values between the units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 3, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 21, "end": 27}]}]}
{"content": "Activation values are propagated from input to output layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Activation", "start": 0, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "values", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 54, "end": 59}]}]}
{"content": "At each unit the input activation values are summed and a bias parameter is added ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 17, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 58, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 23, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "values", "start": 34, "end": 39}]}]}
{"content": "The network learns by adapting the weights of the connections between units until the correct output is produced ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 35, "end": 41}]}]}
{"content": "In the output layer all units have a value of zero except the correct unit tag which gets the value of one ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 7, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 14, "end": 18}]}]}
{"content": "In a system of this kind tagging a word means i copying the tag probabilities of the word and its neighbours into the input units and ii propagating the activations to the output units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 118, "end": 122}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 172, "end": 177}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 124, "end": 128}, {"start": 179, "end": 183, "text": "units"}]}]}
{"content": "We will first discuss the acquisition of tagging rules using supervised learning where a manually tagged corpus is available to be used as a 'gold standard' to guide learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "supervised", "start": 61, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 72, "end": 79}]}]}
{"content": "In back-propagation learning this training is done by repeatedly iterating over all examples comparing for each example the output predicted by the network random at first to the desired output and changing connection weights between network nodes in such a way that performance increases ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "back-propagation", "start": 3, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "connection", "start": 207, "end": 216}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 218, "end": 224}]}]}
{"content": "Multilayer Perceptrons Rumelhart et al. 1986 are the most popular neural network architecture ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 0, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptrons", "start": 11, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 73, "end": 79}]}]}
{"content": "The activation rule is a local rule which is used by each unit to compute its activation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 4, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rule", "start": 15, "end": 18}]}]}
{"content": "Given two words preceding context and 89 categories the network has an input layer of 178 units and an output layer of 89 units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 71, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 103, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 77, "end": 81}, {"start": 110, "end": 114, "text": "layer"}]}]}
{"content": "Adding a hidden layer to a two-layer network did not improve performance ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 9, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 16, "end": 20}]}]}
{"content": "Connectionist approaches also require the computation of fewer parameters weights than statistical models N-gram probabilities which becomes especially useful when considering a wider context than trigrams ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 63, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 74, "end": 80}]}]}
{"content": "On the theoretical side there is a need for more insight into the differences and similarities in how generalization is achieved in this area by different statistical and machine learning techniques ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 102, "end": 115}]}, {"label": ["B-DeepLearning"], "points": [{"text": "machine", "start": 171, "end": 177}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 179, "end": 186}]}]}
{"content": "Most emphasis in current deep learning artificial neural network based automatic recognition of speech is put on deep net architectures with multiple sequential levels of processing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 25, "end": 28}, {"start": 113, "end": 116, "text": "deep"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 39, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 30, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 57, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 118, "end": 120}]}]}
{"content": "Current state-of-the-art stochastic ASR systems often estimate the likelihood pX|W by a discriminatively-trained multi-layer perceptron artificial neural network MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multi-layer", "start": 113, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 136, "end": 145}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 162, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 125, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 147, "end": 152}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 154, "end": 160}]}]}
{"content": "The rule-based algorithm obtained 60.67% splitting accuracy at word level and 94.31% accuracy within the word at split level ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 51, "end": 58}, {"start": 85, "end": 92, "text": "accuracy"}]}]}
{"content": "The hyperparameters found to optimize it are n = 4 \u00ce\u00b1 = 10\u00e2\u02c6\u20195 marker=true ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 4, "end": 18}]}]}
{"content": "Recently there has been a renewed interest in applying neural networks ANNs to speech recognition thanks to the invention of deep neural nets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 71, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 125, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 62, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 130, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 137, "end": 140}]}]}
{"content": "It treats the network as a deep belief network DBN built out of restricted Bolztmann machines RBMs and optimizes an energy-based target function using the contrastive divergence CD algorithm ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 27, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 47, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 64, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 94, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "belief", "start": 32, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Bolztmann", "start": 75, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 85, "end": 92}]}]}
{"content": "As for the third method it is different from the two above in the sense that in this case it is not the training algorithm that is slightly modified but the neurons themselves ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 104, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "algorithm", "start": 113, "end": 121}]}]}
{"content": "Namely the usual sigmoid activation function is replaced with the rectifier function max0 x ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 17, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 66, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 25, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 36, "end": 43}, {"start": 76, "end": 83, "text": "function"}]}]}
{"content": "These kinds of neural units have been proposed by Glorot et al. and were successfully applied to image recognition and NLP tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 15, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 22, "end": 26}]}]}
{"content": "Rectified linear units were also found to improve restricted Boltzmann machines ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Rectified", "start": 0, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 50, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 10, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 17, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Boltzmann", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 71, "end": 78}]}]}
{"content": "It has been shown recently that a deep rectifier network can attain the same phone recognition performance as that for the pre-trained nets of Mohamed et al. 4 but without the need for any pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 34, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-trained", "start": 123, "end": 133}]}, {"label": ["B-DeepLearning"], "points": [{"text": "4", "start": 158, "end": 158}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 189, "end": 200}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 39, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 135, "end": 138}]}]}
{"content": "This efficient unsupervised algorithm first described in can be used for learning the connection weights of a deep belief network DBN consisting of several layers of restricted Boltzmann machines RBMs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "connection", "start": 86, "end": 95}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 110, "end": 113}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 130, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "restricted", "start": 166, "end": 175}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 196, "end": 199}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 97, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "belief", "start": 115, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 122, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Boltzmann", "start": 177, "end": 185}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 187, "end": 194}]}]}
{"content": "As their name implies RBMs are a variant of Boltzmann machines with the restriction that their neurons must form a bipartite graph ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 22, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Boltzmann", "start": 44, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "machines", "start": 54, "end": 61}]}]}
{"content": "They have an input layer representing the features of the given task a hidden layer which has to learn some representation of the input and each connection in an RBM must be between a visible unit and a hidden unit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 71, "end": 76}, {"start": 203, "end": 208, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RBM", "start": 162, "end": 164}]}, {"label": ["B-DeepLearning"], "points": [{"text": "visible", "start": 184, "end": 190}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 19, "end": 23}, {"start": 78, "end": 82, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 192, "end": 195}, {"start": 210, "end": 213, "text": "unit"}]}]}
{"content": "RBMs can be trained using the one-step contrastive divergence CD algorithm described in 6 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "one-step", "start": 30, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CD", "start": 62, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "contrastive", "start": 39, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "divergence", "start": 51, "end": 60}]}]}
{"content": "It is a simple algorithm where first we train a network with one hidden layer to full convergence using backpropagation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 65, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "full", "start": 81, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 104, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 72, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "convergence", "start": 86, "end": 96}]}]}
{"content": "Then we replace the softmax layer by another randomly initialized hidden layer and a new softmax layer on top and we train the network again ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 20, "end": 26}, {"start": 89, "end": 95, "text": "softmax"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 66, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 73, "end": 77}]}]}
{"content": "This process is repeated until we reach the desired number of hidden layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 69, "end": 74}]}]}
{"content": "Seide et al. found that this method gives the best results if one performs only a few iterations of backpropagation in the pre-training phase instead of training to full convergence with an unusually large learn rate ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 100, "end": 114}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 123, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 170, "end": 180}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 206, "end": 210}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 212, "end": 215}]}]}
{"content": "In their paper they concluded that this simple training strategy performs just as well as the much more complicated DBN pre-training method described above ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DBN", "start": 116, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 120, "end": 131}]}]}
{"content": "In the case of the third method it is not the training algorithm but the neurons that are slightly modified ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 46, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "algorithm", "start": 55, "end": 63}]}]}
{"content": "Instead of the usual sigmoid activation here we apply the rectifier function max0 x for all hidden neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 21, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 58, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 92, "end": 97}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 29, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 68, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "max0", "start": 77, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 99, "end": 105}]}]}
{"content": "There are two fundamental differences between the sigmoid and the rectifier functions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 50, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 66, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 76, "end": 84}]}]}
{"content": "One is that the output of rectifier neurons does not saturate as their activity gets higher ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 26, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 36, "end": 42}]}]}
{"content": "Glorot et al. conjecture that this is very important in explaining their good performance in deep nets: because of this linearity there is no gradient vanishing effect ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 93, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 142, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets:", "start": 98, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "vanishing", "start": 151, "end": 159}]}]}
{"content": "One might suppose that this could harm optimization by blocking gradient backpropagation but the experimental results do not support this hypothesis ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 64, "end": 71}]}, {"label": ["I-DeepLearning"], "points": [{"text": "backpropagation", "start": 73, "end": 87}]}]}
{"content": "It seems that the hard nonlinearities do no harm as long as the gradient can propagate along some paths ", "annotation": []}
{"content": "The main advantage of deep rectifier nets is that they can be trained with the standard backpropagation algorithm without any pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 22, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 88, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 126, "end": 137}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 27, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 37, "end": 40}]}]}
{"content": "A random 10% of the training set was held out for validation purposes and this block of data will be referred to as the \u00e2\u20ac\u2122development set\u00e2\u20ac\u2122 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 20, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 50, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "\u00e2\u20ac\u2122development", "start": 120, "end": 133}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 29, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set\u00e2\u20ac\u2122", "start": 135, "end": 140}]}]}
{"content": "It contains about 28 hours of recordings from which 22 hours were selected for the training set 2 hours for the development set and 4 hours for the test set ", "annotation": []}
{"content": "In the case of the DBN-based pre-training method see Section 2.1 we applied stochastic gradient descent i.e. backpropagation training with a mini-batch size of 128 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 29, "end": 40}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 76, "end": 85}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 109, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mini-batch", "start": 141, "end": 150}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 87, "end": 94}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 96, "end": 102}]}]}
{"content": "For Gaussian-binary RBMs we ran 50 epochs with a fixed learning rate of 0.002 while for binary-binary RBMs we used 30 epochs with a learning rate of 002 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 20, "end": 23}, {"start": 102, "end": 105, "text": "RBMs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 35, "end": 40}, {"start": 118, "end": 123, "text": "epochs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 55, "end": 62}, {"start": 132, "end": 139, "text": "learning"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 64, "end": 67}, {"start": 141, "end": 144, "text": "rate"}]}]}
{"content": "Then to fine-tune the pre-trained nets again backpropagation was applied with the same mini-batch size as that used for pre-training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-trained", "start": 22, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 45, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "mini-batch", "start": 87, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 120, "end": 131}]}]}
{"content": "The initial learn rate was set to 0.01 and it was halved after each epoch when the error on the development set increased ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 12, "end": 16}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 68, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 96, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 18, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 108, "end": 110}]}]}
{"content": "During both the pretraining and fine-tuning phases the learning was accelerated by using a momentum of 0.9 except for the first epoch of fine-tuning which did not use the momentum method ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pretraining", "start": 16, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 91, "end": 98}, {"start": 171, "end": 178, "text": "momentum"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 128, "end": 132}]}]}
{"content": "Turning to the discriminative pre-training method see Section 2.2 the initial learn rate was set to 0.01 and it was halved after each epoch when the error on the development set increased ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pre-training", "start": 30, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 78, "end": 82}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 134, "end": 138}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 162, "end": 172}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 84, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 174, "end": 176}]}]}
{"content": "The learn rate was restored to its initial value of 0.01 after the addition of each layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 4, "end": 8}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 10, "end": 13}]}]}
{"content": "Furthermore we found that using 5 epochs of backpropagation after the introduction of each layer gave the best results ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 34, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 44, "end": 58}]}]}
{"content": "For both the pre-training and fine-tuning phases we used a batch size of 128 and momentum of 0.8 except for the first epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 59, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 81, "end": 88}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 118, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 65, "end": 68}]}]}
{"content": "The initial learn rate for the fine-tuning of the full network was again set to 001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 12, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 18, "end": 21}]}]}
{"content": "The training of deep rectifier nets see Section 2.3 did not require any pre-training at all ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 16, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 21, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 31, "end": 34}]}]}
{"content": "The training of the network was performed using backpropagation with an initial learn rate of 0.001 and a batch size of 128 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 48, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learn", "start": 80, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 106, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 86, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 112, "end": 115}]}]}
{"content": "As can be seen the three training methods performed very similarly on the test set the only exception being the case of five hidden layers where the rectifier net performed slightly better ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 125, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 149, "end": 157}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 132, "end": 137}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 159, "end": 161}]}]}
{"content": "It also significantly outperformed the other two methods on the development set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 64, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 76, "end": 78}]}]}
{"content": "We mention that a single hidden layer net with the same amount of weights as the best deep net yielded 237% ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 66, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 86, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 91, "end": 93}]}]}
{"content": "Similar to the TIMIT tests 2048 neurons were used for each hidden layer with a varying number of hidden layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 59, "end": 64}, {"start": 97, "end": 102, "text": "hidden"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 66, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 104, "end": 109}]}]}
{"content": "The error rates seem to saturate at 4-5 hidden layers and the curves for the three methods run parallel and have only slightly different values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 47, "end": 52}]}]}
{"content": "The lowest error rate is attained with the five-layer rectifier network both on the development and the test sets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 54, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 104, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 64, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 109, "end": 112}]}]}
{"content": "The iteration count we applied here 50 for Gaussian RBMs and 30 for binary RBMs is an average value and follows the work of Seide et al ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RBMs", "start": 52, "end": 55}, {"start": 75, "end": 78, "text": "RBMs"}]}]}
{"content": "Discriminative pre-training is also much faster than the DBN-based method but is still slower than rectifier nets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "rectifier", "start": 99, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 109, "end": 112}]}]}
{"content": "Tuning the parameters so that the two systems had a similar real-time factor was also out of the question as the hybrid model was implemented on a GPU while the HMM used a normal CPU ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Tuning", "start": 0, "end": 5}]}, {"label": ["I-DeepLearning"], "points": [{"text": "the", "start": 7, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 11, "end": 20}]}]}
{"content": "Here we compared two training methods and a new type of activation function for deep neural nets and evaluated them on a large vocabulary recognition task ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 56, "end": 65}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 80, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 67, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 85, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 92, "end": 95}]}]}
{"content": "The three algorithms yielded quite similar recognition performances but based on the training times deep rectifier networks seem to be the preferred choice ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 100, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 105, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 115, "end": 122}]}]}
{"content": "While the resulting speech sound likelihood estimates are demonstrated to be better that the earlier used likelihoods derived by generative Gaussian Mixture Models unexpected signal distortions that were not seen in the training data can still make the acoustic likelihoods unacceptably low ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 220, "end": 227}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 229, "end": 232}]}]}
{"content": "Here we compare three methods; namely the unsupervised pre-training algorithm of Hinton et al. a supervised pre-training method that constructs the network layer-by-layer and deep rectifier networks which differ from standard nets in their activation function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "supervised", "start": 97, "end": 106}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 175, "end": 178}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 240, "end": 249}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pre-training", "start": 108, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 180, "end": 188}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 190, "end": 197}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 251, "end": 258}]}]}
{"content": "Overall for the large vocabulary speech recognition task we study here deep rectifier networks offer the best tradeoff between accuracy and training time ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 71, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 140, "end": 147}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectifier", "start": 76, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 86, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "time", "start": 149, "end": 152}]}]}
{"content": "In this new proposed approach a multi-layer neural network multi-layer perceptron is used to learn the decision function that will then be used to select the words ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multi-layer", "start": 32, "end": 42}, {"start": 59, "end": 69, "text": "multi-layer"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decision", "start": 103, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 51, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 71, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 112, "end": 119}]}]}
{"content": "For training the neural network parameters we have to associate a target value to each word ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 17, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 24, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameters", "start": 32, "end": 41}]}]}
{"content": "The scores produced by the neural network module will then be used to sort the list of candidate words and the top of the list will be selected to define the recognition vocabulary ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 27, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 34, "end": 40}]}]}
{"content": "Feature vectors and associated target values are used for training the neural network: the input-feature vector is the input layer 36 input neurons; the target value is the output of the unique output layer neuron ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 71, "end": 76}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 119, "end": 123}, {"start": 134, "end": 138, "text": "input"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 194, "end": 199}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network:", "start": 78, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 125, "end": 129}, {"start": 201, "end": 205, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons;", "start": 140, "end": 147}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neuron", "start": 207, "end": 212}]}]}
{"content": "There is one hidden layer containing 18 neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 13, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 20, "end": 24}]}]}
{"content": "In recent years feed forward neural networks FFNN  attracted attention due their ability to overcome biggest disadvantage of n-gram models: even when the ngram is not observed in training FFNN estimates probabilities of the word based on the full history ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feed", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "FFNN", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "forward", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 29, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 36, "end": 43}]}]}
{"content": "The RNN is going further in model generalization: instead of considering only the several previous words parameter n the recursive weights are assumed to represent short term memory ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization:", "start": 34, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recursive", "start": 121, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 131, "end": 137}]}]}
{"content": "Long Short-Term Memory LSTM neural network is different type of RNN structure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 23, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 28, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 64, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-Term", "start": 5, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 35, "end": 41}]}]}
{"content": "LSTM approved themselves in various applications and it seems to be very promising course also for the field of language modelling ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 0, "end": 3}]}]}
{"content": "Typical NN unit consists of the input activation which is transformed to output activation with activation function usually sigmoidal ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN", "start": 8, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 32, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 96, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoidal", "start": 124, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 38, "end": 47}, {"start": 80, "end": 89, "text": "activation"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 107, "end": 114}]}]}
{"content": "Firstly the activation function is applied to all gates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 12, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 23, "end": 30}]}]}
{"content": "There is a softmax function used in output layer to produce normalized probabilities ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 11, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 36, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 19, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 43, "end": 47}]}]}
{"content": "Normalization of input vector which is generally advised for neural networks is not needed due the 1-of-N input coding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 61, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 68, "end": 75}]}]}
{"content": "All models were trained with 20 cells in hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 41, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 48, "end": 52}]}]}
{"content": "The recurrent neural network language model RNNLM originally proposed by 2 and 3 incorporates the time dimension by expanding the input layer which represents the current input word with the previous hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 4, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 130, "end": 134}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 200, "end": 205}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 136, "end": 140}, {"start": 207, "end": 211, "text": "layer"}]}]}
{"content": "Theoretically recurrent neural networks can store relevant information from previous time steps for an arbitrarily long period of time making it possible to learn long-term dependencies ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "recurrent", "start": 14, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 24, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 31, "end": 38}]}]}
{"content": "To explain how our approach works let us examine the operation of a simple perceptron model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "perceptron", "start": 75, "end": 84}]}]}
{"content": "Hence if the weights of the feature extraction layer were initialized with 2D DCT or Gabor filter coefficients and only the weights of the hidden and output layers were tuned during training then the model would be equivalent to a more traditional system and incorporating the feature extraction step into the system would be just an implementational detail ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 13, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 28, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 150, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 36, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 47, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 157, "end": 162}]}]}
{"content": "Usually as the backpropagation algorithm guarantees only a locally optimal solution initializing the model with weights that already provide a good solution may help the backpropagation algorithm find a better local optimum than the one found using random initial values ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 15, "end": 29}, {"start": 170, "end": 184, "text": "backpropagation"}]}]}
{"content": "We should add that the same weights are applied on each input block so the number of weights will not change in this layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 28, "end": 34}, {"start": 85, "end": 91, "text": "weights"}]}]}
{"content": "It consisted of a hidden feature extraction layer with a linear activation function a hidden layer with 1000 neurons with the sigmoid activation function and an output layer containing softmax units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 25, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "linear", "start": 57, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 86, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 126, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 161, "end": 166}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 185, "end": 191}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}, {"start": 93, "end": 97, "text": "layer"}, {"start": 168, "end": 172, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 64, "end": 73}, {"start": 134, "end": 143, "text": "activation"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 75, "end": 82}, {"start": 145, "end": 152, "text": "function"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 193, "end": 197}]}]}
{"content": "The number of output neurons was set to the number of classes 39 while the number of neurons in the input and feature extraction layers varied depending on how many neighbouring patches were actually used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feature", "start": 110, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 21, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "extraction", "start": 118, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 129, "end": 134}]}]}
{"content": "The neural net was trained with random initial weights in the hidden and output layers using standard backpropagation on 90% of the training data in semi-batch mode while crossvalidation on the remaining randomly selected 10% of the training set was used as the stopping criterion ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 32, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 102, "end": 116}]}, {"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 132, "end": 139}, {"start": 233, "end": 240, "text": "training"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "semi-batch", "start": 149, "end": 158}]}, {"label": ["B-DeepLearning"], "points": [{"text": "crossvalidation", "start": 171, "end": 185}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 11, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "initial", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 141, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 242, "end": 244}]}]}
{"content": "The \u00e2\u20ac\u02dcextreme learning machine\u00e2\u20ac\u2122 of Huang et al. also exploits this suprising fact: this learning model is practically a twolayer network where both layers are initialized randomly and the lowest layer is not trained at all ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "twolayer", "start": 123, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 132, "end": 138}]}]}
{"content": "In order to verify how the whisper can be recognized by the ANN the two speakerdependent ASRs were developed with MATLAB Neural Network Toolbox ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANN", "start": 60, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Neural", "start": 121, "end": 126}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 128, "end": 134}]}]}
{"content": "The structures of these ANNs were: 396 input nodes 140 hidden neurons and 50 output neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ANNs", "start": 24, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 39, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 77, "end": 82}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 45, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 62, "end": 68}, {"start": 84, "end": 90, "text": "neurons"}]}]}
{"content": "For each speaker all words are divided into three parts: 60% of them are used for training 20% for validation and 20% for testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 99, "end": 108}]}]}
{"content": "The training development and test sets for the classification task were not selected from the corpus in a completely random manner but instead the division respected the websites i.e. one set of websites was denoted the training set another \u00e2\u20ac\u201c development set and the third \u00e2\u20ac\u201c test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 13, "end": 23}, {"start": 245, "end": 255, "text": "development"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 279, "end": 282}]}, {"label": ["I-DeepLearning"], "points": [{"text": "and", "start": 25, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "test", "start": 29, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 34, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 257, "end": 259}, {"start": 284, "end": 286, "text": "set"}]}]}
{"content": "The ASR system uses a hybrid Hidden Markov Model HMM and Deep Neural Network DNN architecture and a general 550k lexicon ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 57, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 69, "end": 75}]}]}
{"content": "The DNN utilizes five hidden layers with 1024 neurons per layer and a learning rate of 008 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 4, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 22, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neurons", "start": 46, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 70, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 29, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "per", "start": 54, "end": 56}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 58, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 79, "end": 82}]}]}
{"content": "The ReLU function is used as the activation function of neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 4, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 44, "end": 51}]}]}
{"content": "This DNN is trained for 35 epochs using 300 h of speech recordings ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 5, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 27, "end": 32}]}]}
{"content": "Recently neural-network based approaches in which words are embedded into a low-dimensional space appeared and became to be used in lexical semantic tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural-network", "start": 9, "end": 22}]}]}
{"content": "Following recent advances in artificial neural network research the recognizer employs parametric rectified linear units PReLU word embeddings and character-level embeddings based on gated linear units GRU ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 29, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 87, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "PReLU", "start": 121, "end": 125}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gated", "start": 183, "end": 187}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 202, "end": 204}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 98, "end": 106}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 108, "end": 113}, {"start": 189, "end": 194, "text": "linear"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 115, "end": 119}, {"start": 196, "end": 200, "text": "units"}]}]}
{"content": "LSTMs are specially shaped units of artificial neural networks designed to process whole sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LSTMs", "start": 0, "end": 4}]}, {"label": ["B-DeepLearning"], "points": [{"text": "artificial", "start": 36, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 54, "end": 61}]}]}
{"content": "Recently a gated linear unit GRU was proposed by Sam as an alternative to LSTM and was shown to have similar performance while being less computationally demanding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gated", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 29, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 74, "end": 77}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 17, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 24, "end": 27}]}]}
{"content": "Instead regularized averaged perceptron we use parametric rectified linear units character-level embeddings and dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularized", "start": 8, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 47, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 112, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "averaged", "start": 20, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 29, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 58, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 68, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 75, "end": 79}]}]}
{"content": "The input layer is connected to a hidden layer of parametric rectified linear units and the hidden layer is connected to the output layer which is a softmax layer producing probability distribution for all possible named entity classes in BILOU encoding ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 34, "end": 39}, {"start": 92, "end": 97, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parametric", "start": 50, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 125, "end": 130}]}, {"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 149, "end": 155}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 10, "end": 14}, {"start": 41, "end": 45, "text": "layer"}, {"start": 99, "end": 103, "text": "layer"}, {"start": 132, "end": 136, "text": "layer"}, {"start": 157, "end": 161, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rectified", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 71, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 78, "end": 82}]}]}
{"content": "The network is trained with AdaGrad and we use dropout on the hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "AdaGrad", "start": 28, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 47, "end": 53}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 69, "end": 73}]}]}
{"content": "We implemented our neural network in Torch7 a scientific computing framework with wide support for machine learning algorithms ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 26, "end": 32}]}]}
{"content": "We tuned most of the hyperparameters on development portion of CNEC 1.0 and used them for all other corpora ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 21, "end": 35}]}]}
{"content": "Notably we utilize window size W = 2 hidden layer of 200 nodes dropout 0.5 minibatches of size 100 and learning rate 0.02 with decay ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 37, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 63, "end": 69}]}, {"label": ["B-DeepLearning"], "points": [{"text": "minibatches", "start": 75, "end": 85}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 103, "end": 110}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 112, "end": 115}]}]}
{"content": "All reported experiments use an ensemble of 5 networks each using different random seed with the resulting distributions being an average of individual networks distributions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "random", "start": 76, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "seed", "start": 83, "end": 86}]}]}
{"content": "A work most similar to ours also proposed neural network architecture with word embeddings and character-level embeddings ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 42, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}]}
{"content": "The best published WER on CMUDict at present is demonstrated in using Long Short-term Memory Recurrent Neural Networks LSTM combined with a 5-gram graphone language model ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Long", "start": 70, "end": 73}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 119, "end": 122}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Short-term", "start": 75, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Memory", "start": 86, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Recurrent", "start": 93, "end": 101}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 103, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 110, "end": 117}]}]}
{"content": "We also use the exact same split of 90 % training data and 10 % test data as in 9 and thus our results are directly comparable to theirs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 41, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 64, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 50, "end": 53}, {"start": 69, "end": 72, "text": "data"}]}]}
{"content": "Hierarchical softmax and related procedures that involve decomposing the output layer into classes can help with this normalization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 13, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 73, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 80, "end": 84}]}]}
{"content": "We found that 15 classes optimized perplexity values for RNNLMs with 50 and 145 hidden nodes and 18 classes optimized perplexity values for RNNLMs with 500 nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNLMs", "start": 57, "end": 62}, {"start": 140, "end": 145, "text": "RNNLMs"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 87, "end": 91}]}]}
{"content": "The learning rate \u00ce\u00b7 adaptation scheme is managed by the adaptive gradient methods ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "adaptive", "start": 57, "end": 64}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 66, "end": 73}]}]}
{"content": "After optimizing on the development set \u00ce\u00b7 was fixed to 0.1 and the dimensionality of the latent space C was fixed at 45 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development", "start": 24, "end": 34}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 36, "end": 38}]}]}
{"content": "An RNNLM with 145 hidden nodes has about the same number of parameters as CDLM and performs 0.1 perplexity points worse than CDLM ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 3, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 18, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 25, "end": 29}]}]}
{"content": "Increasing the hidden units for RNNLM to 500 we obtain the best performing RNNLM ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 15, "end": 20}]}, {"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 32, "end": 36}, {"start": 75, "end": 79, "text": "RNNLM"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 22, "end": 26}]}]}
{"content": "To produce better performing LMs with fewer parameters we constructed an RNNLM with 50 hidden units which when linearly combined with CDLM CDLM+RNNLM  outperforms the best RNNLM using less than half as many parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNLM", "start": 73, "end": 77}, {"start": 172, "end": 176, "text": "RNNLM"}]}]}
{"content": "The architecture for this kind of feature extraction consists of two NNs trained towards phonetic targets ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NNs", "start": 69, "end": 71}]}]}
{"content": "The first-stage NN has four hidden layers with 1500 units each except the BN layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "NN", "start": 16, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 28, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 35, "end": 40}]}]}
{"content": "BN layer\u00e2\u20ac\u2122s size is 80 neurons and it is the third hidden layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 59, "end": 63}]}]}
{"content": "Linear regression is used on all single systems for arousal and all single systems for valence except for processing video geometric features where neural network with one hidden layer is used topology: 948\u00e2\u20ac\u201c474\u00e2\u20ac\u201c3 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 148, "end": 153}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 172, "end": 177}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 155, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 179, "end": 183}]}]}
{"content": "We trained a neural network with one hidden layer with topology 945\u00e2\u20ac\u201c474\u00e2\u20ac\u201c3 in this case ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 37, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 44, "end": 48}]}]}
{"content": "Our approach uses a bidirectional recurrent neural network BRNN since speech has the form of a sequential signal with complex dependencies between the different time steps in both directions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 20, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 59, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "recurrent", "start": 34, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 44, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 51, "end": 57}]}]}
{"content": "Additionally we propose an alternative layout for the BRNN and show that it performs better on that specific task ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 54, "end": 57}]}]}
{"content": "Usage of deep bidirectional GRU layers can be found in the work of Amodei et al. where for example up to seven bidirectional GRU layers are used and even combined with up to three convolutional layers which altogether improved the performance of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 111, "end": 123}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 180, "end": 192}]}, {"label": ["I-DeepLearning"], "points": [{"text": "bidirectional", "start": 14, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "GRU", "start": 28, "end": 30}, {"start": 125, "end": 127, "text": "GRU"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 194, "end": 199}]}]}
{"content": "An approach for speech classification solely using Convolutional Neural Networks CNN instead of recurrent ones can be seen in the work of Milde and Biemann ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 51, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 81, "end": 83}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 65, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 72, "end": 79}]}]}
{"content": "GRU Networks are a variation of the long short term memory LSTM networks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 0, "end": 2}]}, {"label": ["B-DeepLearning"], "points": [{"text": "long", "start": 36, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LSTM", "start": 59, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "short", "start": 41, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "term", "start": 47, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "memory", "start": 52, "end": 57}]}]}
{"content": "Conventional RNN structures propagate information only forwards in time ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNN", "start": 13, "end": 15}]}]}
{"content": "In this context bidirectional RNNs can be helpful by having separate layers processing the two different directions and feeding each others output into the same output layer as it is depicted in Fig ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bidirectional", "start": 16, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "RNNs", "start": 30, "end": 33}]}]}
{"content": "Here the output of the forward layer is not directly propagated towards the output layer instead it serves as the input of the backward layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "forward", "start": 23, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 76, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backward", "start": 127, "end": 134}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 31, "end": 35}, {"start": 83, "end": 87, "text": "layer"}, {"start": 136, "end": 140, "text": "layer"}]}]}
{"content": "Deep neural networks usually come with a large amount of parameters leaving them prone to overfitting ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 90, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 5, "end": 10}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 12, "end": 19}]}]}
{"content": "One straightforward way of avoiding that is using Dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 50, "end": 56}]}]}
{"content": "Those are applied by multiplying the output of each node that propagates towards a dropout layer by some random noise with each batch of training data ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 83, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "batch", "start": 128, "end": 132}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 91, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 134, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "training", "start": 137, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 146, "end": 149}]}]}
{"content": "The amount of deactivated nodes depends on the dropout rate p which can be seen as the distribution\u00e2\u20ac\u2122s parameter in the binary case ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 47, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 55, "end": 58}]}]}
{"content": "The positive effect of better generalization capabilities can be explained in two ways ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 30, "end": 43}]}]}
{"content": "First it essentially produces an ensemble of neural networks and therefore producing an equally averaged result over those ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ensemble", "start": 33, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 42, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 45, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 52, "end": 59}]}]}
{"content": "We use the training test and development-test sets which have originally been provided in the challenge ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "development-test", "start": 29, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sets", "start": 46, "end": 49}]}]}
{"content": "The RNNs are built in Python using the Keras framework ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "RNNs", "start": 4, "end": 7}]}]}
{"content": "We use two GRU layers with 128 hidden states each ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 11, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 31, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 38, "end": 43}]}]}
{"content": "For the merged BRNN those are both connected to the input and combine their results to a dropout layer with p = 04 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNN", "start": 15, "end": 18}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 89, "end": 95}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 97, "end": 101}]}]}
{"content": "For the sequential BRNN the first GRU layer produces another time-dependent sequence which is then fed into the backward GRU layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequential", "start": 8, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 34, "end": 36}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backward", "start": 112, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "BRNN", "start": 19, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 38, "end": 42}, {"start": 125, "end": 129, "text": "layer"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "GRU", "start": 121, "end": 123}]}]}
{"content": "After each of those follows one layer of dropout with p = 04 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 41, "end": 47}]}]}
{"content": "For both types of networks we finally use one to three fully connected dense layers with again 128 hidden states each eventually followed by another single-state layer which produces the output by applying the sigmoid function which is also used as the inner activation of the GRU nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 55, "end": 59}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 99, "end": 104}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 210, "end": 216}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRU", "start": 277, "end": 279}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 61, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dense", "start": 71, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 106, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 218, "end": 225}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 281, "end": 285}]}]}
{"content": "The remaining activation functions between each two nodes are defined as the rectifier or relu function ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 14, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "relu", "start": 90, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 25, "end": 33}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 95, "end": 102}]}]}
{"content": "As optimizer we use Adam. In earlier stages RMSprop has also been tried but it clearly proved to perform worse on that learning task and also oscillated a lot making it useless for the early stopping described in Sect. 44 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Adam.", "start": 20, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "early", "start": 185, "end": 189}]}, {"label": ["I-DeepLearning"], "points": [{"text": "stopping", "start": 191, "end": 198}]}]}
{"content": "The loss which is minimized in the training stage is the binary cross-entropy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "binary", "start": 57, "end": 62}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-entropy", "start": 64, "end": 76}]}]}
{"content": "The model is then trained for a maximum of 50 epochs and evaluated on the remaining 900 samples of the development-test set after each iteration ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "development-test", "start": 103, "end": 118}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 120, "end": 122}]}]}
{"content": "Training is stopped when the UAR of the validation set does not increase for 10 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 40, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 51, "end": 53}]}]}
{"content": "A model checkpoint is used to keep track of the weights which have been used to reach the highest UAR on the validation set up to the last epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 48, "end": 54}]}, {"label": ["B-DeepLearning"], "points": [{"text": "UAR", "start": 98, "end": 100}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 109, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 139, "end": 143}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 120, "end": 122}]}]}
{"content": "Finally we obtain a measure of general performance by using the highest scoring model to predict the labels on the testing set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 115, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 123, "end": 125}]}]}
{"content": "Also the Gaussian dropout leads to slightly higher performance than the binary one ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Gaussian", "start": 9, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 18, "end": 24}]}]}
{"content": "In most cases the network with two fully connected layers after the recurrent ones achieves the highest measures ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 35, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 41, "end": 49}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 51, "end": 56}]}]}
{"content": "Eventually we used the sequential BRNN with two dense layers and Gaussian dropout after being trained on the sets specified in Sect. 4.4 to predict the labels of the testing set and reached a UAR of 71.03 and an accuracy of 71.30 percent ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sequential", "start": 23, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dense", "start": 48, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Gaussian", "start": 65, "end": 72}]}, {"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 166, "end": 172}]}, {"label": ["B-DeepLearning"], "points": [{"text": "UAR", "start": 192, "end": 194}]}, {"label": ["I-DeepLearning"], "points": [{"text": "BRNN", "start": 34, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layers", "start": 54, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 74, "end": 80}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 174, "end": 176}]}]}
{"content": "Also it became clear that there exists a variant of BRNNs which has to our knowledge not been researched thoroughly yet ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "BRNNs", "start": 52, "end": 56}]}]}
{"content": "The topic of the paper is the training of deep neural networks which use tunable piecewise-linear activation functions called maxout for speech recognition tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 42, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 81, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 126, "end": 131}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 47, "end": 52}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 54, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 98, "end": 107}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 109, "end": 117}]}]}
{"content": "Maxout networks are compared to the conventional fully-connected DNNs in case of training with both crossentropy and sequence discriminative sMBR criteria ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "fully-connected", "start": 49, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "crossentropy", "start": 100, "end": 111}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sequence", "start": 117, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 65, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "discriminative", "start": 126, "end": 139}]}]}
{"content": "The clear advantage of maxout networks over DNNs is demonstrated when using the cross-entropy criterion on both corpora ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 44, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 80, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 30, "end": 37}]}]}
{"content": "It is also argued that maxout networks are prone to overfitting during sequence training but in some cases it can be successfully overcome with the use of the KL-divergence based regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 23, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 52, "end": 62}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 179, "end": 192}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 30, "end": 37}]}]}
{"content": "The greedy layerwise pretraining method became an impulse for tempestuous development of DNN training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "greedy", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 89, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layerwise", "start": 11, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pretraining", "start": 21, "end": 31}]}]}
{"content": "The pretraining results in DNN weights initialization that facilitates the subsequent finetuning and improves its quality ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 27, "end": 29}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 31, "end": 37}]}]}
{"content": "Nevertheless the fully connected feedforward deep neural networks hereinafter just DNNs for brevity still remain the workhorses of the large majority of ASR systems ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fully", "start": 17, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 83, "end": 86}]}, {"label": ["I-DeepLearning"], "points": [{"text": "connected", "start": 23, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "feedforward", "start": 33, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "deep", "start": 45, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 50, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 57, "end": 64}]}]}
{"content": "The introduction of piecewise-linear ReLU rectified linear units activation functions made it possible to simplify and improve the optimization process during DNN training significantly ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 20, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "rectified", "start": 42, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 65, "end": 74}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 159, "end": 161}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ReLU", "start": 37, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "linear", "start": 52, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 59, "end": 63}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 76, "end": 84}]}]}
{"content": "It was shown that DNNs with ReLU activations may be successfully learned without layerwise pretraining ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 18, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 28, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "layerwise", "start": 81, "end": 89}]}, {"label": ["I-DeepLearning"], "points": [{"text": "pretraining", "start": 91, "end": 101}]}]}
{"content": "However the fact that ReLU and its analogues are linear almost everywhere may also result in overfitting and instability of the training process ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 22, "end": 25}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 93, "end": 103}]}]}
{"content": "This implies the necessity of using effective regularization techniques ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 46, "end": 59}]}]}
{"content": "Dropout can be treated as an approximate way to learn the exponentially large ensemble of different neural nets with subsequent averaging ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 100, "end": 105}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nets", "start": 107, "end": 110}]}]}
{"content": "To improve efficiency of the dropout regularization a new sort of tunable piecewise-linear activation function called maxout was proposed in 9 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 29, "end": 35}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 74, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 118, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 37, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 91, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 102, "end": 109}]}]}
{"content": "In a short time the deep maxout networks DMN were applied to speech recognition tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 20, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 41, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "maxout", "start": 25, "end": 30}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 32, "end": 39}]}]}
{"content": "It was also shown that dropout for DMNs can be very effective in an annealed mode i.e. if the dropout rate gradually decreases epoch-by-epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 23, "end": 29}, {"start": 94, "end": 100, "text": "dropout"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 35, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch-by-epoch", "start": 127, "end": 140}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 102, "end": 105}]}]}
{"content": "We apply DMNs to two significantly different tasks and demonstrate their clear superiority over conventional DNNs under cross-entropy CE training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 9, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 109, "end": 112}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 120, "end": 132}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CE", "start": 134, "end": 135}]}]}
{"content": "Dropout proposed in is a regularization technique for deep feedforward network training which is effective in particular for training network with a large amount of parameters on a limited size dataset ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Dropout", "start": 0, "end": 6}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 25, "end": 38}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 54, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "technique", "start": 40, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "feedforward", "start": 59, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 71, "end": 77}]}]}
{"content": "In the original form of dropout it was proposed to randomly turn off half of neurons per every training example ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 24, "end": 30}]}]}
{"content": "When the neurons with piecewise-linear activation functions like ReLUx = max0 x are used the input space is divided into multiple regions where data is linearly transformed by the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 22, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLUx", "start": 65, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 39, "end": 48}]}]}
{"content": "From this point of view using of dropout with piecewise-linear activation functions performs the more exact ensemble averaging than with activation functions of nonzero curvature such as sigmoid ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 33, "end": 39}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 46, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 137, "end": 146}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 187, "end": 193}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 63, "end": 72}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 148, "end": 156}]}]}
{"content": "The effectiveness of DNNs with ReLU neurons trained with dropout was demonstrated on many tasks from different domains ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DNNs", "start": 21, "end": 24}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 31, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 57, "end": 63}]}]}
{"content": "Maxout is a piecewise-linear activation function which was proposed to improve neural network training with dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "piecewise-linear", "start": 12, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 79, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 108, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 29, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 86, "end": 92}]}]}
{"content": "The advantage of the maxout activation function over ReLU is the possibility to adjust its form by means of parameters tuning although it comes at the cost of k-fold increasing of the parameters number ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 21, "end": 26}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 53, "end": 56}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 108, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "k-fold", "start": 159, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 28, "end": 37}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 39, "end": 46}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tuning", "start": 119, "end": 124}]}]}
{"content": "Maxout networks both fully-connected and convolutional demonstrated impressive results on several benchmark tasks from the computer vision domain ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 41, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 7, "end": 14}]}]}
{"content": "The first application of Deep Maxout Networks DMN to speech recognition task seems to be done in February and in March where it was shown that training of DMNs can be effective even without using dropout ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 25, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 46, "end": 48}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 155, "end": 158}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 196, "end": 202}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Maxout", "start": 30, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 37, "end": 44}]}]}
{"content": "Nevertheless the training of DMNs can be successfully combined with dropout regularization as it was shown here ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "DMNs", "start": 29, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 68, "end": 74}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 76, "end": 89}]}]}
{"content": "There it was proposed to use not the conventional dropout where the dropout rate is constant during the entire training but the annealed dropout AD which consists of the gradually decreasing dropout rate according to the linear schedule ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 50, "end": 56}, {"start": 68, "end": 74, "text": "dropout"}, {"start": 191, "end": 197, "text": "dropout"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "annealed", "start": 128, "end": 135}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 76, "end": 79}, {"start": 199, "end": 202, "text": "rate"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dropout", "start": 137, "end": 143}]}]}
{"content": "We do not compare Maxout + AD to ReLU + AD because in our experience AD training of ReLU networks does not provide significant WER reduction it is also observed in the previous paper whilist carefully tuned sigmoidal DNNs with L2 weight decay often outperform ReLU DNNs with dropout and other types of regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Maxout", "start": 18, "end": 23}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ReLU", "start": 33, "end": 36}, {"start": 84, "end": 87, "text": "ReLU"}, {"start": 260, "end": 263, "text": "ReLU"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoidal", "start": 207, "end": 215}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 227, "end": 228}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 275, "end": 281}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 217, "end": 220}, {"start": 265, "end": 268, "text": "DNNs"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weight", "start": 230, "end": 235}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 237, "end": 241}]}]}
{"content": "When the model was trained the low-rank factorization based on SVD of the last hidden layer was performed and the model was fine-tuned to provide bottleneck features hereinafter SDBNs Speaker-Dependent BottleNeck ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 79, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 86, "end": 90}]}]}
{"content": "In our first attempts we did not use regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 37, "end": 50}]}]}
{"content": "Since we observed that the cross-validation value of the sMBR criterion either increases or remains constant epoch-by-epoch we concluded that the model is overfit ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross-validation", "start": 27, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch-by-epoch", "start": 109, "end": 122}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfit", "start": 155, "end": 161}]}]}
{"content": "So to improve ST performance an effective regularization is required ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 42, "end": 55}]}]}
{"content": "We tried to use the L1 and L2 penalty as well as dropout and F-smoothing to make ST work on Switchboard however none of these approaches succeeded in overfitting reduction ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "L1", "start": 20, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 27, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "dropout", "start": 49, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 150, "end": 160}]}]}
{"content": "We considered the use of maxout activation functions for training deep neural networks as acoustic models for ASR ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 66, "end": 69}]}, {"label": ["I-DeepLearning"], "points": [{"text": "activation", "start": 32, "end": 41}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 43, "end": 51}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 71, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 78, "end": 85}]}]}
{"content": "Using two English speech corpora namely CHiME Challenge 2015 dataset and Switchboard we demonstrated that in case of training with the cross-entropy criterion Deep Maxout Networks DMN are superior to conventional fully-connected feedforward sigmoidal DNNs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross-entropy", "start": 135, "end": 147}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 159, "end": 162}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 180, "end": 182}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 229, "end": 239}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Maxout", "start": 164, "end": 169}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 171, "end": 178}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sigmoidal", "start": 241, "end": 249}]}, {"label": ["I-DeepLearning"], "points": [{"text": "DNNs", "start": 251, "end": 254}]}]}
{"content": "For the same layer sizes the number of DMN parameters is larger than that of DNN but the increase in DNN layer sizes is unable to provide the comparable accuracy gain ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "layer", "start": 13, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DMN", "start": 39, "end": 41}]}, {"label": ["B-DeepLearning"], "points": [{"text": "DNN", "start": 77, "end": 79}, {"start": 101, "end": 103, "text": "DNN"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "sizes", "start": 19, "end": 23}, {"start": 111, "end": 115, "text": "sizes"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 105, "end": 109}]}]}
{"content": "We also found that sequence discriminative training of maxout networks is prone to overfitting which can be reduced with the use of KLD-regularization ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "maxout", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 83, "end": 93}]}, {"label": ["B-DeepLearning"], "points": [{"text": "KLD-regularization", "start": 132, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 62, "end": 69}]}]}
{"content": "The performance of the developed models was examined in terms of the Area Under the Curve AUC derived from the ROC curves as well as Pearson coefficient R between the predicted and the actual efficacy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ROC", "start": 111, "end": 113}]}, {"label": ["I-DeepLearning"], "points": [{"text": "curves", "start": 115, "end": 120}]}]}
{"content": "Associative Neural Networks ASNN approach and fragment descriptors were used to build the models ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Associative", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "ASNN", "start": 28, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 12, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 19, "end": 26}]}]}
{"content": "In three layers neural networks each neuron in the initial layer corresponded to one molecular descriptor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "initial", "start": 51, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 59, "end": 63}]}]}
{"content": "Hidden layer contained from three to six neurons whereas the output layer contained one for STL and FN or 11 MTL neurons corresponding to the number of simultaneously treated properties ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Hidden", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 61, "end": 66}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 7, "end": 11}, {"start": 68, "end": 72, "text": "layer"}]}]}
{"content": "Each model was validated using external fivefold cross-validation procedure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fivefold", "start": 40, "end": 47}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 49, "end": 64}]}]}
{"content": "Supervised learning is usually achieved in what are called feed-forward NNs that process data in several layers consisting of varying numbers of nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feed-forward", "start": 59, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "NNs", "start": 72, "end": 74}]}]}
{"content": "These nodes are organized as input nodes several layers of hidden nodes and output nodes ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 29, "end": 33}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 59, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 76, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "nodes", "start": 35, "end": 39}, {"start": 66, "end": 70, "text": "nodes"}, {"start": 83, "end": 87, "text": "nodes"}]}]}
{"content": "A sliding window covers 60 nucleotides which is calculated to 240 input units to the neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 66, "end": 70}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 85, "end": 90}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 72, "end": 76}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 92, "end": 98}]}]}
{"content": "The neural network structure is a standard three layer feedforward neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 55, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}, {"start": 74, "end": 80, "text": "network"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 67, "end": 72}]}]}
{"content": "This kind of neural network has several names such as multilayer perceptrons MLP feed forward neural network and backpropagation neural network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 13, "end": 18}, {"start": 129, "end": 134, "text": "neural"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 54, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 77, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "feed", "start": 81, "end": 84}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 113, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 20, "end": 26}, {"start": 101, "end": 107, "text": "network"}, {"start": 136, "end": 142, "text": "network"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptrons", "start": 65, "end": 75}]}, {"label": ["I-DeepLearning"], "points": [{"text": "forward", "start": 86, "end": 92}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 94, "end": 99}]}]}
{"content": "There are two output units corresponding to the donor and acceptor splice sites 128 hidden layer units and 240 input units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 14, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 84, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 111, "end": 115}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 21, "end": 25}, {"start": 117, "end": 121, "text": "units"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 91, "end": 95}]}]}
{"content": "The 240 input units were used since the orthogonal input scheme uses four inputs each nucleotide in the window ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "input", "start": 8, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 14, "end": 18}]}]}
{"content": "The neural network program code was reused from a previous study and in this code the number of hidden units was hard coded and optimized for 128 hidden units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 96, "end": 101}, {"start": 146, "end": 151, "text": "hidden"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 103, "end": 107}, {"start": 153, "end": 157, "text": "units"}]}]}
{"content": "There is also a bias signal added to the hidden layer and the output layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "bias", "start": 16, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 41, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 62, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 48, "end": 52}, {"start": 69, "end": 73, "text": "layer"}]}]}
{"content": "The activation function is a standard sigmoid function shown in Eq. 1 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 4, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 38, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 15, "end": 22}]}]}
{"content": "The \u00ce\u00b2 values for the sigmoid functions are 0.1 for both the hidden layer activation and the output layer activation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 22, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 61, "end": 66}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 93, "end": 98}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 30, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 68, "end": 72}, {"start": 100, "end": 104, "text": "layer"}]}]}
{"content": "When doing forward calculations and backpropagation the sigmoid function is called repetitively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 36, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 56, "end": 62}]}]}
{"content": "A fast and effective evaluation of the sigmoid function can improve the overall performance considerably ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 39, "end": 45}]}]}
{"content": "To improve the performance of the sigmoid function a precalculated table for the exponential function is used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "sigmoid", "start": 34, "end": 40}]}]}
{"content": "There is no momentum used in the training ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "momentum", "start": 12, "end": 19}]}]}
{"content": "We have not implemented any second order methods to help the convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 61, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 80, "end": 86}]}]}
{"content": "The neural network training is done using standard backpropagation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "backpropagation", "start": 51, "end": 65}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}]}
{"content": "The training was done in three sessions and for each session we chose separate but constant learning rates ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 92, "end": 99}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rates", "start": 101, "end": 105}]}]}
{"content": "The learning rate \u00ce\u00b7 was chosen to be 0.2 0.1 and 0.02 respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}]}
{"content": "The shown indicators are computed using a neural network which has been trained for about 80 epochs with a learning rate of 02 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 42, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 93, "end": 98}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 107, "end": 114}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 49, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 116, "end": 119}]}]}
{"content": "The best performing neural network achieved a correlation coefficient of 0552 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 20, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 27, "end": 33}]}]}
{"content": "The MGN works in this case as a discrete time cellular neural network and the training is based on stochastic gradient descent as described in the paper ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 55, "end": 60}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 99, "end": 108}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 62, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 110, "end": 117}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 119, "end": 125}]}]}
{"content": "The training of a TPNN is based on a combination of stochastic gradient descend and back propagation with several improvements that make the training of the shared weights feasible and that were reported in detail in the context of training CNNs for pattern recognition ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 52, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "back", "start": 84, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNNs", "start": 241, "end": 244}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 63, "end": 70}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descend", "start": 72, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "propagation", "start": 89, "end": 99}]}]}
{"content": "In stochastic gradient descent the true gradient is approximated by the gradient of the loss function which is evaluated on a single training sample ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 3, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "loss", "start": 88, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 14, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 23, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 93, "end": 100}]}]}
{"content": "Weight decay is a regularization method that penalizes large weights in the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Weight", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "regularization", "start": 18, "end": 31}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 61, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 7, "end": 11}]}]}
{"content": "The weight decay penalty term causes the insignificant weights to converge to zero ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 55, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 11, "end": 15}]}]}
{"content": "The parameter \u00c2\u00b5 is controlling the stepsize of the gradient descend ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameter", "start": 4, "end": 12}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stepsize", "start": 36, "end": 43}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 52, "end": 59}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descend", "start": 61, "end": 67}]}]}
{"content": "The initial step size is already small around \u00c2\u00b5 = 0.01 and it is decreased after each training epoch with a constant factor ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "step", "start": 12, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 96, "end": 100}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 17, "end": 20}]}]}
{"content": "This is necessary to achieve a slow convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 36, "end": 46}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 55, "end": 61}]}]}
{"content": "A sweep through the whole data set is called one epoch ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 49, "end": 53}]}]}
{"content": "Up to 1000 epochs are necessary to train the TPNN in a typical experimental setup 20 amino acids in the alphabet peptides of length 5 to 10 30-50 training samples from measurements as a starting set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 11, "end": 16}]}]}
{"content": "The slow convergence of the error is a consequence of the relative small stepsize in the gradient descent but it is necessary in order to get overall convergence of the weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convergence", "start": 9, "end": 19}, {"start": 150, "end": 160, "text": "convergence"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stepsize", "start": 73, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 89, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 169, "end": 175}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 98, "end": 104}]}]}
{"content": "It is well known that neural network ensembles perform better in terms of generalisation than single models would do ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 22, "end": 27}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalisation", "start": 74, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 29, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "ensembles", "start": 37, "end": 45}]}]}
{"content": "An ensemble of TPNNs consists of several single TPNN models that are trained on randomly chosen subsets of the training data and the training starts with random weight initializations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 111, "end": 118}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 161, "end": 166}]}, {"label": ["I-DeepLearning"], "points": [{"text": "data", "start": 120, "end": 123}]}, {"label": ["I-DeepLearning"], "points": [{"text": "initializations", "start": 168, "end": 182}]}]}
{"content": "Two analytical models have been used to fit the data set to a prognostic index: a piecewise linear model Cox regression also known as proportional hazards and a flexible model consisting of Partial Logistic Artificial Neural Networks regularised with Automatic Relevance Determination PLANN-ARD ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Artificial", "start": 207, "end": 216}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 218, "end": 223}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Networks", "start": 225, "end": 232}]}]}
{"content": "If these features are stored in a vector f = f1...fh and if we represent the i-th residue in the sequence as ri then f is obtained as: where N h is a non-linear function which we implement by a two-layered feedforward Neural Network with h non-linear output units ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "feedforward", "start": 206, "end": 216}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 251, "end": 256}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 218, "end": 223}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 225, "end": 231}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 258, "end": 262}]}]}
{"content": "The number of free parameters in the overall N1-NN can be controlled by: the number of units in the hidden layer of the sequence-to-feature network N h  N H f ; the number of hidden units in the feature-to-output network N o N H o ; the number of hidden states in the feature vector f which is also the number of output units in the sequence-to-feature network Nf  ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 19, "end": 28}]}, {"label": ["B-DeepLearning"], "points": [{"text": "N1-NN", "start": 45, "end": 49}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}, {"start": 175, "end": 180, "text": "hidden"}, {"start": 247, "end": 252, "text": "hidden"}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 313, "end": 318}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 182, "end": 186}, {"start": 320, "end": 324, "text": "units"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "states", "start": 254, "end": 259}]}]}
{"content": "Each training is conducted by 10 fold-cross validation i.e. 10 different sets of training runs are performed in which a different tenth of the overall set is reserved for testing ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "fold-cross", "start": 33, "end": 42}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 44, "end": 53}]}]}
{"content": "The training set is used to learn the free parameters of the network by gradient descent while the validation set is used to choose model and hyperparameters network size and architecture i.e. N H f  Nf and N H o  ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "parameters", "start": 43, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 72, "end": 79}]}, {"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 99, "end": 108}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperparameters", "start": 142, "end": 156}]}, {"label": ["B-DeepLearning"], "points": [{"text": "network", "start": 158, "end": 164}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 13, "end": 15}, {"start": 110, "end": 112, "text": "set"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 81, "end": 87}]}, {"label": ["I-DeepLearning"], "points": [{"text": "size", "start": 166, "end": 169}]}]}
{"content": "For each different architecture we run three trainings which differ only in the training vs. validation split ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "validation", "start": 93, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "split", "start": 104, "end": 108}]}]}
{"content": "For each fold the three networks for the best architecture are ensemble averaged and evaluated on the corresponding test set ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "test", "start": 116, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set", "start": 121, "end": 123}]}]}
{"content": "Training is performed by gradient descent on the error which we model as the relative entropy between the target class and the output of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "gradient", "start": 25, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 34, "end": 40}]}]}
{"content": "The overall output of the network output layer of N o  is implemented as a softmax function while all internal squashing functions are implemented as hyperbolic tangents ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "softmax", "start": 75, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hyperbolic", "start": 150, "end": 159}]}, {"label": ["I-DeepLearning"], "points": [{"text": "tangents", "start": 161, "end": 168}]}]}
{"content": "Training terminates when either the walltime on the server is reached 6 days for fungi and plants 10 days for animals or the epoch limit is reached 40k for fungi and plants and 20k for animals ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 125, "end": 129}]}]}
{"content": "The gradient is updated 360 times for each epoch or once every 2-6 examples depending on the set and the examples are shuffled between epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "epoch", "start": 43, "end": 47}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 135, "end": 140}]}]}
{"content": "The learning rate is halved every time a reduction of the error is not observed for more than 50 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 4, "end": 11}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 97, "end": 102}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 13, "end": 16}]}]}
{"content": "Both predictors are assessed by ten-fold cross-validation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "ten-fold", "start": 32, "end": 39}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 41, "end": 56}]}]}
{"content": "The type of neural network with which we performed the experiments was Multilayer Perceptron MLP because the results obtained with other types of networks were not satisfactory and they tended to be slower than MLP ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multilayer", "start": 71, "end": 80}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 93, "end": 95}, {"start": 211, "end": 213, "text": "MLP"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 19, "end": 25}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 82, "end": 91}]}]}
{"content": "The preparatory steps for conducting the experiments consisted in determining: a the size of the training and testing set; b the error function; c the number of hidden units; d the activation functions for the hidden and the output neurons; e the minimum and maximum values for weight decay ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 110, "end": 116}]}, {"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 129, "end": 133}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 161, "end": 166}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 181, "end": 190}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 225, "end": 230}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 278, "end": 283}]}, {"label": ["I-DeepLearning"], "points": [{"text": "set;", "start": 118, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function;", "start": 135, "end": 143}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units;", "start": 168, "end": 173}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 192, "end": 200}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons;", "start": 232, "end": 239}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 285, "end": 289}]}]}
{"content": "For the hidden neurons were used as activation functions identity logistic tanh and exponential and the same ones were chosen for the output neurons ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 8, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "activation", "start": 36, "end": 45}]}, {"label": ["B-DeepLearning"], "points": [{"text": "tanh", "start": 75, "end": 78}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 134, "end": 139}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 15, "end": 21}, {"start": 141, "end": 147, "text": "neurons"}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 47, "end": 55}]}]}
{"content": "The error functions used were sum of squares and cross entropy ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 4, "end": 8}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sum", "start": 30, "end": 32}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 49, "end": 53}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 10, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 34, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "squares", "start": 37, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 55, "end": 61}]}]}
{"content": "The minimum and the maximum number of hidden units were chosen differently for each experiment because we conducted several experiments which had different number of inputs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 38, "end": 43}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 45, "end": 49}]}]}
{"content": "The larger the number of hidden units in a neural network model the stronger the model is the more capable the network is to model complex relationships between the inputs and the target variables ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 25, "end": 30}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 43, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 50, "end": 56}]}]}
{"content": "The optimal number of hidden units is minimum 1/10 of the number of training cases and maximum 1/5 but we have varied this interval ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 22, "end": 27}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 29, "end": 33}]}]}
{"content": "The use of decay weights for hidden layer and output layer was preferred in order to prevent overfitting thereby potentially improving generalization performance of the network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 11, "end": 15}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 29, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 46, "end": 51}]}, {"label": ["B-DeepLearning"], "points": [{"text": "overfitting", "start": 93, "end": 103}]}, {"label": ["B-DeepLearning"], "points": [{"text": "generalization", "start": 135, "end": 148}]}, {"label": ["I-DeepLearning"], "points": [{"text": "weights", "start": 17, "end": 23}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 36, "end": 40}, {"start": 53, "end": 57, "text": "layer"}]}]}
{"content": "Weight decay or weight elimination are often used in MLP training and aim to minimize a cost function which penalizes large weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Weight", "start": 0, "end": 5}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 16, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 53, "end": 55}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cost", "start": 88, "end": 91}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 124, "end": 130}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 7, "end": 11}]}, {"label": ["I-DeepLearning"], "points": [{"text": "function", "start": 93, "end": 100}]}]}
{"content": "These techniques tend to result in networks with smaller weights ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 57, "end": 63}]}]}
{"content": "The minimum chosen weight decay was 0.0001 and the maximum 0001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 19, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decay", "start": 26, "end": 30}]}]}
{"content": "To perform the experiments the data were split in a training 67 % and a testing dataset 33 % ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "testing", "start": 72, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "dataset", "start": 80, "end": 86}]}]}
{"content": "As error functions we tested the cross entropy and the sum of squares ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "error", "start": 3, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 33, "end": 37}]}, {"label": ["B-DeepLearning"], "points": [{"text": "sum", "start": 55, "end": 57}]}, {"label": ["I-DeepLearning"], "points": [{"text": "functions", "start": 9, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "entropy", "start": 39, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 59, "end": 60}]}, {"label": ["I-DeepLearning"], "points": [{"text": "squares", "start": 62, "end": 68}]}]}
{"content": "The weight decays in both the hidden and the output layer varied between 0.0001 and 0001 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "weight", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 45, "end": 50}]}, {"label": ["I-DeepLearning"], "points": [{"text": "decays", "start": 11, "end": 16}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 52, "end": 56}]}]}
{"content": "The values of the CV criterion Table 1 show a better performance in correspondence to a choice of 6 hidden units and decay parameter equal to 0.01 that we therefore used ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 100, "end": 105}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 117, "end": 121}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 107, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 123, "end": 131}]}]}
{"content": "Ten-fold cross-validation obtained by averaging over five fits for various values of the number of hidden units H and of the decay parameter ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Ten-fold", "start": 0, "end": 7}]}, {"label": ["B-DeepLearning"], "points": [{"text": "decay", "start": 125, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "cross-validation", "start": 9, "end": 24}]}, {"label": ["I-DeepLearning"], "points": [{"text": "parameter", "start": 131, "end": 139}]}]}
{"content": "Scheme of a multilayer perceptron with the layer of inputs x1...xp t the layer of hidden units z1...zH and the output layer here represented by the unique response h ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "multilayer", "start": 12, "end": 21}]}, {"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 82, "end": 87}]}, {"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 111, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "perceptron", "start": 23, "end": 32}]}, {"label": ["I-DeepLearning"], "points": [{"text": "units", "start": 89, "end": 93}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 118, "end": 122}]}]}
{"content": "Additionally one of the authors recently took advantage of a deep learning algorithm built on a multilayer autoencoder neural network that lead to interesting prediction results in reference ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 61, "end": 64}]}, {"label": ["B-DeepLearning"], "points": [{"text": "autoencoder", "start": 107, "end": 117}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 119, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 66, "end": 73}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 126, "end": 132}]}]}
{"content": "We use the July 2009 version of the datasets for analyzing and selecting hyper-parameters ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters", "start": 73, "end": 88}]}]}
{"content": "Autoencoder neural networks were trained using the free GPU-accelerated software package Torch7 using stochastic gradient descent with a learning rate of 0.01 for 25 iterations ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Autoencoder", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "stochastic", "start": 102, "end": 111}]}, {"label": ["B-DeepLearning"], "points": [{"text": "learning", "start": 137, "end": 144}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 12, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 19, "end": 26}]}, {"label": ["I-DeepLearning"], "points": [{"text": "gradient", "start": 113, "end": 120}]}, {"label": ["I-DeepLearning"], "points": [{"text": "descent", "start": 122, "end": 128}]}, {"label": ["I-DeepLearning"], "points": [{"text": "rate", "start": 146, "end": 149}]}]}
{"content": "L2 regularization was used on all weights which were initialized randomly from the uniform distribution ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "L2", "start": 0, "end": 1}]}, {"label": ["B-DeepLearning"], "points": [{"text": "weights", "start": 34, "end": 40}]}, {"label": ["I-DeepLearning"], "points": [{"text": "regularization", "start": 3, "end": 16}]}]}
{"content": "The hidden unit function is a Sigmoid ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hidden", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Sigmoid", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "unit", "start": 11, "end": 14}]}]}
{"content": "The neural network is a modified version of General Regression Neural Network GRNN used as a classification tool ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 4, "end": 9}]}, {"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 44, "end": 50}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 78, "end": 81}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 11, "end": 17}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 52, "end": 61}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 70, "end": 76}]}]}
{"content": "In order to perform the barcode sequences classification we introduce a modified version of General Regression Neural Network GRNN that use alternatively a function derived from Jaccard distance and fractional distance instead of the euclidean one to compare learned prototypes against test sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 92, "end": 98}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 126, "end": 129}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 100, "end": 109}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 111, "end": 116}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 118, "end": 124}]}]}
{"content": "The proposed method is based on two modified versions of the General Regression Neural Network ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 61, "end": 67}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 69, "end": 78}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 80, "end": 85}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 87, "end": 93}]}]}
{"content": "The General Regression Neural Network is a neural network created for regression i.e. the approximation of a dependent variable y given a set of sample x y where x is the independent variable ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 4, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 43, "end": 48}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 12, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 23, "end": 28}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 30, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 50, "end": 56}]}]}
{"content": "Classification results in terms of accuracy precision and recall scores have been compared with both the GRNN algorithm using J-function and the SVM classifier ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 35, "end": 42}]}, {"label": ["B-DeepLearning"], "points": [{"text": "precision", "start": 44, "end": 52}]}, {"label": ["B-DeepLearning"], "points": [{"text": "recall", "start": 58, "end": 63}]}, {"label": ["B-DeepLearning"], "points": [{"text": "GRNN", "start": 105, "end": 108}]}]}
{"content": "Simple spiking neural network models such as Integrate and fire models without bio-realistic features were simulated in the older generation GPUs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Simple", "start": 0, "end": 5}]}, {"label": ["I-DeepLearning"], "points": [{"text": "spiking", "start": 7, "end": 13}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 15, "end": 20}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 22, "end": 28}]}]}
{"content": "All above described methods require to set two hyper-parameters: \u00ce\u00bb and \u00ce\u00b1 controlling the sparsity and the network influence respectively ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "hyper-parameters:", "start": 47, "end": 63}]}]}
{"content": "Deep learning neural networks are capable to extract significant features from raw data and to use these features for classification tasks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Deep", "start": 0, "end": 3}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 5, "end": 12}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 14, "end": 19}]}, {"label": ["I-DeepLearning"], "points": [{"text": "networks", "start": 21, "end": 28}]}]}
{"content": "In this work we present a deep learning neural network for DNA sequence classification based on spectral sequence representation ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 26, "end": 29}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 31, "end": 38}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 40, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 47, "end": 53}]}]}
{"content": "The framework is tested on a dataset of 16S genes and its performances in terms of accuracy and F1 score are compared to the General Regression Neural Network already tested on a similar problem as well as naive Bayes random forest and support vector machine classifiers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "accuracy", "start": 83, "end": 90}]}, {"label": ["B-DeepLearning"], "points": [{"text": "General", "start": 125, "end": 131}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Regression", "start": 133, "end": 142}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 144, "end": 149}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 151, "end": 157}]}]}
{"content": "The obtained results demonstrate that the deep learning approach outperformed all the other classifiers when considering classification of small sequence fragment 500 bp long ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 42, "end": 45}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 47, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "approach", "start": 56, "end": 63}]}]}
{"content": "Among the deep learning architecture it is usually comprised the LeNet-5 network or Convolutional Neural Network CNN a neural network that is inspired by the visual system\u00e2\u20ac\u2122s structure ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 10, "end": 13}]}, {"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 65, "end": 71}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Convolutional", "start": 84, "end": 96}]}, {"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 113, "end": 115}]}, {"label": ["B-DeepLearning"], "points": [{"text": "neural", "start": 119, "end": 124}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 15, "end": 22}]}, {"label": ["I-DeepLearning"], "points": [{"text": "architecture", "start": 24, "end": 35}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Neural", "start": 98, "end": 103}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Network", "start": 105, "end": 111}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 126, "end": 132}]}]}
{"content": "In this work we want to understand if the convolutional network is capable to identify and to use these features for sequence classification outperforming the classifiers proposed in the past ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 42, "end": 54}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 56, "end": 62}]}]}
{"content": "The one used in this work is a modified version of the LeNet-5 network introduced by LeCun et al. in and it is implemented using the python Theano package for deep learning ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 55, "end": 61}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 159, "end": 162}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 164, "end": 171}]}]}
{"content": "The modified LeNet-5 proposed network is made of two lower layers of convolutional and max-pooling processing elements followed by two traditional fully connected Multi Layer Perceptron MLP processing layers so that there are 6 processing layers ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "LeNet-5", "start": 13, "end": 19}]}, {"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 69, "end": 81}]}, {"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 87, "end": 97}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Multi", "start": 163, "end": 167}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 186, "end": 188}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Layer", "start": 169, "end": 173}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Perceptron", "start": 175, "end": 184}]}]}
{"content": "The max-pooling is a non-linear down-sampling layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 4, "end": 14}]}, {"label": ["B-DeepLearning"], "points": [{"text": "down-sampling", "start": 32, "end": 44}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 46, "end": 50}]}]}
{"content": "The first layer of convolutional kernels named kernel 0 is made of L = 10 kernels of dimension 5 so that n = 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 19, "end": 31}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernels", "start": 33, "end": 39}]}]}
{"content": "From a spectral representation vector made of 1024 components this layer produces 10 vectors of 1024 dimensions that the pooling layer reduces to the 10 feature maps of 510 dimensions ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "pooling", "start": 121, "end": 127}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 129, "end": 133}]}]}
{"content": "These vectors are the input for the second convolutional layer ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 43, "end": 55}]}, {"label": ["I-DeepLearning"], "points": [{"text": "layer", "start": 57, "end": 61}]}]}
{"content": "The second layer of kernel kernel 1  is made of L = 20 kernels of dimension 5 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "layer", "start": 11, "end": 15}]}, {"label": ["I-DeepLearning"], "points": [{"text": "of", "start": 17, "end": 18}]}, {"label": ["I-DeepLearning"], "points": [{"text": "kernel", "start": 20, "end": 25}]}]}
{"content": "In both cases the max-pooling layer has dimension 2 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "max-pooling", "start": 18, "end": 28}]}]}
{"content": "Convolution and maxpooling are usually considered together and they are represented in the lower part of Fig. 2 as two highly connected blocks ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "Convolution", "start": 0, "end": 10}]}, {"label": ["B-DeepLearning"], "points": [{"text": "maxpooling", "start": 16, "end": 25}]}]}
{"content": "The two upper level layers corresponds to a traditional fully-connected MLP: the first layer of the MLP operates on the total number of output from the lower level the output is flattened to a 1-D vector and the total number of units in the Hidden Layer is 500 ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "MLP:", "start": 72, "end": 75}]}, {"label": ["B-DeepLearning"], "points": [{"text": "MLP", "start": 100, "end": 102}]}, {"label": ["B-DeepLearning"], "points": [{"text": "Hidden", "start": 241, "end": 246}]}, {"label": ["I-DeepLearning"], "points": [{"text": "Layer", "start": 248, "end": 252}]}]}
{"content": "In the second case the ten-fold cross validation scheme was repeated considering as test set the sequence fragments of shorter size 500 bp long obtained randomly extracting 500 consecutive nucleotides from the original full length sequences ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "cross", "start": 32, "end": 36}]}, {"label": ["I-DeepLearning"], "points": [{"text": "validation", "start": 38, "end": 47}]}]}
{"content": "The CNN 134 has been run considering two different kernels sizes: kernel 0 = kernel 1 = 5 in the first run; kernel 0 = 25 kernel 1 = 15 in the second run ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 4, "end": 6}]}]}
{"content": "In both configurations the training phase has been run for 200 epochs ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "training", "start": 27, "end": 34}]}, {"label": ["B-DeepLearning"], "points": [{"text": "epochs", "start": 63, "end": 68}]}, {"label": ["I-DeepLearning"], "points": [{"text": "phase", "start": 36, "end": 40}]}]}
{"content": "The spectral representation is obtained as k-mers frequencies along the sequences; the CNN belongs to the so called deep learning algorithms ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "CNN", "start": 87, "end": 89}]}, {"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 116, "end": 119}]}, {"label": ["I-DeepLearning"], "points": [{"text": "learning", "start": 121, "end": 128}]}]}
{"content": "We designed and tested some topic modeling techniques and we took advantage of a deep neural network approach ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "deep", "start": 81, "end": 84}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 86, "end": 91}]}, {"label": ["I-DeepLearning"], "points": [{"text": "network", "start": 93, "end": 99}]}]}
{"content": "A convolutional neural net learns to classify patches as salient long looks or not ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "convolutional", "start": 2, "end": 14}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neural", "start": 16, "end": 21}]}, {"label": ["I-DeepLearning"], "points": [{"text": "net", "start": 23, "end": 25}]}]}
{"content": "Two output neurons were connected to the reinitialized layer then training followed on augmented 800 \u00c3\u2014 800 px patches for 10000 iterations in Caffe ", "annotation": [{"label": ["B-DeepLearning"], "points": [{"text": "output", "start": 4, "end": 9}]}, {"label": ["I-DeepLearning"], "points": [{"text": "neurons", "start": 11, "end": 17}]}]}
